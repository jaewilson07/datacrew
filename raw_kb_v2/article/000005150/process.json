{"file_path": "../../raw_kb_v2\\article\\000005150\\index.html", "content": {"title": "\n\n\n Subset Processing in Magic ETL\n\n\n", "article_body": "\n\nIntro\n-------\n\n\n**Note:**\n This feature is in beta. Visit this link to\n\njoin the beta\n\n.\n\nSubset Processing in Magic ETL provides a precise and efficient solution to data processing. With\u00a0Subset Processing, you can choose to load only the batches updated since the last successful execution or you can specify the batches to be loaded based on their creation date or partition name.\u00a0You can customize your approach to your organization's specific needs. To learn more about partitions, data architecture, and how to optimize your data pipeline, visit our\n\nData Center Overview\n\ntopic in the knowledge base.\n\n\n This article provides information\u00a0about using Subset Processing in the following topics:\n\n Partition\u00a0basics\n* Partition best practices\n* Configure initial data processing\n* Use case\n* Configure update method\n\nPartition Basics\n--------------------\n\nPartitions are a way of organizing your data into groups to make data updates faster and more efficient. The graphic below illustrates\n\n\n replacing and appending data with partitions.\n\nA common way to understand partitions is to think of\u00a0an encyclopedia\u2014a collection of books that are part of the same set, but individually different. If an update is made to one book and it is republished, you can buy a new copy of that book without replacing the entire set of books. Partitions divide your data into separate collections, or subsets, so that when you replace some data, you do not need to replace all of it. Appending data when using partitions is like buying a new volume of the encyclopedia and adding it to the end of your collection on the shelf.\n\n\n The following notes further define using the partitions:\n\n Partition key tags divide data into subsets.\n* Ingestion of new data only affects the specified partitions.\n* Accessing and updating only the necessary data partitions makes the process faster and more efficient.\n\nPartition Best Practices\n----------------------------\n\nA good partition needs to be large enough to take advantage of the enhanced processing speed that partitioning offers, but also\n\n\n granular enough to meet your specific needs.\n\n\n For most situations, a date-based, day-level partition strikes a good balance between these alternatives. However, if you need to keep more than five year's worth\u00a0of\u00a0day-based partitions in a single DataSet, you may want to consider using a week-level or month-level partition instead.\n\n\n Some organizations need partitions that are more granular than day level. If that is the case for your organization, you can create a custom partition by combining two or more columns in your DataFlow. For example, if you have eight store locations, you might want to create a partition that combines the store ID and the date.\n\n*Important:**\n There is a 1500-partition limit for each DataSet. If you create too many partitions, you may reach this limit and experience problems. In the example from the previous paragraph, if you create a partition key using store ID + date, you can only store 187 day's worth\u00a0of data. (1500 partitions divided by 8 stores.)\n\nConfigure Initial Data Processing\n-------------------------------------\n\nMagic ETL provides three data selection methods when configuring your input DataSet for a DataFlow: All, New, and Filter by batch. These methods are described in the process below.\n\nFollow the steps below to set up initial data processing in Magic ETL.\n\n. Access Magic ETL by locating your DataSet in Domo and going to the\n **Data**\n tab for the DataSet.\n2. In the\n **Data**\n tab, select\n **Open With**\n >\n **Magic ETL**\n .\n\n\n The Magic ETL interface opens with your DataSet on the canvas as the input DataSet.\n3. Select the DataSet on the canvas to expand the editor\u00a0at the bottom of the screen and display four tabs:\n **Configuration**\n ,\n **Details**\n ,\n **Data**\n , and\n **Notes**\n .\n4. In the\n **Configuration**\n tab, expand the\n **Data Selection**\n menu and choose the data selection method that meets your needs. These methods are described below:\n\t* **All \u2014**\n\t This is the default option.\u00a0Selecting this option loads all rows from the selected input DataSet. Some DataFlow options may be automatically optimized.\n\n To learn more about automatic optimization, see our\n\n Magic ETL DataFlow Auto Append Processing\n\n article.\n\t* **New\u00a0\u2014**\n\t Select this option if you want to load to the DataSet batches\u00a0updated since the last successful execution, including restated partitions.\n\n If you select this option and your DataSet has a replace operation, the next DataFlow execution loads all batches from the replace and any batches appended since the replace. Your outputs continue to perform the action that you have saved for the DataFlow (such as append or replace).\n\n**Note:**\n\t If you occasionally replace all batches in your input DataSet and always append to your output DataSet within the DataFlow, there may be duplicate data in your output DataSet.\n\t* **Filter by batch \u2014**\n\t Select this option if you want to specify which batches\u00a0to load based on their creation date or, in the case of partitions, their name.\u00a0If you choose this option, you must also complete the section of the editor labeled\n\t **Filter by Batch**\n\t .\n\n In the\n\t **Import data when**\n\t field, there are two options:\n\n\n\t\t+ If you want to load data using a specific processing date, select\n\t\t **Data processed**\n\t\t . Then select the logic you want from the list options in the\n\t\t **Date is in**\n\t\t fields.\n\t\t+ If you want to load data using a custom expression, select\n\t\t **Custom expression**\n\t\t . Use the formula field to create a custom expression. The table below contains information about what values you can use to write a custom expression.\n\n\t When it comes to partitioning a DataSet, the process is based on the metadata about the DataSet rather than the values within the data itself. This means that when you define a subset of the DataSet, you are not selecting specific values within the data, but rather are selecting a subset of the metadata that defines the data.\n\n\n\t\t|  |  |\n\t\t| --- | --- |\n\t\t|\n\t\t```\n\n\tbatch.id\n\t\t```\n\t\t |\n\t\t The numerical identifier for a data part. (This is system generated.)\n\t\t  |\n\t\t|\n\t\t```\n\n\tbatch.name\n\t\t```\n\t\t |\n\t\t The symbolic name for a data part.\n\t\t  |\n\t\t|\n\t\t```\n\n\tbatch.created\n\t\t```\n\t\t |\n\t\t The timestamp indicating when the upload of the data part corresponding to part\n\n\t .\n\n\t id began.\n\t\t  |\n\t\t|\n\t\t```\n\n\tbatch.completed\n\t\t```\n\t\t |\n\t\t The timestamp indicating when the upload of the data part corresponding to part\n\n\t .\n\n\t id was completed.\n\t\t  |\n\t\t|\n\t\t```\n\n\tbatch.recorded\n\t\t```\n\t\t |\n\t\t The timestamp indicating when the data part corresponding to\u00a0part\n\n\t .\n\n\t id was added to the DataSet.\n\t\t  |\n\t\t|\n\t\t```\n\n\tcursor\\_batch.id\n\t\t```\n\t\t |\n\t\t The identifier of the most recent batch successfully processed by this DataFlow. Compare with batch.id.\n\t\t  |\n\t\t|\n\t\t```\n\t\tpartition.name\n\t\t```\n\t\t |\n\t\t The name of the partition with which the batch is associated. Identical to batch\n\n\t .\n\n\t name in this context. The value is null if the batch is not associated with a partition.\n\t\t  |\n\nUse Case\n------------\n\nThe formula displayed in this image (there is a code sample below)\u00a0takes advantage\u00a0of the fact that the input DataSet is partitioned by date. This means that the data is split into different partitions based on the date, which allows the formula to use date-related functions to filter the data.\n\nThe formula creates a filter that compares the partition name (which is a date) to the current date. If the current date is the first day of the month, the filter allows all partitions from the past year to pass through. If the current date is any other day of the month, the filter only allows the last 30 day's worth of partitions to pass through.\n\n\n This filtering strategy enables you to process less data every day while still maintaining a full year's worth of data one time per month. You can use this approach in various scenarios, and we recommend that you experiment\n\n\n with it.\n\n\n```\nDATE(batch.name) >= CURRENT_DATE() \u2013 (CASE WHEN DAYOFMONTH(CURRENT_DATE()) = 1 THEN 365 ELSE 30 END)\n\n``\n\n\n Configure Update Method\n---------------------------\n\nThe following data update methods are available in Magic ETL:\n\n\n Update Method\n  |\n Description\n  |\n| --- | --- |\n|\n Replace\n  |\n Each execution replaces the output DataSet's contents with its latest results.\n  |\n|\n Append\n  |\n Each execution appends its results to the output DataSet as a new batch.\n  |\n|\n Partition\n  |\n Each execution groups rows into batches according to a name, which is the value of a specific column. Each unique batch name in a DataSet is called a partition. These partitions are all appended to the DataSet, and any pre-existing partitions with matching names are replaced. Finally, if a partition filter expression is specified, all partitions are evaluated against it and any that do not pass are deleted.\n\n\n**Important:**\n* Data partitions are limited to 1500 data versions on any output DataSet that uses data partitions.\n* Column Value must be non-null and not exceed 128 bytes.\n\n|\n\nTo learn more about update methods, see our\n\nDataSet Update Methods\n\nand\n\nData Fundamentals\n\narticles.\n\n\n Follow the steps below to configure the update method for your DataSet in Magic ETL.\n\n. After you have an output DataSet on your canvas for the\u00a0DataFlow, select the output DataSet to expand the editor at the bottom of the screen.\n2. In the\n **Configuration**\n tab of the editor, choose the update method that meets your needs from the\n **Update Method**\n list (Replace, Append, or Partition).\n3. (Conditional) If you select Partition as the update method, you must then select the column you want to use to divide the data from the\n **Partition name column**\n list.\n\n\n After you select a column, enable\n **Specify which partitions to keep**\n unless you want to delete all partitions.\n4. (Optional) If you enable\n **Specify which partitions to keep**\n , a field displays in which you can enter an expression. This expression keeps any specified partitions; all other partitions are deleted. Note that this field only validates the\n\npartition.name\n\nexpression.\n\n\n", "preview_article": "\n\nClick here to preview\n\n", "url_name": "\n\n\n 000005150\n\n\n", "summary____________________________________________briefly_describe_the_article_the_summary_is_used_in_search_results_to_help_users_find_relevant_articles_you_can_improve_the_accuracy_of_search_results_by_including_phrases_that_your_customers_use_to_describe_this_issue_or_topic": "\n\n", "article_number": "\n\n\n 000005150\n\n\n", "archived_date": "\n\n\n", "language": "\n\n\n English\n\n\n", "primary_version": "\n\nSubset Processing in Magic ETL\n\n", "article_total_view_count": "\n\n\n 1,340\n\n\n", "": "\n\n", "article_created_date": "\n\n\n 3/9/2023, 9:03 PM\n\n\n", "first_published_date": "\n\n\n 3/13/2023, 12:04 PM\n\n\n"}, "title": "subset_processing_in_magic_etl", "breadcrumbs": [{"text": "domo", "url": "/s/knowledge-base/"}, {"text": "transforming_data_in_domo", "url": "/s/topic/0TO5w000000ZamzGAC"}, {"text": "magic_etl", "url": "/s/topic/0TO5w000000ZanvGAC"}]}