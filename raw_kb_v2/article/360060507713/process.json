{"file_path": "../../raw_kb_v2\\article\\360060507713\\index.html", "content": {"title": "\n\n\n Google BigQuery High Bandwidth Service Connector\n\n\n", "article_body": "\n\nIntro\n-------\n\nBigQuery is a data platform to create, manage, share and query data. Domo's\u00a0Google BigQuery High Bandwidth Service connector lets you bring in larger results from BigQuery through the assistance of the Google Cloud Storage service. To learn more about the\u00a0BigQuery\u00a0API, go to\n\nhttps://cloud.google.com/bigquery/docs/reference/v2/\n\n.\n\n\n The Google BigQuery\u00a0High Bandwidth Service connector is a \"Cloud App\" connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the connector page for this and other Cloud App connectors by clicking\n ********Cloud App********\n in the toolbar at the top of the window.\n\n\n This topic discusses the fields and menus that are specific to the\u00a0Google BigQuery\u00a0High Bandwidth Service connector user interface.\u00a0For general information about adding DataSets, setting update schedules, and editing DataSet information, see\n\nAdding a DataSet Using a Data Connector\n\n.\n\n\n Prerequisites\n---------------\n\nTo connect to BigQuery\u00a0and create a DataSet, you must have the following:\n\n A JSON BigQuery service account key\n* A JSON Google Cloud Storage service account key\n\n\n#####\n\n*To generate the necessary service account keys, do the following**\n :\n\n\n1. In the Google Cloud Platform Console, open the\n **IAM\u00a0& Admin**\n page.\n\n\n 2. Click\n ********Service accounts********\n .\n\n\n 3. Select your project and click\n ********Open********\n .\n\n\n 4. Click\n ********Create Service Account********\n .\n\n5. Enter a name and description for the service account.\n\n\n 6. Click\n ********Create********\n .\n\n\n 7. To grant this\u00a0service account access to the project, click\n **Select a role**\n drop down.\n\n\n 8. Click\n **Project**\n and then click\n ********Owner********\n .\n\n9. Owner will appear in the Role field.\n\n\n 10. Click\n ********Continue********\n .\n\n\n 11. Click\n ********Create key********\n .\n\n12. Select\n ********JSON********\n as the key type.\n\n13. Click\n ********Create********\n .\n\n\n A private key will be saved to your computer.\n\n####\n**Include the following permissions for the Service keys**\n :\n\n*BIGQUERY ROLES**\n\n\n bigquery.datasets.get\n\n\n bigquery.jobs.create\n\n\n bigquery.jobs.get\n\n\n bigquery.jobs.list\n\n\n bigquery.tables.list\n\n\n bigquery.tables.export\n\n\n bigquery.tables.create\n\n\n bigquery.tables.delete\n\n\n**CLOUD STORAGE ROLES**\n\n\n storage.buckets.get\n\n\n storage.buckets.list\n\n\n storage.objects.create\n\n\n storage.objects.delete\n\n\n storage.objects.get\n\n\n storage.objects.list\n\nConnecting to BigQuery\n-------------------------\n\nThis section enumerates the options in the\n ****Credentials****\n and\n ****Details****\n panes in the Google BigQuery High Bandwidth Service Connector page.\u00a0The components of the other panes in this page,\n ****Scheduling****\n and\n ****Name & Describe Your DataSet****\n , are universal across most connector types and are discussed in greater length in\n\nAdding a DataSet Using a Data Connector\n\n.\n\n\n You cannot export nested and repeated data in CSV format. Nested and repeated data are supported for Avro, JSON, and Parquet exports.\n\n\n Basically, if the customer has a column with type as ARRAY, BigQuery doesn't support unloading that data in CSV.\n\n\n The customer needs to exclude the ARRAY columns or UNNEST them to run the unload successfully\n\n##\n\nCredentials Pane\n\n\n This pane contains fields for entering credentials to connect to your\n\nBigQuery Unload\n\naccount. The following table describes what is needed for each field:\n\n\n|\n\nField\n\n|\n\nDescription\n\n|\n| --- | --- |\n|\n Service Account Key JSON BigQuery\n  |\n Enter your Google BigQuery JSON service account key.\n  |\n|\n Service Account Key JSON Google Cloud Storage\n  |\n Enter your Google Cloud Storage JSON service account key.\n  |\n\n\n Once you have entered valid credentials, you can use the same account any time you go to create a new\n\nGoogle BigQuery High Bandwidth Service\n\nDataSet. You can manage connector accounts in the\n **Accounts**\n tab in the Data Center. For more information about this tab, see\n\nManaging User Accounts for Connectors\n\n.\n\n\n###\n Details Pane\n\nThis pane contains a primary\n **Reports**\n menu, along with various other menus which may or may not appear depending on the report type you select.\n\n\n Menu\n  |\n Description\n  |\n| --- | --- |\n|\n Query\n  |\n Runs a BigQuery SQL query and returns results if the query completes.\n  |\n|\n Dataset ID\n  |\n Select the BigQuery dataset id for your data.\n\nFor more info about BigQuery datasets, visit\n\nhttps://cloud.google.com/bigquery/docs/datasets-intro\n\n.\n  |\n|\n Google Cloud Storage bucket name\n  |\n Select the Google Cloud Storage bucket name that will be used for temporary storage as we transfer your data into Domo.\n  |\n|\n Query\n  |\n Enter a query to execute. Only Standard SQL query is supported.\u00a0Example: Select \\* from Table\\_Name.\n  |\n|\n Partition query to determine partition tags\n  |\n Enter partition query to determine the distinct partition tags. The column containing the\n **Date**\n data is your partition column. Example:\u00a0Select\n **Date**\n from Table\\_Name.\n  |\n|\n Force to string\n  |\n Enter a comma separated list of fields that need to be treated as STRING in Domo.\n  |\n|\n Force to number\n  |\n Enter a comma separated list of fields that need to be treated as NUMBER in Domo.\n  |\n|\n How would you like to pull your data?\n  |\n Choose how you would like to pull your data.\n  |\n|\n Partition column name\n  |\n Enter partition column name.\u00a0The column containing the\n **Date**\n data is your partition column.\n  |\n|\n Upsert key columns\n  |\n Enter a comma separated list of upsert key column names.\n  |\n|\n Days Back\n  |\n The number of days back that you would like to get data from. Example: Specify 7 to get data for the last 7 days.\n  |\n|\n Data Rolling Window\n  |\n The data will be retained for the number of days specified. Example: Specify 60 to retain the data for 60 days.\n  |\n\n\n#####\n**Entering the SQL Query and Partition Query**\n\n####\n**Entering the Partition column name and other parameters**\n\n\n Your data will be fetched from the number of days mentioned in the\n **Days Back**\n field and will be retained for the number of days specified in the\n **Data Rolling Window**\n field.\n\n*Note:**\n When using the Partition option, your DataSet must be set to\n **Append**\n and NOT Replace.\n\n\n#####\n**Using the Upsert mode**\n\n##\n Other Panes\n\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\n\nAdding\u00a0a DataSet Using a Data Connector\n\n.\n\n*Note:**\n Currently this connector only supports date partitions.\n\n\n###\n Troublleshooting\n\n\n####\n What ff no rows of data are being returned but there are rows in the run history?\n\nGoogle API documentation has stated that BigQuery and Cloud Storage needs to follow the below rules:\n\n If the BigQuery dataset is in a specific location then Cloud Storage should be in the same location.\n* If BigQuery Dataset is in a multi-region then Cloud Storage also needs to be either in the same multi-region or in a location that is contained within that multi-region.\n\n\n Refer to\n\nCloud Storage\n\nfor the API documentation reference.\n\nFor Bucket locations, visit\n\nhttps://cloud.google.com/storage/docs/locations\n\n.\n\n", "preview_article": "\n\nClick here to preview\n\n", "url_name": "\n\n\n 360060507713\n\n\n", "summary____________________________________________briefly_describe_the_article_the_summary_is_used_in_search_results_to_help_users_find_relevant_articles_you_can_improve_the_accuracy_of_search_results_by_including_phrases_that_your_customers_use_to_describe_this_issue_or_topic": "\n\n", "article_number": "\n\n\n 000003275\n\n\n", "archived_date": "\n\n\n", "language": "\n\n\n English\n\n\n", "primary_version": "\n\nGoogle BigQuery High Bandwidth Service Connector\n\n", "article_total_view_count": "\n\n\n 4,836\n\n\n", "": "\n\n", "article_created_date": "\n\n\n 10/24/2022, 9:12 PM\n\n\n", "first_published_date": "\n\n\n 10/24/2022, 10:41 PM\n\n\n"}, "title": "google_bigquery_high_bandwidth_service_connector", "breadcrumbs": [{"text": "domo", "url": "/s/knowledge-base/"}, {"text": "connecting_data_to_domo", "url": "/s/topic/0TO5w000000ZammGAC"}, {"text": "connectors", "url": "/s/topic/0TO5w000000ZanLGAS"}, {"text": "api_connectors", "url": "/s/topic/0TO5w000000ZaoQGAS"}]}