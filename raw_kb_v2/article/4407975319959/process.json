{"file_path": "../../raw_kb_v2\\article\\4407975319959\\index.html", "content": {"title": "\n\n\n Google BigQuery High Bandwidth Unload Connector\n\n\n", "article_body": "\n\nIntro\n---------\n\nGoogle BigQuery is a server-less, highly scalable, and cost-effective cloud data warehouse designed to create, manage, share, and query data. It's a cloud-based big data analytics web service for processing very large read-only data sets. With this connector you can\n\nunload larger results from BigQuery through the assistance of the Google Cloud Storage service\n\n. Google BigQuery queries are written using a variation of the standard SQL SELECT statement. To learn more about the BigQuery API, go to\n\nhttps://cloud.google.com/bigquery/docs/reference/v2/\n\n.\n\n\n The Google BigQuery High Bandwidth Unload Connector is a \"Cloud App\" connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the connector page for this and other Cloud App connectors by clicking\n **Cloud App**\n in the toolbar at the top of the window.\n\n\n This topic discusses the fields and menus that are specific to the Google BigQuery High Bandwidth Unload Connector user interface. For general information about adding DataSets, setting update schedules, and editing DataSet information, see\n\nAdding a DataSet Using a Data Connector\n\n.\n\n\n Prerequisites\n---------------\n\nTo connect to a BigQuery High Bandwidth account,\n\nyou must have the following:\n\n\n* A Google BigQuery service account JSON key\n* A Google Cloud Storage service account JSON key\n\n\n####\n\nTo generate a Google BigQuery service account JSON key, do the following:\n\n\n1. In the\n\nGoogle Cloud Platform Console\n\n, open the\n **IAM & Admin**\n page.\n2. Click\n **Service accounts**\n in the left-hand navigation pane.\n3. Select your project and click\n **Open**\n .\n4. Click\n **Create Service Account**\n .\n5. In the Create service account pane, enter a name and description for the service account, and click\n\n*Create and continue**\n\n.\n6. In the\n\n*Grant this service account access to the project**\n section, select\n **Project > Owner**\n from the\n **Select a role**\n dropdown\n\n.\n7. Click\n **Done**\n .\n\n*Note:**\n You may need the \u201cBigQuery Admin\u201d role in the service account permissions dialog. Please consult with your Google administrator for additional guidance.\n8. To create a new JSON key, in the\n\n*Service accounts**\n\npane, click the three dots (expansion menu) and select\n\n*Manage keys**\n\n.\n9. In the\n\n*Keys**\n\nsection, click\n\n*Add Key > Create new key**\n\n.\n10. Select\n **JSON**\n as the key type and click\n **Create**\n .\n11. Click\n **Create**\n .\n\n\n A private key will be saved to your computer.\n\nConnecting to BigQuery\n------------------------\n\nThis section enumerates the options in the\n\nCredentials\n\nand\n\nDetails\n\npanes in the\u00a0Google BigQuery\u00a0Service Connector page.\u00a0The components of the other panes in this page,\n **Scheduling**\n and\n **Name & Describe Your DataSet**\n , are universal across most connector types and are discussed in greater length in\n\nAdding a DataSet Using a Data Connector\n\n.\n\n##\n Credentials Pane\n\nThis pane contains fields for entering credentials to connect to your BigQuery account. The following table describes what is needed for each field:\n\n\n Field\n  |\n Description\n  |\n| --- | --- |\n|\n Service Account Key JSON BigQuery\n  |\n Copy and paste the JSON for your BigQuery service account key.\n  |\n|\n Service Account Key JSON Google Cloud Storage\n  |\n Copy and paste the JSON for your Google Cloud Storage account key.\n  |\n\nOnce you have entered valid keys, you can use the same account any time you go to create a new Google BigQuery High Bandwidth Unload DataSet. You can manage connector accounts in the\n **Accounts**\n tab in the\n\nData Center\n\n. For more information about this tab, see\n\nManaging User Accounts for Connectors\n\n.\n\n##\n Details Pane\n\nThis pane contains a number of fields and menus you can use to configure your report.\n\n\n Menu\n  |\n Description\n  |\n| --- | --- |\n|\n\nHow would you like to import data into Domo?\n\n|\n\n  |  |\n| --- | --- |\n|\n**Import Type**\n |\n**Description**\n |\n|\n\nStandard Update\n\n|\n\nTo use standard update, select the Replace or Append mode in the scheduling section.\n\n|\n|\n Use Partitions\n  |\n\nTo use partition update, select the Append mode in the scheduling section.\n\n|\n|\n Use Upsert\n  |\n\nTo use upsert update, select the Merge mode in the scheduling section.\n\n|\n\n|\n|\n Project ID\n  |\n\nSelect the Google project Id obtained from the service key.\n\n|\n|\n\nGoogle BigQuery Dataset ID\n\n|\n Select the BigQuery dataset id for your data. For more information about BigQuery datasets, see\n\nIntroduction to DataSets\n\n.\n  |\n|\n\nGoogle Cloud Storage Bucket Name\n\n|\n\nSelect the Google Cloud Storage bucket name that will be used for temporary storage as we transfer your data into Domo.\n\n|\n|\n Query\n  |\n\nEnter a query to execute. Only StandardSQL query is supported.\n\n|\n|\n\nHow would you like to perform your partition?\n\n|\n\nSelect whether you want to perform partition by date or by meta query.\n\n|\n|\n\nPartition Meta Query to determine partition tags\n\n|\n\nEnter partition query to determine the distinct partition tags.\n\n|\n|\n\nPartition Column Name\n\n|\n\nEnter partition column name.\n\n|\n|\n\nDays Back\n\n|\n\nThe number of days back that you would like to get data. Specify 7 to get data for the last 7 days.\n\n|\n|\n Upsert Key Column(s)\n  |\n\nEnter upsert key column name or a comma separated list of upsert key column names.\n **This is Required only when the Update method is Merge**\n .\n\n|\n\n\n###\n Other Panes\n\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\n\nAdding\u00a0a DataSet Using a Data Connector\n\n.\n\n\n Troubleshooting\n-----------------\n\n\n* Ensure that the credentials have the proper access to the query the tables needed.\n* Make sure the queries are correct and calling the correctly named data sources.\n\n\n", "preview_article": "\n\nClick here to preview\n\n", "url_name": "\n\n\n 4407975319959\n\n\n", "summary____________________________________________briefly_describe_the_article_the_summary_is_used_in_search_results_to_help_users_find_relevant_articles_you_can_improve_the_accuracy_of_search_results_by_including_phrases_that_your_customers_use_to_describe_this_issue_or_topic": "\n\n", "article_number": "\n\n\n 000003213\n\n\n", "archived_date": "\n\n\n", "language": "\n\n\n English\n\n\n", "primary_version": "\n\nGoogle BigQuery High Bandwidth Unload Connector\n\n", "article_total_view_count": "\n\n\n 4,650\n\n\n", "": "\n\n", "article_created_date": "\n\n\n 10/24/2022, 9:11 PM\n\n\n", "first_published_date": "\n\n\n 10/24/2022, 10:39 PM\n\n\n"}, "title": "google_bigquery_high_bandwidth_unload_connector", "breadcrumbs": [{"text": "domo", "url": "/s/knowledge-base/"}, {"text": "connecting_data_to_domo", "url": "/s/topic/0TO5w000000ZammGAC"}, {"text": "connectors", "url": "/s/topic/0TO5w000000ZanLGAS"}, {"text": "api_connectors", "url": "/s/topic/0TO5w000000ZaoQGAS"}]}