{"file_path": "../../raw_kb_v2\\article\\360043427673\\index.html", "content": {"title": "\n\n\n Scheduling a Magic ETL DataFlow\n\n\n", "article_body": "\n\nIntro\n-------\n\nCustomize a schedule for your DataFlow runtime. With DataFlow schedules, you can continue kicking off your DataFlow when an input has updated or set a specific time at an hourly, daily, weekly, or monthly cadence. You can specify what time zone the DataSet trigger times are in as well as running on specific days of the week or month.\n\n\n Schedule Types\n------------------\n\n\n|  |  |\n| --- | --- |\n|\n Manually\n  |\n The DataFlow will not update without someone selecting the run option manually.\n  |\n|\n On a schedule\n  |\n Set exactly when the DataFlow runs by selecting both the time and frequency (hourly, daily, weekly, monthly.)\n  |\n|\n Only when DataSets are updated\n  |\n The DataFlow will run as soon as the chosen DataSet input has updated.\n  |\n\nHow to Schedule a DataFlow\n----------------------------\n\n*To schedule a\u00a0DataFlow\u00a0from the details view,**\n\n. Navigate to the Data Center and find and select the DataFlow you want to schedule.\n2. Click on the\n **Settings**\n tab.\n3. Choose the type of schedule you would like.\n\n*Note:**\n If you need your DataSet to update faster than every 15 minutes, please reach out to your account team for evaluation.\n4. Click Apply\n\n*To schedule a DataFlow from the edit view,**\n\n. Find and select the DataFlow you want to schedule.\n2. Click\n **Edit**\n to edit the DataFlow.\n3. Click the\n\nicon.\n4. Choose the type of schedule you would like.\n\n*Note:**\n If you need your DataSet to update faster than every 15 minutes, please reach out to your account team for evaluation.\n5. Click\n **Apply**\n and\n **Save**\n the DataFlow.\n\nFAQs\n------\n\n\n####\n If you have multiple inputs set to trigger a DataFlow, does it run when any of the input DataSets are updated or when all of them are updated?\n\nCurrently, if you have multiple DataSets set to trigger a DataFlow, it will run when any of those input DataSets update. If all of the inputs need to be updated before the DataFlow runs, we recommend staggering the input DataSet update times. Then, only set the input with the latest update time to trigger the DataFlow. This will ensure all the inputs have been updated before the DataFlow runs.\n\n", "preview_article": "\n\nClick here to preview\n\n", "url_name": "\n\n\n 360043427673\n\n\n", "summary____________________________________________briefly_describe_the_article_the_summary_is_used_in_search_results_to_help_users_find_relevant_articles_you_can_improve_the_accuracy_of_search_results_by_including_phrases_that_your_customers_use_to_describe_this_issue_or_topic": "\n\n", "article_number": "\n\n\n 000004577\n\n\n", "archived_date": "\n\n\n", "language": "\n\n\n English\n\n\n", "primary_version": "\n\nScheduling a Magic ETL DataFlow\n\n", "article_total_view_count": "\n\n\n 5,119\n\n\n", "": "\n\n", "article_created_date": "\n\n\n 10/24/2022, 10:15 PM\n\n\n", "first_published_date": "\n\n\n 10/24/2022, 10:41 PM\n\n\n"}, "title": "scheduling_a_magic_etl_dataflow", "breadcrumbs": [{"text": "domo", "url": "/s/knowledge-base/"}, {"text": "transforming_data_in_domo", "url": "/s/topic/0TO5w000000ZamzGAC"}, {"text": "magic_etl", "url": "/s/topic/0TO5w000000ZanvGAC"}]}