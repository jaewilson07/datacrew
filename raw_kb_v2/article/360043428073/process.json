{"file_path": "../../raw_kb_v2\\article\\360043428073\\index.html", "content": {"title": "\n\n\n Archiving Historical Data Using a DataFlow\n\n\n", "article_body": "\n\nIntro\n-------\n\n\n Let's say you\n\nput your\u00a0information into a year-to-date DataSet that\u2019s one month behind, starting by loading January\u2019s data in February. Because this is a year-to-date DataSet, once February rolls around the next year, the data from the previous year disappears from the DataSet to make way for the new current year\u2019s data. What if you wanted to archive the data into another DataSet once\u00a0December\u2019s data is loaded in the year-to-date file? As it turns out, there are a couple of ways to accomplish that. They can be done both in MySQL and Magic\n\nETL\n\n.\n\n\n This process applies to any DataSet containing any year-to-date data. Once it hits a year mark, you can archive it for historical purposes.\n\n\n Method #1\n-----------\n\n\n###\n Method #1 Using a MySQL DataFlow\n\nThis can be done using a stored procedure. This is the most flexible method and can be adapted to account for any extra logic that might need to be applied.\n\n\n The code in this stored procedure assumes a few things:\n\n The current year-to-date data will be appended to a historical archive via a recursive DataFlow.\n* This recursive DataFlow has already been created.\n* The DataFlow has been set up to automatically run when the year-to-date DataSet updates.\n\nIn the below example, the data will be coming from a DataSet called\n\nytd\\_premium\\_data\n\n, and the historical archive will be called\n\nmove\\_to\\_historical\\_method\\_1\n\n.\n\n\n Below is the code for the stored procedure:\n\n\n`CREATE PROCEDURE archive()`\n\n\n`BEGIN`\n\n\n`-- Get the max date from the premium data`\n\n\n`SELECT @maxDataDate := MAX(`Date`)`\n\n\n`FROM\u00a0\u00a0 ytd\\_premium\\_data;`\n\n\n`-- Get the max date from the archived data`\n\n\n`SELECT @maxArchiveDate := MAX(`Date`)`\n\n\n`FROM\u00a0\u00a0 move\\_to\\_historical\\_method\\_1;`\n\n\n`-- We will use these two dates later to determine whether or not the`\n\n\n`-- current year-to-date data has already been added to the historical`\n\n\n`-- archive`\n\n\n`-- The following is a shortcut to dynamically create a table structure`\n\n\n`-- that's identical to another table, but leave it with no data in it.`\n\n\n`-- We\u2019re going to use this trick to create a table with the same`\n\n\n`-- structure as the year-to-date-data`\n\n\n`-- Create a table by selecting only one row from the premium data`\n\n\n`CREATE TABLE archive\\_data`\n\n\n`SELECT\u00a0\u00a0\u00a0\u00a0 \\*`\n\n\n`FROM\u00a0\u00a0\u00a0 ytd\\_premium\\_data LIMIT 1;`\n\n\n`-- Get rid of all rows from the newly created table`\n\n\n`TRUNCATE TABLE archive\\_data;`\n\n\n`-- If the conditions are met to archive the year-to-date data, this`\n\n\n`-- new table will be populated with the year-to-date data and appended`\n\n\n`-- to the historical DataSet. If the conditions are not met, the table`\n\n\n`-- will remain empty, and an empty table will be appended to the`\n\n\n`-- historical DataSet.`\n\n\n`-- The year of data to be archived is last year's data. Remember that`\n\n\n`-- the year-to-date data is a month behind, so December\u2019s data won\u2019t`\n\n\n`-- be loaded until January of the next year.`\n\n\n`IF YEAR(CURRENT\\_DATE()) - 1 = YEAR(@maxDataDate)`\n\n\n`-- It's January, which means we should have December's data`\n\n\n`AND MONTH(CURRENT\\_DATE()) = 1`\n\n\n`-- The data to be archived contains December's data`\n\n\n`AND MONTH(@maxDataDate) = 12`\n\n\n`-- The data from the year-to-date has not already been added to the`\n\n\n`-- archive`\n\n\n`AND @maxDataDate != @maxArchiveDate`\n\n\n`-- If every condition checks out, then insert the year-to-date data to`\n\n\n`-- be archived into the archive\\_data table for unioning to the`\n\n\n`-- historical data. Because of the date checks in the previous step,`\n\n\n`-- the following code will only ever be executed once.`\n\n\n`THEN INSERT INTO archive\\_data`\n\n\n`SELECT\u00a0\u00a0\u00a0\u00a0 \\*`\n\n\n`FROM ytd\\_premium\\_data;`\n\n\n`END IF;`\n\n\n`END;`\n\n\n To call\u00a0the stored procedure...\n\n\n`CALL archive();`\n\n\n Appending year-to-date data (if it\u2019s ready to archive) to historical archive data\u00a0(step name is\n **DataFlow\\_output**\n )...\n\n\n`-- Union whatever's in archive\\_data to thie historical table. If the coniditons for moving the data weren't met, archive\\_data will be empty.`\n\nSELECT\u00a0\u00a0\u00a0\u00a0 \\*\n\nFROM move\\_to\\_historical\\_method\\_1`\n\n\n`UNION ALL`\n\n\n`SELECT\u00a0\u00a0\u00a0\u00a0 \\*`\n\n\n`FROM archive\\_data`\n\n\n This compares what\u2019s already in the archived DataSet to what\u2019s being put in. The\n\nMAX\n\ndates for the data are matched. Then, we create a table that\u2019s going to hold the data to be archived. We can use a shortcut of\n\nSELECT \\*\n\nfrom the table with a limit of 1. \u00a0Then you truncate the table. This is a quick way to create the table schema without having to go through the process manually. That way, we don\u2019t have to replicate the table schema; it does it automatically.\n\n\n Run a couple of checks:\n\n Is it the right year?\n* Is it January? (If so, December's DataSet\u00a0will be loaded.)\n* Is the month to be archived December?\n\nThen the last piece of code checks to make sure that the data to be archived hasn\u2019t already been archived. Once it\u2019s archived you don\u2019t want it to archive it again.\n\n\n I\n\n\n f it passes, it will be put in the archive data table that was created above. Then, later, all you need to do is just a simple union all with the rest of the archived data. This puts everything together. The process only archives it the one time.\n\n##\n\n\n Method #1 Using Magic\n\nThis method is not as straightforward as using MySQL, but the DataFlow uses the same logic.\n\n. To get the\n\nMAX\n\ndate in Magic, you have to do a\n **Group By**\n transform on a column that is identical in every row. So the first step is to add a column with a constant value using the\n **Add Constant**\n transform. In this case, I\u2019ve given it a numerical value of 1 and named it \"One.\" This is done twice\u2014once for the year-to-date premium data and once for the historical archive data.\n2. Insert a\n **Group By**\n transform and group by the column we just created (\"One\"), create a new column for each grouping, and select\n **Maximum**\n for the \"Date\" column. This step will also be done twice\u2014once for the year-to-date premium data and once for the historical archive data. But you need to give different names to each new aggregated column.\n3. Use a\n **Join**\n transform to join the two DataSets in Step 2 together on column \"One.\"\n\nYour results will contain the maximum date in the year-to-date data, the maximum date in the historical archive data, and two columns containing \"One.\"\n4. Use an\n **Add Constant**\n to add today\u2019s date to the other dates in the DataSet.\n5. Insert a\n **Date Operations**\n transform and create the columns\u00a0\"Current Month\" and \"Current Year\" from the column \"Today\" and \"Data Month\" and \"Data Year\" from the column \"MaxDataDate.\"\n\nYour data will now look something like this:\n\nYou now have all the dates and information you need to determine whether or not to append year-to-date data to the historical archive data.\n\n. To ensure every row in the year-to-date DataSet contains the data just created, insert a\n **Join Data**\n transform to an inner join on both DataSets and join on the column \"One.\"\n7. Using the\n **Calculator**\n transform, subtract 1 from \"Current Year\" to determine \"Prior Year.\"\n8. Add a\n **Filter Rows**\n transform to apply the logic to determine whether or not the year-to-date DataSet contains data from December and whether or not it has already been added to the historical data archive.\n\nIf this step passes all the checks, it will contain all the data from the year-to-date DataSet. If not, it will contain nothing.\n\n\n9. Use a\n **Select Columns**\n transform to get rid of all the extra columns we created to make the schema match what\u2019s in the historical data archive.\n10. Add an\n **Append Rows**\n transform to append the year-to-date DataSet (if applicable) to the historical archive DataSet. Be sure to select\n **Include all columns**\n from the dropdown. If you\u2019ve done everything correctly, a\u00a0\u201cNo changes\u201d message will appear beside each input DataSet.\n11. Under\n **Settings**\n , check the box to make sure the transform will run whenever the year-to-date DataSet gets updated.\n\nMethod #2\n-------------\n\n\n###\n\n\n Method #2 Using MySQL\n\nIf there are no complex requirements for determining whether or not to archive the year-to-date data, and you know that the year-to-date DataSet will be complete by a certain date, then you can create a DataFlow that does nothing but archive the data, and then you can use a trick to schedule to run it on a certain date every year.\n\n\n For our purposes, let\u2019s suppose that we know the year-to-date DataSet will be complete and final by January 20 every year. The first thing we will need to do is create a CSV DataSet and schedule it to update every year on January 20.\n\n. Navigate to https://\n *yourinstancename*\n .\n\ndomo.com/connectors\n\n/com.domo.connector.csv.easy.\n2. Put anything that makes sense into the box. Here we create a column called \"Date\" with a value of\n\n1800-01-01\n\n.\n3. Click\n **Next**\n to\u00a0bring\u00a0up the\n **Scheduling**\n section.\n4. Click the\n **Advanced Scheduling**\n tab.\n5. Check the appropriate\n **Month**\n (s) and\n **Days of Month**\n .\n6. Click the\n **Time**\n tab and choose a time for the job to run.\n7. Click\n **Next**\n .\n8. Give the CSV DataSet a meaningful name and click\n **Save**\n .\n\nNow that we have a DataSet that automatically updatex every January 20, we just need a MySQL DataFlow that appends the year-to-date data to the archive DataSet. We will add this CSV DataSet to the DataSets in the DataFlow, and change the settings so that the DataFlow will run once that DataSet gets updated. Because the DataSet only gets update every January 20, then this has the effect of creating a scheduled DataFlow.\n\nHere is the SQL used in the\n **DataFlow Output**\n transform:\n\n\n`-- This DataFlow won't automatically trigger until the scheduled`\n\n\n`-- update file gets updated. When that happens, it will append last`\n\n\n`-- year's data to historical data.`\n\n\n`SELECT\u00a0\u00a0\u00a0\u00a0 \\*`\n\n\n`FROM move\\_to\\_historical\\_method\\_2`\n\n\n`UNION ALL`\n\n\n`SELECT\u00a0\u00a0\u00a0\u00a0 \\*`\n\n\n`FROM ytd\\_premium\\_data`\n\n\n Finally, save, but\n **do not run the DataFlow!**\n Otherwise\u00a0it immediately archives the year-to-date data.\n\n##\n\n\n Method #2 Using\n\n\n Magic ETL\n\nThe automatically scheduled Magic ETL simply appends year-to-date data to historical archive data. To add the automatic scheduling, you have to add the Auto Update DataSet and check the setting to run the transform whenever it updates. Unlike with a MySQL DataFlow, with Magic ETL, you can\u2019t just add an input DataSet and not incorporate it into a transform because Magic will error out if you try. So you have do a couple of things to incorporate it into the DataFlow, but make sure it doesn\u2019t actually do anything to the data.\n\n\n1. Add the \"Auto Update\" DataSet as an\n **Input Dataset**\n .\n2. Insert a\n **Filter**\n transform. Since one of the columns in this DataSet is a date, set the filter to select only the records where the \"Date\" IS NULL. This will result in no rows of data, which is good\u00a0because we will later append it to the rest of the data, and because there are no rows, there will be zero effect on anything.\n\nBecause the CSV file adds two columns when it updates,\u00a0\"\\_BATCH\\_ID\\_\" and \"\\_BATCH\\_LAST\\_RUN\\_,\" we need to eliminate them from the data.\n3. Insert a\n **Select Columns**\n transform and select only \"Date.\"\n4. Insert an\n **Append**\n transform in which you select\n **Include All Columns**\n in the first dropdown.\n5. Save, but\n **do not run the DataFlow**\n ! If you run this DataFlow, it will append whatever\u2019s currently in the year-to-date DataSet to the archived data DataSet.\n\n\n", "preview_article": "\n\nClick here to preview\n\n", "url_name": "\n\n\n 360043428073\n\n\n", "summary____________________________________________briefly_describe_the_article_the_summary_is_used_in_search_results_to_help_users_find_relevant_articles_you_can_improve_the_accuracy_of_search_results_by_including_phrases_that_your_customers_use_to_describe_this_issue_or_topic": "\n\n", "article_number": "\n\n\n 000004608\n\n\n", "archived_date": "\n\n\n", "language": "\n\n\n English\n\n\n", "primary_version": "\n\nArchiving Historical Data Using a DataFlow\n\n", "article_total_view_count": "\n\n\n 4,987\n\n\n", "": "\n\n", "article_created_date": "\n\n\n 10/24/2022, 10:16 PM\n\n\n", "first_published_date": "\n\n\n 10/24/2022, 10:41 PM\n\n\n"}, "title": "archiving_historical_data_using_a_dataflow", "breadcrumbs": [{"text": "domo", "url": "/s/knowledge-base/"}, {"text": "transforming_data_in_domo", "url": "/s/topic/0TO5w000000ZamzGAC"}, {"text": "transformation_tips_and_tricks", "url": "/s/topic/0TO5w000000ZaoJGAS"}]}