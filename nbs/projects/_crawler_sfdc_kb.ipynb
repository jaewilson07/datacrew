{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use with local installs that don't have nbdev\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mdutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://domo-support.domo.com/s/article/360047400753?language=en_US\"\n",
    "BASE_URL = \"https://domo-support.domo.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacrew.crawler.crawler as dcc\n",
    "\n",
    "driver = dcc.driversetup(is_headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí§ loading https://domo-support.domo.com/s/article/360047400753?language=en_US üí§\n",
      "Page https://domo-support.domo.com/s/article/360047400753?language=en_US is loaded within 10 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jwilson1\\GitHub\\datacrew\\nbs\\projects\\../..\\datacrew\\crawler\\crawler.py:106: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 106 of the file c:\\Users\\jwilson1\\GitHub\\datacrew\\nbs\\projects\\../..\\datacrew\\crawler\\crawler.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(driver.page_source)\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import selenium.webdriver.support.expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import datacrew.crawler.crawler as dcc\n",
    "\n",
    "test_page_source = dcc.pagesource(url = URL, driver = driver , element_type= By.CLASS_NAME, element_id= \"slds-form-element\")\n",
    "# test_page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import urllib.parse as url_parse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium.webdriver\n",
    "import markdown as md\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Article:\n",
    "    base_url: str\n",
    "    soup: BeautifulSoup = field(repr=False, default=None)\n",
    "    linked_url_ls: list[str] = field(default_factory=list)\n",
    "    images_ls: list[str] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.get_linked_urls()\n",
    "\n",
    "    @classmethod\n",
    "    def get_from_url(cls, url: str, driver, base_url: str):\n",
    "        soup = dcc.pagesource(\n",
    "            driver=driver, url=url, element_type=By.CLASS_NAME, element_id=\"slds-form-element\")\n",
    "\n",
    "        return cls(soup=soup, base_url=base_url)\n",
    "\n",
    "    @staticmethod\n",
    "    def md_soup(soup, **options):\n",
    "        return md.MarkdownConverter(**options).convert_soup(soup)\n",
    "\n",
    "    def get_linked_urls(self, is_remove_query_string_parameters: bool = True):\n",
    "        self.linked_url_ls = []\n",
    "        for soup_link in self.soup.find_all(\"a\"):\n",
    "            url = soup_link.get(\"href\")\n",
    "\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            if url.startswith(\"/\"):\n",
    "                url = url_parse.urljoin(self.base_url, url)\n",
    "\n",
    "            if is_remove_query_string_parameters:\n",
    "                url = url_parse.urljoin(url, url_parse.urlparse(url).path)\n",
    "\n",
    "            if url.startswith(self.base_url) and url not in self.linked_url_ls:\n",
    "                self.linked_url_ls.append(url)\n",
    "\n",
    "        return self.linked_url_ls\n",
    "\n",
    "    def get_images(self, test_base_url: str = None , debug_prn: bool = False):\n",
    "        self.image_ls = [{\n",
    "            \"url\": f\"{self.base_url}{item.get('src')}\",\n",
    "            \"relative_url\": item.get('src'),\n",
    "            \"name\": item.get('alt') } for item in self.soup.find_all('img')]\n",
    "\n",
    "        if test_broken_url:\n",
    "           self.image_ls = [img for img in self.image_ls if img.get('url').startswith(test_base_url)]\n",
    "                \n",
    "        if debug_prn:\n",
    "            print(self.image_ls)\n",
    "        return self.image_ls\n",
    "\n",
    "\n",
    "# test_art = Article.get_from_url(url = URL, driver = driver, base_url = base_url)\n",
    "# test_art\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí§ loading https://domo-support.domo.com/s/article/360047400753?language=en_US üí§\n",
      "Page https://domo-support.domo.com/s/article/360047400753?language=en_US is loaded within 10 seconds.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import datetime as dt\n",
    "import markdownify as md\n",
    "from dataclasses import dataclass, field\n",
    "from dateutil import parser\n",
    "\n",
    "# @dataclass\n",
    "# class Article:\n",
    "#   soup: BeautifulSoup = field(repr = False)\n",
    "#   base_url:str\n",
    "\n",
    "class ArticleKB_GetSoupError:\n",
    "    def __init__(self, url):\n",
    "        super().__init__(f\"failed to retrieve soup for {url}\")\n",
    "\n",
    "class ArticleKB_ProcessSoupError(Exception):\n",
    "    def __init__(self, url, search_term):\n",
    "        super().__init__(f\"search term {search_term} does not exist in {url}\")\n",
    "\n",
    "\n",
    "@dataclass(init=False)\n",
    "class Article_KB(Article):\n",
    "    url: str\n",
    "    base_url: str\n",
    "    driver: selenium.webdriver\n",
    "\n",
    "    is_success:bool = False\n",
    "    article: Article = field(default=None, repr=False)\n",
    "    kb_soup: BeautifulSoup = field(default=None, repr=False)\n",
    "    kb_url_ls: list[str] = field(default=None)\n",
    "\n",
    "    title: str = None\n",
    "    md_str: str = field(default=None, repr=False)\n",
    "    article_id: str = None\n",
    "    views: int = None\n",
    "    created: dt.date = None\n",
    "    last_updated: dt.date = None\n",
    "\n",
    "    def __init__(self, url, base_url, driver):\n",
    "        self.url = url\n",
    "        self.base_url = base_url\n",
    "        self.driver = driver\n",
    "\n",
    "        soup = dcc.pagesource(driver=self.driver, url=self.url,\n",
    "                              element_type=By.CLASS_NAME, element_id=\"slds-form-element\")\n",
    "\n",
    "        if not soup:\n",
    "            raise ArticleKB_GetSoupError(url = self.url)\n",
    "\n",
    "        super().__init__(base_url = base_url, soup = soup)\n",
    "\n",
    "        self.article = Article(soup=soup, base_url=self.base_url)\n",
    "        self.kb_url_ls = self.article.linked_url_ls\n",
    "        \n",
    "        try:\n",
    "            self.process_kb_soup(soup)\n",
    "            self.is_success = True\n",
    "\n",
    "        except ArticleKB_ProcessSoupError as e:\n",
    "            print(e)\n",
    "\n",
    "        \n",
    "\n",
    "    def process_kb_soup(self, soup: BeautifulSoup):\n",
    "        search_term = \"slds-form-element\"\n",
    "\n",
    "        table = soup.find_all(class_=[search_term])\n",
    "\n",
    "        if not table or table == []:\n",
    "            raise ArticleKB_ProcessSoupError(url = self.url, search_term= search_term)\n",
    "\n",
    "        tarticle = []\n",
    "        for row in table:\n",
    "            # print(\"‚ù§Ô∏è\")\n",
    "\n",
    "            cells = row.find(class_=\"slds-form-element__label\")\n",
    "\n",
    "            if list(cells.strings):\n",
    "                content = row.find(class_=\"slds-form-element__control\")\n",
    "                tarticle.append((list(cells.strings)[0], content))\n",
    "\n",
    "        kb_soup = dict(tarticle)\n",
    "        self.kb_soup = kb_soup\n",
    "\n",
    "        self.title = self.article.md_soup(kb_soup.get(\"Title\"))\n",
    "\n",
    "        self.md_str = self.article.md_soup(kb_soup.get(\"Article Body\"))\n",
    "        self.article_id = self.article.md_soup(kb_soup.get(\"Article Number\"))\n",
    "        self.views = self.article.md_soup(kb_soup.get(\"Article Total View Count\"))\n",
    "        self.created = parser.parse(\n",
    "            self.article.md_soup(kb_soup.get(\"Article Created Date\"))\n",
    "        )\n",
    "\n",
    "        self.last_updated = parser.parse(\n",
    "            self.article.md_soup(kb_soup.get(\"First Published Date\"))\n",
    "        )\n",
    "\n",
    "        self.get_images(\n",
    "            test_base_url='https://domo-support.domo.com//servlet/rtaImage')\n",
    "\n",
    "        return self.kb_soup\n",
    "\n",
    "test_broken_url = 'https://domo-support.domo.com/s/knowledge-base/'\n",
    "test_cls = Article_KB(url=URL, base_url=BASE_URL, driver=driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cls.kb_soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting Jupyter Workspaces to ../kb_md\\jupyter_workspaces\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkK\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkP\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkL\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkR\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkX\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkI\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkV\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkH\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkZ\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkU\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkG\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkY\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkT\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkW\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkQ\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkJ\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkM\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkN\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005wycQ\n",
      "/servlet/rtaImage?eid=ka05w00000125tb&feoid=00N5w00000Ri7BU&refid=0EM5w000005vXkS\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../kb_md\\\\jupyter_workspaces.md'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 68\u001b[0m\n\u001b[0;32m     56\u001b[0m     frontmatter_obj \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m---\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[39m    title: \u001b[39m\u001b[39m{\u001b[39;00marticle\u001b[39m.\u001b[39mtitle\u001b[39m}\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m    url: \u001b[39m\u001b[39m{\u001b[39;00marticle\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39m    last updated: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(article\u001b[39m.\u001b[39mlast_updated)\u001b[39m}\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m    ---\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     add_frontmatter(data\u001b[39m=\u001b[39mfrontmatter_obj, file_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00moutput_path\u001b[39m}\u001b[39;00m\u001b[39m.md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m output_md(article \u001b[39m=\u001b[39;49m test_cls, debug_prn\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[10], line 66\u001b[0m, in \u001b[0;36moutput_md\u001b[1;34m(article, output_folder, debug_prn, is_create_subfolder)\u001b[0m\n\u001b[0;32m     54\u001b[0m md_file\u001b[39m.\u001b[39mcreate_md_file()\n\u001b[0;32m     56\u001b[0m frontmatter_obj \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m---\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[39mtitle: \u001b[39m\u001b[39m{\u001b[39;00marticle\u001b[39m.\u001b[39mtitle\u001b[39m}\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39murl: \u001b[39m\u001b[39m{\u001b[39;00marticle\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mlast updated: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(article\u001b[39m.\u001b[39mlast_updated)\u001b[39m}\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m---\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m---> 66\u001b[0m add_frontmatter(data\u001b[39m=\u001b[39;49mfrontmatter_obj, file_path\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00moutput_path\u001b[39m}\u001b[39;49;00m\u001b[39m.md\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36madd_frontmatter\u001b[1;34m(data, file_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_frontmatter\u001b[39m( data, file_path: \u001b[39mstr\u001b[39m ):\n\u001b[1;32m---> 12\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr+\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m md_file:\n\u001b[0;32m     13\u001b[0m         file_data \u001b[39m=\u001b[39m md_file\u001b[39m.\u001b[39mread()  \u001b[39m# Save all the file's content\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         md_file\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)  \u001b[39m# Place file pointer at the beginning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../kb_md\\\\jupyter_workspaces.md'"
     ]
    }
   ],
   "source": [
    "import mdutils\n",
    "import os\n",
    "from mdutils.mdutils import MdUtils\n",
    "import requests\n",
    "\n",
    "def convert_to_snake(text_str):\n",
    "    \"\"\"converts 'snake_case_str' to 'snakeCaseStr'\"\"\"\n",
    "\n",
    "    return text_str.replace(\" \", \"_\").lower()\n",
    "\n",
    "def add_frontmatter( data, file_path: str ):\n",
    "    with open(file_path, 'r+', encoding='utf-8') as md_file:\n",
    "        file_data = md_file.read()  # Save all the file's content\n",
    "        md_file.seek(0, 0)  # Place file pointer at the beginning\n",
    "        md_file.write(data)  # Write data\n",
    "        md_file.write('\\n' + file_data)\n",
    "\n",
    "def download_img(image_url, image_path):\n",
    "    img_data = requests.get(image_url).content\n",
    "    with open(image_path, 'wb') as handler:\n",
    "        handler.write(img_data)\n",
    "\n",
    "def output_md(\n",
    "    article: Article_KB, output_folder: str = \"../kb_md\", debug_prn: bool = False, is_create_subfolder: bool = True\n",
    "):\n",
    "\n",
    "    output_path = os.path.join(output_folder, convert_to_snake(article.title))\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"outputting {article.title} to {output_path}\")\n",
    "\n",
    "    output_index = output_path\n",
    "\n",
    "    if is_create_subfolder:\n",
    "        is_path_exist = os.path.exists(output_path)\n",
    "\n",
    "        if not is_path_exist:\n",
    "            os.makedirs(output_path)\n",
    "        \n",
    "        for img in article.image_ls:\n",
    "            download_img(image_url=img.get('url'),\n",
    "                         image_path=f\"{output_path}\\{img.get('name')}\")\n",
    "            \n",
    "            print(img.get('relative_url'))\n",
    "            article.md_str = article.md_str.replace(img.get('relative_url'), img.get('name'))\n",
    "            \n",
    "\n",
    "        output_index = f\"{output_path}\\index\"\n",
    "\n",
    "    md_file = MdUtils(file_name=output_index)\n",
    "    \n",
    "    md_file.write(article.md_str)\n",
    "\n",
    "    md_file.create_md_file()\n",
    "\n",
    "    frontmatter_obj = f\"\"\"---\n",
    "    title: {article.title}\n",
    "    url: {article.url}\n",
    "    linked_kbs:  { [ md_file.new_inline_link(link) for link in article.kb_url_ls]}\n",
    "    article_id: {article.article_id}\n",
    "    views: {article.views}\n",
    "    created_date: {str(article.created)}\n",
    "    last updated: {str(article.last_updated)}\n",
    "    ---\"\"\"\n",
    "\n",
    "    add_frontmatter(data=frontmatter_obj, file_path=f\"{output_path}.md\")\n",
    "\n",
    "output_md(article = test_cls, debug_prn= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    base_url: str\n",
    "    output_folder: str\n",
    "    urls_visited_ls: list[str]\n",
    "    urls_to_vist_ls: list[str]\n",
    "\n",
    "    driver: selenium.webdriver\n",
    "\n",
    "    def __init__(self,\n",
    "                 urls_to_visit_ls: list[str] = None,\n",
    "                 base_url=None,\n",
    "                 output_folder='kb_md',\n",
    "                 ):\n",
    "\n",
    "        self.base_url = base_url\n",
    "        self.output_folder = output_folder\n",
    "        self.urls_visited_ls = []\n",
    "        self.urls_to_visit_ls = urls_to_visit_ls\n",
    "        self.article_ls = []\n",
    "        self.driver = dcc.driversetup(is_headless=False)\n",
    "\n",
    "    def add_url_to_visit(self, url, debug_prn: bool = False):\n",
    "        if url not in self.urls_visited_ls and url not in self.urls_to_visit_ls:\n",
    "            if debug_prn:\n",
    "                print(f'adding url to list - {url}')\n",
    "\n",
    "            self.urls_to_visit_ls.append(url)\n",
    "\n",
    "    def crawl(self, url, debug_prn: bool = False):\n",
    "        if debug_prn:\n",
    "            print(f\"starting crawl - {url}\")\n",
    "\n",
    "        article = Article_KB(url=url, base_url=self.base_url, driver=driver)\n",
    "\n",
    "        for url in article.kb_url_ls:\n",
    "            self.add_url_to_visit(url=url, debug_prn=debug_prn)\n",
    "        \n",
    "        if article.is_success:\n",
    "            output_md(article=article, output_folder=self.output_folder,\n",
    "                    debug_prn=debug_prn)\n",
    "\n",
    "\n",
    "    def run(self, debug_prn: bool = False):\n",
    "        while self.urls_to_visit_ls:\n",
    "            url = self.urls_to_visit_ls.pop(0)\n",
    "\n",
    "            logging.info(f'Crawling: {url}')\n",
    "\n",
    "            try:\n",
    "                self.crawl(url, debug_prn)\n",
    "            except Exception:\n",
    "                logging.exception(f'Failed to crawl: {url}')\n",
    "            finally:\n",
    "                self.urls_visited_ls.append(url)\n",
    "\n",
    "        print('done')\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 23:10:48,049 INFO:Crawling: https://domo-support.domo.com/s/article/360047400753?language=en_US\n",
      "2023-02-23 23:10:48,053 ERROR:Failed to crawl: https://domo-support.domo.com/s/article/360047400753?language=en_US\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jwilson1\\AppData\\Local\\Temp\\1\\ipykernel_36792\\2042628069.py\", line 57, in run\n",
      "    self.crawl(url, debug_prn)\n",
      "  File \"C:\\Users\\jwilson1\\AppData\\Local\\Temp\\1\\ipykernel_36792\\2042628069.py\", line 40, in crawl\n",
      "    article = Article_KB(url=url, base_url=self.base_url, driver=driver)\n",
      "  File \"C:\\Users\\jwilson1\\AppData\\Local\\Temp\\1\\ipykernel_36792\\1665088112.py\", line 44, in __init__\n",
      "    soup = dcc.pagesource(driver=self.driver, url=self.url,\n",
      "  File \"c:\\Users\\jwilson1\\GitHub\\datacrew\\nbs\\projects\\../..\\datacrew\\crawler\\crawler.py\", line 94, in pagesource\n",
      "    driver.get(url)\n",
      "  File \"c:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 449, in get\n",
      "    self.execute(Command.GET, {\"url\": url})\n",
      "  File \"c:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 440, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"c:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 245, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools\n",
      "  (failed to check if window was closed: disconnected: not connected to DevTools)\n",
      "  (Session info: chrome=110.0.5481.104)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\t(No symbol) [0x005637D3]\n",
      "\t(No symbol) [0x004F8B81]\n",
      "\t(No symbol) [0x003FB36D]\n",
      "\t(No symbol) [0x003ECB66]\n",
      "\t(No symbol) [0x003EC889]\n",
      "\t(No symbol) [0x003FCB00]\n",
      "\t(No symbol) [0x00460AA0]\n",
      "\t(No symbol) [0x0044B216]\n",
      "\t(No symbol) [0x00420D97]\n",
      "\t(No symbol) [0x0042253D]\n",
      "\tGetHandleVerifier [0x007DABF2+2510930]\n",
      "\tGetHandleVerifier [0x00808EC1+2700065]\n",
      "\tGetHandleVerifier [0x0080C86C+2714828]\n",
      "\tGetHandleVerifier [0x00613480+645344]\n",
      "\t(No symbol) [0x00500FD2]\n",
      "\t(No symbol) [0x00506C68]\n",
      "\t(No symbol) [0x00506D4B]\n",
      "\t(No symbol) [0x00510D6B]\n",
      "\tBaseThreadInitThunk [0x75F000F9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x773C7BBE+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x773C7B8E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting crawl - https://domo-support.domo.com/s/article/360047400753?language=en_US\n",
      "üí§ loading https://domo-support.domo.com/s/article/360047400753?language=en_US üí§\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Crawler at 0x2d9a1f9fc10>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler = Crawler(urls_to_visit_ls=[URL], base_url=BASE_URL, output_folder=\"kb_md\")\n",
    "\n",
    "crawler.run(debug_prn=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b43e631a983881eee635638ba8d16a40e1a13e8bbb48ce0aff152a316858538a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
