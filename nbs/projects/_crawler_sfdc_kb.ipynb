{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use with local installs that don't have nbdev\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: cannot find Chrome binary\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x002FDCE3+50899]\n\t(No symbol) [0x0028E111]\n\t(No symbol) [0x00195588]\n\t(No symbol) [0x001B0AAB]\n\t(No symbol) [0x001AF479]\n\t(No symbol) [0x001E1FFE]\n\t(No symbol) [0x001E1CEC]\n\t(No symbol) [0x001DB6F6]\n\t(No symbol) [0x001B7708]\n\t(No symbol) [0x001B886D]\n\tGetHandleVerifier [0x00563EAE+2566302]\n\tGetHandleVerifier [0x005992B1+2784417]\n\tGetHandleVerifier [0x0059327C+2759788]\n\tGetHandleVerifier [0x00395740+672048]\n\t(No symbol) [0x00298872]\n\t(No symbol) [0x002941C8]\n\t(No symbol) [0x002942AB]\n\t(No symbol) [0x002871B7]\n\tBaseThreadInitThunk [0x767100F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77097BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77097B8E+238]\n\t(No symbol) [0x00000000]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m IMG_BASE_URL \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://domo-support.domo.com/servlet/rtaImage\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m OUTPUT_FOLDER \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../../raw_kb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m driver \u001b[39m=\u001b[39m dcc\u001b[39m.\u001b[39;49mdriversetup(is_headless\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     18\u001b[0m test_article \u001b[39m=\u001b[39m dca\u001b[39m.\u001b[39mArticle_KB(url\u001b[39m=\u001b[39mURL, driver\u001b[39m=\u001b[39mdriver, base_url\u001b[39m=\u001b[39mBASE_URL)\n",
      "File \u001b[1;32mc:\\Users\\jwilson1\\GitHub\\datacrew\\nbs\\projects\\../..\\datacrew\\crawler\\crawler.py:26\u001b[0m, in \u001b[0;36mdriversetup\u001b[1;34m(is_headless)\u001b[0m\n\u001b[0;32m     22\u001b[0m     options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--headless\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--no-sandbox\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome(options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m driver\n",
      "File \u001b[1;32mc:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:80\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m service:\n\u001b[0;32m     78\u001b[0m     service \u001b[39m=\u001b[39m Service(executable_path, port, service_args, service_log_path)\n\u001b[1;32m---> 80\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     81\u001b[0m     DesiredCapabilities\u001b[39m.\u001b[39;49mCHROME[\u001b[39m\"\u001b[39;49m\u001b[39mbrowserName\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     82\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mgoog\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     83\u001b[0m     port,\n\u001b[0;32m     84\u001b[0m     options,\n\u001b[0;32m     85\u001b[0m     service_args,\n\u001b[0;32m     86\u001b[0m     desired_capabilities,\n\u001b[0;32m     87\u001b[0m     service_log_path,\n\u001b[0;32m     88\u001b[0m     service,\n\u001b[0;32m     89\u001b[0m     keep_alive,\n\u001b[0;32m     90\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:104\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    105\u001b[0m         command_executor\u001b[39m=\u001b[39;49mChromiumRemoteConnection(\n\u001b[0;32m    106\u001b[0m             remote_server_addr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice\u001b[39m.\u001b[39;49mservice_url,\n\u001b[0;32m    107\u001b[0m             browser_name\u001b[39m=\u001b[39;49mbrowser_name,\n\u001b[0;32m    108\u001b[0m             vendor_prefix\u001b[39m=\u001b[39;49mvendor_prefix,\n\u001b[0;32m    109\u001b[0m             keep_alive\u001b[39m=\u001b[39;49mkeep_alive,\n\u001b[0;32m    110\u001b[0m             ignore_proxy\u001b[39m=\u001b[39;49m_ignore_proxy,\n\u001b[0;32m    111\u001b[0m         ),\n\u001b[0;32m    112\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquit()\n",
      "File \u001b[1;32mc:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:286\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authenticator_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_client()\n\u001b[1;32m--> 286\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_session(capabilities, browser_profile)\n",
      "File \u001b[1;32mc:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:378\u001b[0m, in \u001b[0;36mWebDriver.start_session\u001b[1;34m(self, capabilities, browser_profile)\u001b[0m\n\u001b[0;32m    376\u001b[0m w3c_caps \u001b[39m=\u001b[39m _make_w3c_caps(capabilities)\n\u001b[0;32m    377\u001b[0m parameters \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcapabilities\u001b[39m\u001b[39m\"\u001b[39m: w3c_caps}\n\u001b[1;32m--> 378\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mNEW_SESSION, parameters)\n\u001b[0;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m response:\n\u001b[0;32m    380\u001b[0m     response \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    441\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\jwilson1\\Miniconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: unknown error: cannot find Chrome binary\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x002FDCE3+50899]\n\t(No symbol) [0x0028E111]\n\t(No symbol) [0x00195588]\n\t(No symbol) [0x001B0AAB]\n\t(No symbol) [0x001AF479]\n\t(No symbol) [0x001E1FFE]\n\t(No symbol) [0x001E1CEC]\n\t(No symbol) [0x001DB6F6]\n\t(No symbol) [0x001B7708]\n\t(No symbol) [0x001B886D]\n\tGetHandleVerifier [0x00563EAE+2566302]\n\tGetHandleVerifier [0x005992B1+2784417]\n\tGetHandleVerifier [0x0059327C+2759788]\n\tGetHandleVerifier [0x00395740+672048]\n\t(No symbol) [0x00298872]\n\t(No symbol) [0x002941C8]\n\t(No symbol) [0x002942AB]\n\t(No symbol) [0x002871B7]\n\tBaseThreadInitThunk [0x767100F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77097BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77097B8E+238]\n\t(No symbol) [0x00000000]\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "import datacrew.crawler.crawler as dcc\n",
    "import datacrew.crawler.article as dca\n",
    "\n",
    "\n",
    "TEST_ARTICLE_URL = \"https://domo-support.domo.com/s/article/360047400753?language=en_US\"\n",
    "TEST_ARTICLE_URL = \"https://domo-support.domo.com/s/article/360043429913\"\n",
    "TEST_TOPIC_URL = \"https://domo-support.domo.com/s/topic/0TO5w000000ZamoGAC/creating-content-in-domo?language=en_US\"\n",
    "\n",
    "BASE_URL = \"https://domo-support.domo.com\"\n",
    "BASE_TOPIC_URL = \"https://domo-support.domo.com/s/topic/\"\n",
    "IMG_BASE_URL = \"https://domo-support.domo.com/servlet/rtaImage\"\n",
    "\n",
    "OUTPUT_FOLDER = \"../../raw_kb\"\n",
    "\n",
    "driver = dcc.driversetup(is_headless=False)\n",
    "\n",
    "test_article = dca.Article_KB(url=URL, driver=driver, base_url=BASE_URL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "## string manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_snake(text_str):\n",
    "    \"\"\"converts 'snake_case_str' to 'snakeCaseStr'\"\"\"\n",
    "\n",
    "    return text_str.replace(\" \", \"_\").lower()\n",
    "\n",
    "\n",
    "def clean_url_name(path_name):\n",
    "    valid_chars = r\"[^a-zA-Z0-9_]\"\n",
    "\n",
    "    return re.sub(valid_chars, \"\", path_name)\n",
    "\n",
    "\n",
    "def get_id_from_url(url: str, url_match: str):\n",
    "    \"\"\"use url_match\" to identify the id of an object\"\"\"\n",
    "    return url.split(url_match)[1].split(\"/\")[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process html files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def download_img(image_url, image_path, debug_prn: bool = False):\n",
    "\n",
    "    img_data = requests.get(image_url).content\n",
    "\n",
    "    with open(image_path, \"wb\") as handler:\n",
    "        if debug_prn:\n",
    "            print(f\"downloading {image_url} to {image_path}\")\n",
    "        handler.write(img_data)\n",
    "\n",
    "\n",
    "def get_images(\n",
    "    article: dca.Article,\n",
    "    output_path: str,\n",
    "    test_base_url: str = None,\n",
    "    debug_prn: bool = False,\n",
    "):\n",
    "\n",
    "    # process image soup\n",
    "    image_ls = [\n",
    "        {\n",
    "            \"url\": f\"{BASE_URL if item.get('src').startswith('/') else ''}{item.get('src')}\",\n",
    "            \"relative_url\": item.get(\"src\"),\n",
    "            \"name\": item.get(\"alt\"),\n",
    "        }\n",
    "        for item in article.soup.find_all(\"img\")\n",
    "    ]\n",
    "\n",
    "    if test_base_url:\n",
    "        image_ls = [img for img in image_ls if img.get(\"url\").startswith(test_base_url)]\n",
    "\n",
    "    # download images\n",
    "    for img in image_ls:\n",
    "\n",
    "        img_name = img.get(\"name\")\n",
    "\n",
    "        if not img_name:\n",
    "            continue\n",
    "\n",
    "        img_url = img.get(\"url\")\n",
    "        img_path = f\"{output_path}/{img_name}\"\n",
    "        img_rel_path = img.get(\"relative_url\")\n",
    "\n",
    "        if debug_prn:\n",
    "            print(\n",
    "                f\"downloading {img_url} to {img_path}.  replacing article with {img_rel_path} with {img_name}\"\n",
    "            )\n",
    "\n",
    "        download_img(image_url=img_url, image_path=img_path)\n",
    "\n",
    "        article.md_str = article.md_str.replace(img_rel_path, img_name)\n",
    "\n",
    "    return image_ls\n",
    "\n",
    "\n",
    "test_base_url = \"https://domo-support.domo.com/servlet/rtaImage\"\n",
    "test_output_path = \"../../raw_kb/adding_a_beast_mode_calculation_to_your_chart\"\n",
    "\n",
    "get_images(\n",
    "    article=test_article,\n",
    "    test_base_url=test_base_url,\n",
    "    output_path=test_output_path,\n",
    "    debug_prn=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handle category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_article(url, base_topic_url, output_folder):\n",
    "    driver = dcc.driversetup(is_headless=False)\n",
    "\n",
    "    dca_article = dca.Article_Category(url=url, driver=driver, base_url=base_topic_url)\n",
    "\n",
    "    new_category_ls = pd.DataFrame(process_article_category_hierarchy(dca_article))\n",
    "\n",
    "    new_category_ls.drop_duplicates(inplace=True)\n",
    "\n",
    "    output_file = f\"{output_folder}/category_mapping.csv\"\n",
    "\n",
    "    update_category_csv(new_category_ls=new_category_ls, output_file=output_file)\n",
    "\n",
    "\n",
    "\n",
    "def process_article_category(url: str, output_folder, url_match=\"/s/topic/\"):\n",
    "\n",
    "    url_id = get_id_from_url(url, url_match)\n",
    "\n",
    "    output_file = f\"{output_folder}/category_mapping.csv\"\n",
    "    \n",
    "    category_df = pd.read_csv(output_file, index_col = 'id')\n",
    "    # category_df[]\n",
    "\n",
    "    try:\n",
    "        return category_df.loc['100'].to_dict()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        base_topic_url = f\"{url.split(url_match)[0]}{url_match}\"\n",
    "\n",
    "        process_article(url, base_topic_url, output_folder)\n",
    "\n",
    "\n",
    "\n",
    "process_article_category(url=TEST_TOPIC_URL, output_folder=OUTPUT_FOLDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url_match=\"/s/topic/\"\n",
    "f\"{TEST_TOPIC_URL.split(test_url_match)[0]}{test_url_match}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkstemp\n",
    "from os import fdopen, remove\n",
    "import textwrap\n",
    "from shutil import move, copymode\n",
    "\n",
    "\n",
    "def dedent_frontmatter(file_path):\n",
    "    # Create temp file\n",
    "    fh, abs_path = mkstemp()\n",
    "    with fdopen(fh, \"w\") as new_file:\n",
    "        with open(file_path) as old_file:\n",
    "            count_frontmatter = 0\n",
    "            for line in old_file:\n",
    "                if count_frontmatter < 2:\n",
    "                    if \"---\" in line:\n",
    "                        count_frontmatter += 1\n",
    "                    new_file.write(textwrap.dedent(line))\n",
    "                else:\n",
    "                    new_file.write(line)\n",
    "    # Copy the file permissions from the old file to the new file\n",
    "    copymode(file_path, abs_path)\n",
    "    # Remove original file\n",
    "    remove(file_path)\n",
    "    # Move new file\n",
    "    move(abs_path, file_path)\n",
    "\n",
    "\n",
    "def add_frontmatter(front_matter, file_path: str):\n",
    "    with open(file_path, \"r+\", encoding=\"utf-8\") as md_file:\n",
    "        file_data = md_file.read()  # Save all the file's content\n",
    "        md_file.seek(0, 0)  # Place file pointer at the beginning\n",
    "        md_file.write(front_matter)\n",
    "        md_file.write(\"\\n\" + file_data)\n",
    "\n",
    "    # dedent_frontmatter(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def article_cleansing(article: dca.Article):\n",
    "    compiled = re.compile(re.escape(\"youtube-nocookie.com\"), re.IGNORECASE)\n",
    "    article.md_str = compiled.sub(\"youtube.com\", article.md_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdutils.mdutils import MdUtils\n",
    "import pandas as pd\n",
    "\n",
    "def update_categories(url, lookup_file):\n",
    "    category_lu_df = pd.read_csv(lookup_file)\n",
    "    \n",
    "    \n",
    "\n",
    "def output_md(article: dca.Article, output_index: str, debug_prn: bool = False):\n",
    "    \n",
    "    md_file = MdUtils(file_name=output_index)\n",
    "\n",
    "    md_file.write(article.md_str)\n",
    "\n",
    "    md_file.create_md_file()\n",
    "\n",
    "    frontmatter_obj = f\"\"\"---\n",
    "title : {article.title}\n",
    "categories: { [link for link in article.url_ls if '/s/topic/' in link]}\n",
    "date: {str(article.last_updated)}\n",
    "\n",
    "url : {article.url}\n",
    "linked_kbs :  { list(set([ md_file.new_inline_link(link) for link in article.url_ls]))}\n",
    "article_id : {article.article_id}\n",
    "views : {article.views}\n",
    "created_date : {str(article.created)}\n",
    "last updated : {str(article.last_updated)}\n",
    "---\"\"\"\n",
    "    if debug_prn:\n",
    "        print(f\"front_matter {output_index}\")\n",
    "\n",
    "    add_frontmatter(front_matter=frontmatter_obj, file_path=f\"{output_index}.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_qmd(output_index):\n",
    "    qmd_path = f\"{output_index}.qmd\"\n",
    "\n",
    "    if os.path.exists(qmd_path):\n",
    "        os.remove(qmd_path)\n",
    "\n",
    "    os.rename(f\"{output_index}.md\", qmd_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_html(output_html, soup):\n",
    "    with open(output_html, 'w') as f:\n",
    "        f.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def output_article(\n",
    "    article: str,\n",
    "    output_folder: str = \"../../raw_kb\",\n",
    "    debug_prn: bool = False,\n",
    "):\n",
    "    article_title = article.title\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        output_folder, clean_url_name(convert_to_snake(article_title))\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"outputing '{article_title}' to {output_path}\")\n",
    "\n",
    "    get_images(\n",
    "        article=article,\n",
    "        test_base_url=IMG_BASE_URL,\n",
    "        output_path=output_path,\n",
    "        debug_prn=debug_prn,\n",
    "    )\n",
    "\n",
    "    article_cleansing(article = article)\n",
    "\n",
    "    output_index = f\"{output_path}\\index\"\n",
    "\n",
    "    output_html(output_html = f\"{output_path}/doc.html\", soup = article.soup)\n",
    "\n",
    "    output_md(article=article, output_index=output_index, debug_prn=debug_prn)\n",
    "\n",
    "    output_qmd(output_index=output_index)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "test_article.get_images()\n",
    "output_article(article=test_article, debug_prn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import selenium.webdriver\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s %(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    base_url: str\n",
    "    output_folder: str\n",
    "    urls_visited_ls: list[str]\n",
    "    urls_to_vist_ls: list[str]\n",
    "\n",
    "    driver: selenium.webdriver\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        urls_to_visit_ls: list[str] = None,\n",
    "        base_url=None,\n",
    "        output_folder=\"../../raw_kb/\",\n",
    "    ):\n",
    "\n",
    "        self.base_url = base_url\n",
    "        self.output_folder = output_folder\n",
    "        self.urls_visited_ls = []\n",
    "        self.urls_to_visit_ls = urls_to_visit_ls\n",
    "        self.article_ls = []\n",
    "        self.driver = dcc.driversetup(is_headless=False)\n",
    "\n",
    "    def add_url_to_visit(self, url, debug_prn: bool = False):\n",
    "        if url not in self.urls_visited_ls and url not in self.urls_to_visit_ls:\n",
    "            if debug_prn:\n",
    "                print(f\"adding url to list - {url}\")\n",
    "\n",
    "            self.urls_to_visit_ls.append(url)\n",
    "\n",
    "    def crawl(self, url, debug_prn: bool = False):\n",
    "        if debug_prn:\n",
    "            print(f\"starting crawl - {url}\")\n",
    "        \n",
    "        article = None\n",
    "        if 's/topic/' in url:\n",
    "            article = dca.Article_Category(url=TOPIC_URL,\n",
    "                                 driver=driver,\n",
    "                                 base_url=BASE_TOPIC_URL,\n",
    "                                 child_recursive = True\n",
    "                                 )\n",
    "\n",
    "        article = dca.Article_KB(url=url, base_url=self.base_url, driver=driver)\n",
    "\n",
    "        for url in article.url_ls:\n",
    "            self.add_url_to_visit(url=url, debug_prn=debug_prn)\n",
    "\n",
    "        if article.is_success:\n",
    "            output_article(\n",
    "                article=article, output_folder=self.output_folder, debug_prn=debug_prn\n",
    "            )\n",
    "\n",
    "    def run(self, debug_prn: bool = False):\n",
    "        while self.urls_to_visit_ls:\n",
    "            url = self.urls_to_visit_ls.pop(0)\n",
    "\n",
    "            logging.info(f\"Crawling: {url}\")\n",
    "\n",
    "            try:\n",
    "                self.crawl(url, debug_prn)\n",
    "            except Exception:\n",
    "                logging.exception(f\"Failed to crawl: {url}\")\n",
    "            finally:\n",
    "                self.urls_visited_ls.append(url)\n",
    "\n",
    "        print(\"done\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(\n",
    "    urls_to_visit_ls=[URL], base_url=BASE_URL, output_folder= OUTPUT_FOLDER\n",
    ")\n",
    "\n",
    "crawler.run(debug_prn=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b43e631a983881eee635638ba8d16a40e1a13e8bbb48ce0aff152a316858538a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
