{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://domo-support.domo.com\"\n",
    "OUTPUT_FOLDER = \"../../raw_kb_v2\"\n",
    "\n",
    "ELEMENT_LIST = [\"topics-list\", \"blocks-list\", \"section-list\", \"article-list\", \"selfServiceArticleLayout\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class PageType_Enum(Enum):\n",
    "  article = \"/s/article/\"\n",
    "  category = \"/s/topic/\"\n",
    "  home = \"/s/knowledge-base\"\n",
    "\n",
    "\n",
    "def test_page_type(url):\n",
    "  match_page_type = next(( page_type for page_type in PageType_Enum if page_type.value in url), None)\n",
    "\n",
    "  if not match_page_type:\n",
    "    raise Exception(f'test_page_type: not matched - {url}')\n",
    "  \n",
    "  return match_page_type\n",
    "\n",
    "def extract_url_id(url, page_type):\n",
    "  url_split = url.split(page_type.value)\n",
    "\n",
    "  if url_split[1] == '':\n",
    "    url = url.split('/s/')[1]\n",
    "\n",
    "    return url[:-1] if url.endswith('/') else url\n",
    "      \n",
    "  \n",
    "  return url_split[1].split('/')[0]\n",
    "\n",
    "\n",
    "TEST_ARTICLE_URL = \"https://domo-support.domo.com/s/article/360043429913\"\n",
    "TEST_TOPIC_URL = 'https://domo-support.domo.com/s/topic/0TO5w000000ZamsGAC'\n",
    "TEST_HOME_URL = \"https://domo-support.domo.com/s/knowledge-base\"\n",
    "\n",
    "test_url_ls = [ TEST_ARTICLE_URL, TEST_TOPIC_URL, TEST_HOME_URL]\n",
    "\n",
    "# [(test_page_type(url).name, url) for url in test_url_ls]\n",
    "# [extract_url_id(url, test_page_type(url)) for url in test_url_ls]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## string functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_snake(text_str):\n",
    "    \"\"\"converts 'snake_case_str' to 'snakeCaseStr'\"\"\"\n",
    "\n",
    "    return text_str.replace(\" \", \"_\").lower()\n",
    "\n",
    "\n",
    "def clean_url_name(path_name):\n",
    "    valid_chars = r\"[^a-zA-Z0-9_]\"\n",
    "\n",
    "    return re.sub(valid_chars, \"\", path_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process html files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💤 retrieving https://domo-support.domo.com/s/topic/0TO5w000000ZamsGAC 💤\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def driversetup(is_headless: bool = True) -> webdriver:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # run Selenium in headless mode\n",
    "\n",
    "    if is_headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def pagesource(\n",
    "    url: str,\n",
    "    driver: webdriver = None,\n",
    "    element_ls: [str] = None,\n",
    "    element_type=By.ID, # or By.CLASS_NAME\n",
    "    max_sleep_time=15,\n",
    "    is_return_soup:bool = True\n",
    "):\n",
    "    \"\"\"retrieve page_source\"\"\"\n",
    "    try:\n",
    "        is_driver_close = False if driver else True\n",
    "        driver = driver or driversetup(is_headless=False)\n",
    "\n",
    "        print(f\"💤 retrieving {url} 💤\")\n",
    "        driver.get(url)\n",
    "\n",
    "        WebDriverWait(driver, timeout=max_sleep_time).until(\n",
    "            EC.any_of(\n",
    "                *[EC.presence_of_element_located(\n",
    "                    (element_type, element_id)) for element_id in element_ls]\n",
    "            ))\n",
    "\n",
    "        if is_return_soup:\n",
    "            return BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "\n",
    "        return driver.page_source\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e} -  {url} failed to load page within {max_sleep_time} seconds.  is the element represented in the element list?\")\n",
    "\n",
    "    finally:\n",
    "        if is_driver_close:\n",
    "            driver.close()\n",
    "\n",
    "\n",
    "test_page_source = pagesource( url = TEST_TOPIC_URL, element_ls = ELEMENT_LIST, element_type = By.CLASS_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/knowledge-base',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZanlGAC',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZanxGAC',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZanCGAS',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZanmGAC',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000Zao5GAC',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZaoIGAS',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZaoLGAS',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZanBGAS',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZankGAC',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZancGAC']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def get_links(soup, base_url):\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        url = link.get('href')\n",
    "\n",
    "        if not url:\n",
    "            continue\n",
    "        \n",
    "        elif url.startswith('/s/'):\n",
    "            url = urljoin(base_url, url)\n",
    "\n",
    "        elif not url.startswith(base_url):\n",
    "            continue \n",
    "\n",
    "\n",
    "        url = urljoin(url, urlparse(url).path)\n",
    "        if url.endswith('/'):\n",
    "            url = url[:-1] \n",
    "\n",
    "        url = \"/\".join(url.split('/')[:6])\n",
    "        \n",
    "        if url not in links:\n",
    "            links.append(url) \n",
    "\n",
    "    return links\n",
    "\n",
    "get_links(test_page_source, BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def write_file(file_path, content, method = 'w'):\n",
    "    with open(file_path, method, encoding='utf-8') as f:\n",
    "        f.write( content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💤 retrieving https://domo-support.domo.com/s/article/360043429913 💤\n",
      "done processing https://domo-support.domo.com/s/article/360043429913\n",
      "💤 retrieving https://domo-support.domo.com/s/topic/0TO5w000000ZamsGAC 💤\n",
      "done processing https://domo-support.domo.com/s/topic/0TO5w000000ZamsGAC\n",
      "💤 retrieving https://domo-support.domo.com/s/knowledge-base 💤\n",
      "done processing https://domo-support.domo.com/s/knowledge-base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['https://domo-support.domo.com/s/knowledge-base',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC',\n",
       "  'https://domo-support.domo.com/s/article/360043429933',\n",
       "  'https://domo-support.domo.com/s/article/360042925494',\n",
       "  'https://domo-support.domo.com/s/article/360043931814',\n",
       "  'https://domo-support.domo.com/s/article/360043429913',\n",
       "  'https://domo-support.domo.com/s/article/360043429693'],\n",
       " ['https://domo-support.domo.com/s/knowledge-base',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZanlGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZanxGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZanCGAS',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZanmGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000Zao5GAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZaoIGAS',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZaoLGAS',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZanBGAS',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZankGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZancGAC'],\n",
       " ['https://domo-support.domo.com/s/knowledge-base',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamsGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZammGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamzGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamoGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamnGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamlGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamqGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000Zan0GAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000Zan2GAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZampGAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000Zan1GAC',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZamyGAC']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def update_listing(\n",
    "        id: str,  # will be url\n",
    "        listing_file_path: str,\n",
    "        page_output_folder: str):\n",
    "\n",
    "    columns = ['id', 'updated', 'output_folder']\n",
    "    df = pd.DataFrame(columns=columns).set_index('id')\n",
    "\n",
    "    if os.path.exists(listing_file_path):\n",
    "        df = pd.read_csv(listing_file_path, index_col='id', encoding='utf-8')\n",
    "\n",
    "    df.loc[id] = [dt.datetime.now(), page_output_folder]\n",
    "\n",
    "    df.to_csv(listing_file_path)\n",
    "\n",
    "    return df.loc[id]\n",
    "\n",
    "\n",
    "def create_page_output_folder(base_output_folder_path, url):\n",
    "    page_type = test_page_type(url)\n",
    "    page_id = extract_url_id(url, page_type)\n",
    "\n",
    "    page_output_folder = os.path.join(\n",
    "        base_output_folder_path, page_type.name, page_id)\n",
    "\n",
    "    if not os.path.exists(page_output_folder):\n",
    "        os.makedirs(page_output_folder)\n",
    "\n",
    "    write_file(file_path=os.path.join(\n",
    "        page_output_folder, 'source.txt'), content=url)\n",
    "\n",
    "    return page_output_folder\n",
    "\n",
    "\n",
    "def process_url(url,\n",
    "                driver: webdriver,\n",
    "                base_output_folder: str,\n",
    "                debug_prn: bool = False):\n",
    "\n",
    "    page_source = pagesource(\n",
    "        url=url,\n",
    "        driver=driver,\n",
    "        element_ls=ELEMENT_LIST,\n",
    "        element_type = By.CLASS_NAME,\n",
    "        is_return_soup = True,\n",
    "        max_sleep_time=15)\n",
    "\n",
    "    page_type = test_page_type(url)\n",
    "\n",
    "    page_id = extract_url_id(url=url, page_type=page_type)\n",
    "\n",
    "    page_output_folder = create_page_output_folder(\n",
    "        base_output_folder,\n",
    "        url=url)\n",
    "\n",
    "    url_ls = None\n",
    "\n",
    "    if page_source:\n",
    "        write_file(os.path.join(page_output_folder, 'index.html'), content = page_source.prettify())\n",
    "\n",
    "        update_listing(id=url,\n",
    "                    listing_file_path=os.path.join(\n",
    "                        base_output_folder, 'listing.csv'),\n",
    "                    page_output_folder=page_output_folder)\n",
    "        \n",
    "        url_ls = get_links(page_source, base_url= BASE_URL)\n",
    "    \n",
    "    print(f\"done processing {url}\")\n",
    "\n",
    "    return url_ls\n",
    "\n",
    "\n",
    "driver = driversetup(is_headless=False)\n",
    "\n",
    "[process_url(url=url,\n",
    "             driver=driver,\n",
    "             base_output_folder=OUTPUT_FOLDER) for url in test_url_ls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import selenium.webdriver\n",
    "import os\n",
    "import time\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    base_url: str\n",
    "    base_output_folder: str\n",
    "\n",
    "    urls_visited_ls: list[str]\n",
    "    urls_to_vist_ls: list[str]\n",
    "    urls_error_ls: list[str]\n",
    "\n",
    "    path_to_visit: str\n",
    "    path_errors : str\n",
    "    path_visited: str\n",
    "\n",
    "    driver: selenium.webdriver\n",
    "\n",
    "    counter: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        urls_to_visit_ls: list[str] = None,\n",
    "\n",
    "        base_url=None,\n",
    "\n",
    "        base_output_folder=\"../../raw_kb/\",\n",
    "\n",
    "        is_fresh_start: bool = False\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.base_output_folder = base_output_folder\n",
    "\n",
    "        self.counter = 0\n",
    "\n",
    "        self.path_to_visit = os.path.join(\n",
    "            self.output_folder, 'crawler_to_visit.csv')\n",
    "\n",
    "        self.path_visited = os.path.join(\n",
    "            self.output_folder, 'crawler_visited.csv')\n",
    "        \n",
    "        self.path_errors = os.path.join(self.output_folder, 'crawler_errors.csv')\n",
    "\n",
    "        self.article_ls = []\n",
    "        self.driver = dcc.driversetup(is_headless=False)\n",
    "\n",
    "        if is_fresh_start:\n",
    "            \"✂️ deleting files\"\n",
    "            self._delete_file(self.path_to_visit)\n",
    "            self._delete_file(self.path_visited)\n",
    "            self._delete_file(self.path_errors)\n",
    "\n",
    "        self.urls_visited_ls = []\n",
    "        self.urls_to_visit_ls = []\n",
    "        self.urls_error_ls =[]\n",
    "\n",
    "        if not is_fresh_start:\n",
    "            self.urls_visited_ls = self._read_file_ls(self.path_visited)\n",
    "\n",
    "            urls_to_visit_ls += self._read_file_ls(self.path_errors, is_reverse = True)\n",
    "            urls_to_visit_ls += self._read_file_ls(self.path_to_visit)\n",
    "        \n",
    "        print(urls_to_visit_ls)\n",
    "\n",
    "        [self.add_url_to_visit(url) for url in list(set(urls_to_visit_ls))]\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _delete_file(file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            print(f'deleting {file_path}')\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            print(f\"{file_path} cannot be deleted\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_file_ls(file_path, is_reverse: bool = False):\n",
    "        try:\n",
    "            file = open(file_path, '+r')\n",
    "\n",
    "            if is_reverse:\n",
    "                file.reverse()\n",
    "\n",
    "            return [line.strip() for line in file]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    @staticmethod\n",
    "    def _write_file_ls(file_path, data):\n",
    "\n",
    "        file = open(file_path, 'w+', encoding=\"utf-8\")\n",
    "\n",
    "        for item in data:\n",
    "            file.write(item+\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "    def add_url_to_visit(self, url, debug_prn: bool = False):\n",
    "        if url not in self.urls_visited_ls and url not in self.urls_to_visit_ls:\n",
    "            if debug_prn:\n",
    "                print(f\"adding url to list - {url}\")\n",
    "\n",
    "            self.urls_to_visit_ls.append(url)\n",
    "\n",
    "    def crawl(self, url, debug_prn: bool = False):\n",
    "        if debug_prn:\n",
    "            print(f\"starting crawl - {url}\")\n",
    "\n",
    "    \n",
    "        url_ls = process_url(url = url, driver = self.driver,base_output_folder=self.base_output_folder )\n",
    "\n",
    "        [self.add_url_to_visit(url=url, debug_prn=debug_prn) for url in url_ls]\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, debug_prn: bool = False):\n",
    "        while self.urls_to_visit_ls:\n",
    "            url = self.urls_to_visit_ls.pop(0)\n",
    "\n",
    "            logging.info(f\"Crawling: {url}\")\n",
    "\n",
    "            try:\n",
    "                is_visited = self.crawl(url, debug_prn)\n",
    "\n",
    "                if is_visited:\n",
    "                    self.urls_visited_ls.append(url)\n",
    "                \n",
    "                if not is_visited:\n",
    "                    self.urls_error_ls.append(url)\n",
    "                \n",
    "            except Exception:\n",
    "                logging.exception(f\"Failed to crawl: {url}\")\n",
    "                self.urls_error_ls.append(url)\n",
    "            finally:\n",
    "                self.counter += 1\n",
    "                \n",
    "                if self.counter % 10 == 0:\n",
    "                    self._write_file_ls(self.path_to_visit, self.urls_to_visit_ls)\n",
    "                    self._write_file_ls(self.path_visited, self.urls_visited_ls)\n",
    "                    self._write_file_ls(self.path_errors, self.urls_error_ls)\n",
    "\n",
    "        print(\"done\")\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "crawler = Crawler(\n",
    "    urls_to_visit_ls=[TEST_HOME_URL], \n",
    "    base_url=BASE_URL, \n",
    "    output_folder= OUTPUT_FOLDER, \n",
    "    is_fresh_start = True\n",
    "    \n",
    ")\n",
    "\n",
    "pprint(crawler.__dict__)\n",
    "\n",
    "crawler.run(debug_prn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "def read_listing(\n",
    "    output_folder,\n",
    "    file_name: str,\n",
    "    ):\n",
    "\n",
    "    output_file = f\"{output_folder}/{file_name}\"\n",
    "\n",
    "    return pd.read_csv(output_file, index_col='id')\n",
    "\n",
    "\n",
    "def write_file_ls(file_path, data):\n",
    "\n",
    "    file = open(file_path, 'w+', encoding=\"utf-8\")\n",
    "\n",
    "    for item in data:\n",
    "        file.write(item+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "# articles to visited\n",
    "article_ls = list(read_listing(OUTPUT_FOLDER, 'article_listing.csv')['url'])\n",
    "write_file_ls(os.path.join(OUTPUT_FOLDER, 'crawler_visited.csv'), data = article_ls)\n",
    "\n",
    "#category to to_visit\n",
    "category_ls = list(read_listing(OUTPUT_FOLDER, 'category_listing.csv')['url'])\n",
    "write_file_ls(os.path.join(OUTPUT_FOLDER, 'crawler_to_visit.csv'), data = category_ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b43e631a983881eee635638ba8d16a40e1a13e8bbb48ce0aff152a316858538a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
