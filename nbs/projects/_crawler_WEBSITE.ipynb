{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use with local installs that don't have nbdev\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "import datacrew.crawler.crawler as dcc\n",
    "import datacrew.crawler.article as dca\n",
    "\n",
    "\n",
    "TEST_ARTICLE_URL = \"https://domo-support.domo.com/s/article/360047400753?language=en_US\"\n",
    "TEST_ARTICLE_URL = \"https://domo-support.domo.com/s/article/360043429913\"\n",
    "TEST_TOPIC_URL = \"https://domo-support.domo.com/s/topic/0TO5w000000ZamoGAC/creating-content-in-domo?language=en_US\"\n",
    "\n",
    "BASE_URL = \"https://domo-support.domo.com\"\n",
    "\n",
    "IMG_BASE_URL = \"https://domo-support.domo.com/servlet/rtaImage\"\n",
    "\n",
    "OUTPUT_FOLDER = \"../../raw_kb\"\n",
    "\n",
    "driver = dcc.driversetup(is_headless=False)\n",
    "\n",
    "test_article = dca.Article_KB(\n",
    "    url=TEST_ARTICLE_URL, driver=driver, base_url=BASE_URL)\n",
    "\n",
    "test_category = dca.Article_Category(\n",
    "    url=TEST_TOPIC_URL, driver=driver, base_url=BASE_URL)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "## string manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_snake(text_str):\n",
    "    \"\"\"converts 'snake_case_str' to 'snakeCaseStr'\"\"\"\n",
    "\n",
    "    return text_str.replace(\" \", \"_\").lower()\n",
    "\n",
    "\n",
    "def clean_url_name(path_name):\n",
    "    valid_chars = r\"[^a-zA-Z0-9_]\"\n",
    "\n",
    "    return re.sub(valid_chars, \"\", path_name)\n",
    "\n",
    "\n",
    "def get_id_from_url(url: str, url_match: str):\n",
    "    \"\"\"use url_match\" to identify the id of an object\"\"\"\n",
    "    return url.split(url_match)[1].split(\"/\")[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process html files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def download_img(image_url, image_path, debug_prn: bool = False):\n",
    "\n",
    "    img_data = requests.get(image_url).content\n",
    "\n",
    "    with open(image_path, \"wb\") as handler:\n",
    "        if debug_prn:\n",
    "            print(f\"downloading {image_url} to {image_path}\")\n",
    "        handler.write(img_data)\n",
    "\n",
    "\n",
    "def download_article_images(\n",
    "    article: dca.Article,\n",
    "    output_path: str,\n",
    "    debug_prn: bool = False,\n",
    "):\n",
    "    image_ls = article.image_ls\n",
    "\n",
    "    if not image_ls:\n",
    "        return \n",
    "\n",
    "    # download images\n",
    "    for img in image_ls:\n",
    "        img_name = img.get(\"name\")\n",
    "\n",
    "        if not img_name:\n",
    "            continue\n",
    "\n",
    "        img_url = img.get(\"url\")\n",
    "        img_path = f\"{output_path}/{img_name}\"\n",
    "        img_rel_path = img.get(\"relative_url\")\n",
    "\n",
    "        if debug_prn:\n",
    "            print(\n",
    "                f\"downloading {img_url} to {img_path}.  replacing article with {img_rel_path} with {img_name}\"\n",
    "            )\n",
    "\n",
    "        download_img(image_url=img_url, image_path=img_path)\n",
    "\n",
    "        article.md_str = article.md_str.replace(img_rel_path, img_name)\n",
    "\n",
    "    return image_ls\n",
    "\n",
    "test_output_path = \"../../raw_kb/article/adding_a_beast_mode_calculation_to_your_chart\"\n",
    "\n",
    "pd.DataFrame(download_article_images(\n",
    "    article=test_article,\n",
    "    output_path=test_output_path,\n",
    "    debug_prn=False,\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def create_output_folder(output_folder, title: str):\n",
    "    output_path = os.path.join(\n",
    "        output_folder, clean_url_name(convert_to_snake(title))\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def output_html(output_html_path, soup):\n",
    "    with open(output_html_path, 'w') as f:\n",
    "        f.write(str(soup))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handle category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "def update_listing(\n",
    "        url: str,\n",
    "        url_id: str,\n",
    "        name: str,\n",
    "        file_name: str,\n",
    "        output_folder):\n",
    "\n",
    "    output_file = f\"{output_folder}/{file_name}\"\n",
    "\n",
    "    df = None\n",
    "    if os.path.exists(output_file):\n",
    "        df = pd.read_csv(output_file, index_col='id')\n",
    "\n",
    "    else:\n",
    "        columns = ['id','name','url', 'updated']\n",
    "        df = pd.DataFrame(columns=columns).set_index('id')\n",
    "\n",
    "\n",
    "   \n",
    "    df.loc[url_id] = [name, url, dt.datetime.now()]\n",
    "\n",
    "    df.to_csv(output_file)\n",
    "\n",
    "    return df.loc[url_id]\n",
    "\n",
    "\n",
    "update_listing(url=test_category.url,\n",
    "                 url_id = test_category.url_id,\n",
    "                 name = test_category.category,\n",
    "                 file_name = 'category_mapping.csv',\n",
    "                 output_folder=os.path.join(OUTPUT_FOLDER, 'category'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdutils.mdutils import MdUtils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_category(article: dca.Article,\n",
    "                    output_folder: str,\n",
    "                    debug_prn: bool = False):\n",
    "\n",
    "    output_path = create_output_folder(os.path.join(output_folder,'category'), article.category)\n",
    "\n",
    "    output_html_path = os.path.join(output_path, 'index.html')\n",
    "    output_html(output_html_path, article.soup)\n",
    "\n",
    "    update_listing(url=article.url,\n",
    "                 url_id = article.url_id,\n",
    "                 name = article.category,\n",
    "                 file_name = 'category_listing.csv',\n",
    "                 output_folder=OUTPUT_FOLDER)\n",
    "\n",
    "    download_article_images(\n",
    "        article=article,\n",
    "        output_path= output_path)\n",
    "\n",
    "process_category(test_category, output_folder=OUTPUT_FOLDER)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdutils.mdutils import MdUtils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_article(article: dca.Article,\n",
    "                    output_folder: str,\n",
    "                    debug_prn: bool = False):\n",
    "\n",
    "    output_path = create_output_folder(os.path.join(output_folder, 'article'), article.title)\n",
    "    \n",
    "    output_html(os.path.join(output_path, 'index.html'), article.soup)\n",
    "\n",
    "    update_listing(url=article.url,\n",
    "                url_id = article.url_id,\n",
    "                name = article.title,\n",
    "                file_name = 'article_listing.csv',\n",
    "                output_folder=OUTPUT_FOLDER)\n",
    "\n",
    "    download_article_images(\n",
    "        article=article,\n",
    "        output_path= output_path)\n",
    "\n",
    "    \n",
    "\n",
    "process_article(test_article, output_folder=OUTPUT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import selenium.webdriver\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s %(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    base_url: str\n",
    "    output_folder: str\n",
    "    urls_visited_ls: list[str]\n",
    "    urls_to_vist_ls: list[str]\n",
    "\n",
    "    path_to_visit: str\n",
    "\n",
    "    driver: selenium.webdriver\n",
    "\n",
    "    counter: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        urls_to_visit_ls: list[str] = None,\n",
    "        base_url=None,\n",
    "        output_folder=\"../../raw_kb/\",\n",
    "        is_fresh_start: bool = False\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.output_folder = output_folder\n",
    "\n",
    "        self.urls_to_visit_ls = urls_to_visit_ls  \n",
    "        self.urls_visited_ls = []\n",
    "        \n",
    "        \n",
    "        self.counter = 0\n",
    "        self.path_to_visit = os.path.join(self.output_folder, 'crawler_to_visit.csv')\n",
    "        self.path_visited = os.path.join(self.output_folder, 'crawler_visited.csv')\n",
    "\n",
    "        self.article_ls = []\n",
    "        self.driver = dcc.driversetup(is_headless=False)\n",
    "        \n",
    "        if is_fresh_start:\n",
    "            \"✂️ deleting files\"\n",
    "            self._delete_file(self.path_to_visit)\n",
    "            self._delete_file(self.path_visited)\n",
    "\n",
    "        if not is_fresh_start:\n",
    "            self.urls_visited_ls += self._read_file_ls(self.path_visited)\n",
    "            self.urls_to_visit_ls += self._read_file_ls( self.path_to_visit) \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _delete_file(file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            print(f'deleting {file_path}')\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            print(f\"{file_path} cannot be deleted\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_file_ls(file_path):\n",
    "        try:\n",
    "            file = open(file_path, '+r')\n",
    "            return [line.strip() for line in file]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _write_file_ls(file_path, data):\n",
    "    \n",
    "        file = open(file_path, 'w+')\n",
    "\n",
    "        for item in data:\n",
    "            file.write(item+\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    def add_url_to_visit(self, url, debug_prn: bool = False):\n",
    "        if url not in self.urls_visited_ls and url not in self.urls_to_visit_ls:\n",
    "            if debug_prn:\n",
    "                print(f\"adding url to list - {url}\")\n",
    "\n",
    "            self.urls_to_visit_ls.append(url)\n",
    "        \n",
    "        \n",
    "\n",
    "    def crawl(self, url, debug_prn: bool = False):\n",
    "        self.counter += 1\n",
    "        if debug_prn:\n",
    "            print(f\"starting crawl - {url}\")\n",
    "        \n",
    "        article = None\n",
    "        if '/s/topic/' in url:\n",
    "            article = dca.Article_Category(url=url,\n",
    "                                 driver=driver,\n",
    "                                 base_url=self.base_url,\n",
    "                                 is_child_recursive = False\n",
    "                                 )\n",
    "            process_category(article = article, output_folder = self.output_folder)\n",
    "        \n",
    "        if '/s/article/' in url:\n",
    "            article = dca.Article_KB(url=url, base_url=self.base_url, driver=driver)\n",
    "            process_article(article = article, output_folder = self.output_folder)\n",
    "\n",
    "        if not article:\n",
    "            return\n",
    "\n",
    "        for url in article.url_ls:\n",
    "            self.add_url_to_visit(url=url, debug_prn=debug_prn)\n",
    "        \n",
    "        if self.counter % 10 == 0:\n",
    "            self._write_file_ls(self.path_to_visit, self.urls_to_visit_ls)\n",
    "            self._write_file_ls(self.path_visited, self.urls_visited_ls)\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, debug_prn: bool = False):\n",
    "        while self.urls_to_visit_ls:\n",
    "            url = self.urls_to_visit_ls.pop(0)\n",
    "\n",
    "            logging.info(f\"Crawling: {url}\")\n",
    "\n",
    "            try:\n",
    "                self.crawl(url, debug_prn)\n",
    "            except Exception:\n",
    "                logging.exception(f\"Failed to crawl: {url}\")\n",
    "            finally:\n",
    "                self.urls_visited_ls.append(url)\n",
    "\n",
    "        print(\"done\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(\n",
    "    urls_to_visit_ls=[TEST_ARTICLE_URL, TEST_TOPIC_URL], \n",
    "    base_url=BASE_URL, \n",
    "    output_folder= OUTPUT_FOLDER, \n",
    "    is_fresh_start = True\n",
    ")\n",
    "from pprint import pprint\n",
    "pprint(crawler.__dict__)\n",
    "\n",
    "crawler.run(debug_prn=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b43e631a983881eee635638ba8d16a40e1a13e8bbb48ce0aff152a316858538a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
