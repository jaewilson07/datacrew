{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Article\n",
    "description: class based approach to webcrawling\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp crawler.article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import datetime as dt\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import urllib.parse as url_parse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium.webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "import markdownify as md\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "import datacrew.crawler.crawler as dcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class Article:\n",
    "    url: str\n",
    "    base_url: str\n",
    "\n",
    "    url_entity_prefix: str = None\n",
    "\n",
    "    driver: selenium.webdriver = field(repr=False, default=None)\n",
    "    soup: BeautifulSoup = field(repr=False, default=None)\n",
    "\n",
    "    is_success: bool = False\n",
    "    id: str = None\n",
    "\n",
    "    url_ls: list[str] = field(default_factory=list)\n",
    "    image_ls: list[str] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.get_linked_urls()\n",
    "\n",
    "    @classmethod\n",
    "    def get_from_url(cls, url: str,\n",
    "                     driver: selenium.webdriver,\n",
    "                     base_url: str,\n",
    "                     url_entity_prefix: str = None,\n",
    "                     element_type=By.CLASS_NAME,\n",
    "                     element_id=\"slds-form-element\"):\n",
    "\n",
    "        soup = dcc.pagesource(\n",
    "            driver=driver, url=url, element_type=element_type, element_id=element_id, )\n",
    "\n",
    "        return cls(\n",
    "            url=url,\n",
    "            url_entity_prefix=url_entity_prefix,\n",
    "            base_url=base_url,\n",
    "            soup=soup,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def md_soup(soup, **options):\n",
    "        return md.MarkdownConverter(**options).convert_soup(soup)\n",
    "\n",
    "    def get_linked_urls(self, is_remove_query_string_parameters: bool = True):\n",
    "        self.linked_url_ls = []\n",
    "        for soup_link in self.soup.find_all(\"a\"):\n",
    "            url = soup_link.get(\"href\")\n",
    "\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            if url.startswith(\"/\"):\n",
    "                url = url_parse.urljoin(self.base_url, url)\n",
    "\n",
    "            if is_remove_query_string_parameters:\n",
    "                url = url_parse.urljoin(url, url_parse.urlparse(url).path)\n",
    "\n",
    "            if url.startswith(self.base_url) and url not in self.linked_url_ls:\n",
    "                self.linked_url_ls.append(url)\n",
    "\n",
    "        return self.linked_url_ls\n",
    "\n",
    "    def get_images(self,\n",
    "                   soup=None, # pass a soup to just exctract images from the selected content.  Default will exctract all images on the page\n",
    "                   test_base_url: str = None, # pass to limit URLs to a specific base\n",
    "                   debug_prn: bool = False):\n",
    "        \n",
    "        \"extract image urls from soup\"\n",
    "\n",
    "        soup = soup or self.soup\n",
    "\n",
    "        self.image_ls = list(set([{\n",
    "            \"url\": f\"{self.base_url if item.get('src').startswith('/') else ''}{item.get('src')}\",\n",
    "            \"relative_url\": item.get('src'),\n",
    "            \"name\": item.get('alt')} for item in soup.find_all('img')]))\n",
    "\n",
    "        if test_base_url:\n",
    "            self.image_ls = [img for img in self.image_ls if img.get(\n",
    "                'url').startswith(test_base_url)]\n",
    "\n",
    "        if debug_prn:\n",
    "            print(self.image_ls)\n",
    "        return self.image_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ArticleKB_GetSoupError(Exception):\n",
    "    def __init__(self, url):\n",
    "        super().__init__(f\"failed to retrieve soup for {url}\")\n",
    "\n",
    "\n",
    "class ArticleKB_ProcessSoupError(Exception):\n",
    "    def __init__(self, url, search_term):\n",
    "        super().__init__(f\"search term {search_term} does not exist in {url}\")\n",
    "\n",
    "\n",
    "@dataclass(init=False)\n",
    "class Article_KB(Article):\n",
    "    title: str = None\n",
    "    md_str: str = field(default=None, repr=False)\n",
    "    views: int = None\n",
    "    created: dt.date = None\n",
    "    last_updated: dt.date = None\n",
    "\n",
    "    def __init__(self, url, base_url, driver, url_entity_prefix='/s/article/'):\n",
    "        self.url = url\n",
    "        self.base_url = base_url\n",
    "        self.driver = driver\n",
    "\n",
    "        soup = dcc.pagesource(driver=self.driver, url=self.url,\n",
    "                              element_type=By.CLASS_NAME, element_id=\"slds-form-element\")\n",
    "\n",
    "        if not soup:\n",
    "            raise ArticleKB_GetSoupError(url=self.url)\n",
    "\n",
    "        super().__init__(base_url=base_url, soup=soup, url_entity_prefix=url_entity_prefix)\n",
    "\n",
    "        self.article = Article(soup=soup, base_url=self.base_url)\n",
    "        self.kb_url_ls = self.article.linked_url_ls\n",
    "\n",
    "        try:\n",
    "            self.process_kb_soup(soup)\n",
    "            self.is_success = True\n",
    "\n",
    "        except ArticleKB_ProcessSoupError as e:\n",
    "            print(e)\n",
    "\n",
    "    def process_kb_soup(self, soup: BeautifulSoup):\n",
    "        search_term = \"slds-form-element\"\n",
    "\n",
    "        table = soup.find_all(class_=[search_term])\n",
    "\n",
    "        if not table or table == []:\n",
    "            raise ArticleKB_ProcessSoupError(\n",
    "                url=self.url, search_term=search_term)\n",
    "\n",
    "        tarticle = []\n",
    "        for row in table:\n",
    "            # print(\"❤️\")\n",
    "\n",
    "            cells = row.find(class_=\"slds-form-element__label\")\n",
    "\n",
    "            if list(cells.strings):\n",
    "                content = row.find(class_=\"slds-form-element__control\")\n",
    "                tarticle.append((list(cells.strings)[0], content))\n",
    "\n",
    "        kb_soup = dict(tarticle)\n",
    "        self.kb_soup = kb_soup\n",
    "\n",
    "        self.title = self.article.md_soup(kb_soup.get(\"Title\"))\n",
    "\n",
    "        self.md_str = self.article.md_soup(kb_soup.get(\"Article Body\"))\n",
    "        self.article_id = self.article.md_soup(kb_soup.get(\"Article Number\"))\n",
    "        self.views = self.article.md_soup(\n",
    "            kb_soup.get(\"Article Total View Count\"))\n",
    "        self.created = parser.parse(\n",
    "            self.article.md_soup(kb_soup.get(\"Article Created Date\"))\n",
    "        )\n",
    "\n",
    "        self.last_updated = parser.parse(\n",
    "            self.article.md_soup(kb_soup.get(\"First Published Date\"))\n",
    "        )\n",
    "\n",
    "        self.get_images(\n",
    "            test_base_url='https://domo-support.domo.com//servlet/rtaImage')\n",
    "\n",
    "        return self.kb_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass(init=False)\n",
    "class Article_Category(Article):\n",
    "    url: str\n",
    "    base_url: str\n",
    "    driver: selenium.webdriver = field(repr=False)\n",
    "\n",
    "    is_success: bool = False\n",
    "    article: Article = field(default=None, repr=False)\n",
    "\n",
    "    category: str = None\n",
    "    category_id: str = None\n",
    "    category_description: str = None\n",
    "\n",
    "    child_category_ls: list[dict] = None\n",
    "\n",
    "    def __init__(self, url, base_url, driver, url_entity_prefix = 's/topic/'):\n",
    "        self.url = url\n",
    "        self.base_url = base_url\n",
    "\n",
    "        self.driver = driver\n",
    "\n",
    "        soup = dcc.pagesource(driver=self.driver, url=self.url,\n",
    "                              element_type=By.CLASS_NAME, element_id=\"section-list-item\")\n",
    "\n",
    "        if not soup:\n",
    "            raise dcc.ArticleKB_GetSoupError(url=self.url)\n",
    "\n",
    "        super().__init__(base_url=base_url, soup=soup, url_entity_prefix=url_entity_prefix)\n",
    "\n",
    "        self.article = Article(soup=soup, base_url=self.base_url)\n",
    "\n",
    "        self.set_category_id()\n",
    "\n",
    "        self.kb_url_ls = self.article.linked_url_ls\n",
    "\n",
    "        try:\n",
    "            self.process_kb_soup(soup)\n",
    "            self.is_success = True\n",
    "\n",
    "        except ArticleKB_ProcessSoupError as e:\n",
    "            print(e)\n",
    "\n",
    "    def set_category_id(self):\n",
    "        url_str = self.url.replace(self.base_url, '')\n",
    "        url_str = url_str.split('/')[-2]\n",
    "        self.category_id = url_str\n",
    "        return url_str\n",
    "\n",
    "    def process_kb_soup(self, soup: BeautifulSoup):\n",
    "\n",
    "        # process parent attributes\n",
    "        parent_term = \"page-header\"\n",
    "        parent = soup.find(class_=[parent_term])\n",
    "        self.category = parent.find(\"h1\").get_text()\n",
    "\n",
    "        self.category_description = parent.find(\n",
    "            \"p\") and parent.find(\"p\").get_text()\n",
    "\n",
    "        table_item_term = \"section-list-item\"\n",
    "\n",
    "        table = soup.find_all(class_=[table_item_term])\n",
    "\n",
    "        if not table or table == []:\n",
    "            raise ArticleKB_ProcessSoupError(\n",
    "                url=self.url, search_term=table_item_term)\n",
    "\n",
    "        tarticle = []\n",
    "        for row in table:\n",
    "            url = row.find(\"a\").get(\"href\")\n",
    "            if not url.startswith('/s/topic/'):\n",
    "                continue\n",
    "\n",
    "            child_id = url.split('/')[-1]\n",
    "\n",
    "            tarticle.append({'category': row.get_text(),\n",
    "                             'id':  child_id,\n",
    "                             'url': f\"{self.base_url}{id}\"}\n",
    "                            )\n",
    "\n",
    "        self.child_category_ls = tarticle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b43e631a983881eee635638ba8d16a40e1a13e8bbb48ce0aff152a316858538a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
