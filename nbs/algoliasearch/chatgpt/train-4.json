[{"objectID": "64a38506b4bd-0", "text": "TitleCreating a Job for Uploading Data Using Workbench 5.1Article BodyYou can upload data from external\u00a0data sources\u00a0to Domo after specifying connection and data information as a job, which you can save and use again.\nThe process for creating a DataSet job in Workbench 5.1 goes as follows:\nIn the Jobs\u00a0pane, you add a new job by clicking the \"+\" icon. Here you specify\u00a0basic settings for the job, including the\u00a0Domo\u00a0domain (server), the\u00a0DataSet\u00a0job name, the\u00a0method of data transport (such as Local File Provider, ODBC Connection Provider, etc.), the data reader (data source) type (such as JIRA, Excel, etc.), the DataSet name and type, and an optional DataSet description.\u00a0(The DataSet name is the only\u00a0setting you can change after creating the job. If you want to change any other settings, you will need to create a new job.) This adds an entry for the DataSet Job to your\u00a0jobs listing. However, at this stage the DataSet Job is still not ready for execution.In the Configure\u00a0subtab for the job...You indicate the source of the data in the Source menu. For example, if the data source was a local file, you would select the file path here.You specify additional\u00a0settings for the data source in the Processing\u00a0menu. These\u00a0settings differ between data source types.\u00a0Some, such as replacement variables for queries, are optional, while others are required.You choose whether you want newly uploaded data to REPLACE the existing data in the DataSet or be APPENDED (added) to the existing data. You do this in the\u00a0Update method\u00a0menu.If desired, you\u00a0can configure miscellaneous options for the job, such as transforms, notifications, excluded columns, etc. \u00a0\u00a0\u00a0\u00a0\u00a0\nFor information about the controls and field settings in Workbench, see Understanding the Workbench 5.1 User Interface.", "source": "../../raw_kb/article/creating_a_job_for_uploading_data_using_workbench_51/index.html", "title": "Creating a Job for Uploading Data Using Workbench 5.1"}, {"objectID": "64a38506b4bd-1", "text": "Video - Creating a Job in Workbench 5", "source": "../../raw_kb/article/creating_a_job_for_uploading_data_using_workbench_51/index.html", "title": "Creating a Job for Uploading Data Using Workbench 5.1"}, {"objectID": "64a38506b4bd-2", "text": "For specific information about all of the connector types available in Workbench 5.1, visit the following links:\nConnecting to Excel Data in Workbench 5.1Connecting to CSV Data in Workbench 5.1Connecting to JIRA Data in Workbench 5.1Connecting to ODBC Data in Workbench 5.1Connecting to an OLAP Cube in Workbench 5.1Connecting to XML Data in Workbench 5.1Connecting to QuickBooks Data in Workbench 5.1Connecting to JSON Data in Workbench 5.1", "source": "../../raw_kb/article/creating_a_job_for_uploading_data_using_workbench_51/index.html", "title": "Creating a Job for Uploading Data Using Workbench 5.1"}, {"objectID": "2078a314b890-0", "text": "Title\n\nCreating a Magic ETL DataFlow\n\nArticle Body\n\nIntro\nYou create Magic ETL DataFlows using an intuitive drag-and-drop interface available in the Data Center. You simply drag your DataSets onto a canvas, then add your desired tiles to indicate how the DataSets should be joined and transformed. The curved lines are color-coded according to the DataSet used as an input. The color will persist after a Join tile to show which input DataSet was the main table in the join. A huge list of tiles is available. You can combine columns, filter rows, replace text, and so on. For detailed information about tiles, see the following topics:\nMagic ETL Tiles: AggregateMagic ETL Tiles: Combine DataMagic ETL Tiles: Data ScienceMagic ETL Tiles: DataSetsMagic ETL Tiles: Dates and NumbersMagic ETL Tiles: FilterMagic ETL Tiles: PivotMagic ETL Tiles: ScriptingMagic ETL Tiles: TextMagic ETL Tiles: Utility\n\n\n \n\nImportant: There are significant behavioral differences in the new Magic ETL. Read the Behavior Changes and Feature Updates in Magic ETL article before converting mission-critical Magic ETL DataFlows.\u00a0Failure to do so may risk an unintended change to your DataFlow\u2019s behavior.\n\n\n\nVideo - Magic ETL Overview", "source": "../../raw_kb/article/creating_a_magic_etl_dataflow/index.html", "title": "Creating a Magic ETL DataFlow"}, {"objectID": "2078a314b890-1", "text": "Video - Magic ETL Overview\n\n\n\n \n\n\nImportant:\u00a0Input DataSets\u00a0in a DataFlow\u00a0cannot be restricted by PDP policies\u2014all available rows\u00a0must\u00a0pass through the DataFlow. Because of this, you must apply PDP policies to the output DataSets generated by a DataFlow.\u00a0\nWhen you build a DataFlow using an input DataSet\u00a0with PDP\u00a0policies in place,\u00a0the DataFlow breaks unless\u00a0at least one of the following criteria\u00a0applies:\nYou have an \"Admin\" security role or a custom role with \"Manage DataFlows\" enabled.You are\u00a0the DataSet owner.You are\u00a0part of the \"All Rows\" policy. This gives you access to all of the rows in the DataSet.\nFor more information about using PDP with DataFlows, see\u00a0PDP and DataFusions/DataFlows.\u00a0\n\n\n\n\nCreating Magic ETL DataFlows\nUse the steps in this section to help you create Magic ETL DataFlows.\n\nTo create a Magic ETL DataFlow,\nIn Domo,\u00a0click\u00a0Data in the toolbar at the top of the screen.Click ETL in the Magic Transform toolbar at the top of the window.\n\n\n \n\n\nTip: You can also open the Magic ETL editor from anywhere in Domo by selecting \u00a0Add in the navigation header and selecting Data > ETL.", "source": "../../raw_kb/article/creating_a_magic_etl_dataflow/index.html", "title": "Creating a Magic ETL DataFlow"}, {"objectID": "2078a314b890-2", "text": "Add and configure an Input DataSet by doing the following:In the Tiles panel, expand DataSets, then drag Input DataSet\u00a0to the canvas.Click the Input DataSet tile, then select the DataSet to transform.Add an Output DataSet by doing the following:In the Tiles\u00a0panel, in DataSets, drag Output DataSet to the canvas.You can configure the Output DataSet tile after you connect a tile\u00a0to it.Drag other tiles from the Tiles panel to the canvas to transform (clean, aggregate, join, etc.)\u00a0the input DataSets.For more information, see the following:Magic ETL Tiles: AggregateMagic ETL Tiles: Combine DataMagic ETL Tiles: Data ScienceMagic ETL Tiles: DataSetsMagic ETL Tiles: Dates and NumbersMagic ETL Tiles: FilterMagic ETL Tiles: PivotMagic ETL Tiles: ScriptingMagic ETL Tiles: TextMagic ETL Tiles: Utility\n\n\n \n\n\nTip: You can search for items listed\u00a0in the tooltip text to help you find the tile that you need.\n\n\n\nDraw connections between the transform tiles to sequence operations in the transformation flow.Configure each tile, by clicking the tile, then specifying the options.\u00a0\n\n\n \n\n\nTip:\u00a0You can get help on a tile in the canvas by clicking the tile, then clicking .\u00a0You can also select a number of tiles at once by clicking on the canvas then dragging the mouse pointer over them. Once multiple tiles are selected, you can drag all of the selected tiles as a group to where you want them. You can also delete the selected tiles by clicking Delete in the panel on the left side of the screen.", "source": "../../raw_kb/article/creating_a_magic_etl_dataflow/index.html", "title": "Creating a Magic ETL DataFlow"}, {"objectID": "2078a314b890-3", "text": "Configure the Output DataSet tile by doing the following:Connect a tile\u00a0to the Output DataSet tile.Click the Output DataSet tile, then specify the name of the new DataSet to output.(Optional) Configure settings for when the\u00a0transformation flow runs.By default, the transformation flow runs only when you run it manually. You can schedule the Magic ETL DataFlow to run whenever the specified input DataSets change or at a set time. For detailed instructions on how to do this, see Scheduling a Magic ETL DataFlow.\u00a0Specify the name and description of the Magic ETL DataFlow.Click\u00a0Save to save the Magic ETL DataFlow, enter a version description if desired, then click Save to confirm.\nWhen you save a DataFlow, an entry for this version is added to the\u00a0Versions tab in the Details view for the DataFlow. If you entered a description when saving, that description is shown in the entry for the DataFlow. For more information about versions, see Viewing the Version History for a DataFlow.\n\n\n \n\nNote: Many users ask why output DataSets for a DataFlow are not marked as \"Updated\" when the DataFlow runs successfully. This is usually because the data has not actually changed\u2014no update has occurred. Therefore, the DataSets do not show as updated.", "source": "../../raw_kb/article/creating_a_magic_etl_dataflow/index.html", "title": "Creating a Magic ETL DataFlow"}, {"objectID": "2078a314b890-4", "text": "Best Practices for Creating DataFlows\nEach DataFlow should...\nonly include DataSets\u00a0that are necessary for the output DataSet.filter out rows you don't need at the beginning of the DataFlow.reduce the number of columns to only those you need.include descriptive names for each tile in the DataFlow.include a description of the DataFlow that lists:the input DataSets being merged or manipulatedthe DataSet being createdthe owner of the DataSetsbe named the same as the output DataSet\u2014Because the outputs of a DataFlow become their own DataSet in the Data Center, this allows for easy identification of which DataSets are produced by which DataFlows.be aware that some tiles take longer than others, including:Group ByJoin DataRemove DuplicatesPivotRank & WindowScriptingData Science", "source": "../../raw_kb/article/creating_a_magic_etl_dataflow/index.html", "title": "Creating a Magic ETL DataFlow"}, {"objectID": "5b64c0511aa4-0", "text": "Title\n\nCreating a Map with Regions\n\nArticle Body\n\nIntro\nUsing Domo's\u00a0Custom Regions tool, you can set up a map with regional groupings that match your business regions.\nFor example, let's say you manage a number of time share properties in the United States. These properties are divided across five distinct regions\u2014West, Southwest, Midwest, Southeast, and Northeast. You want\u00a0to be able to see at a glance the revenue gained from each region, something like the following:However, Domo does not include a preinstalled map for regions of the United States (and if it did, the states in each grouping would probably not exactly correspond to the states in the regions in your company).\nThe solution is to set up custom regional groupings. There are two ways to do this in Domo:\nCustomize an SVG map of the United States by adding grouping tags, as discussed in detail in\u00a0Custom Charts. This requires a rudimentary knowledge of XML.Use the Custom Regions tool to upload a DataSet\u00a0that includes a column for the new custom regions matched with the sub-regions within them. This solution requires a simple understanding of Domo dataset structure; however, knowledge of XML is not required.\u00a0\nThis topic discusses the second option, including how to set up your DataSets properly. For an in-depth discussion of building custom charts from scratch, see\u00a0Custom Charts.\u00a0\nVideo - Custom Map Regions", "source": "../../raw_kb/article/creating_a_map_with_regions/index.html", "title": "Creating a Map with Regions"}, {"objectID": "5b64c0511aa4-1", "text": "Setting Up Your DataSet\nBefore you can incorporate regions into a map, you must first ensure that your DataSet\u00a0is set up properly.\u00a0The DataSet\u00a0must contain the following columns for regions to work:\nA column with the names of the regions (i.e. the larger geographical areas your sub-regions will be divided between). For example, in the U.S. map shown above, there are 5 regions\u2014a Southwestern region that includes Arizona, New Mexico, Texas, and Oklahoma; a Western region with California, Nevada, Idaho, etc.; and so on.A column with the names of the sub-regions (i.e. the smaller geographical areas that will be divided across the larger areas). Sub-regions in the above map include individual states such as California, Texas, Alaska, etc.\u00a0\u00a0\nSo the DataSet\u00a0used to divide the above U.S. map into regions\u00a0would include columns that look something like the following:\u00a0\n\nAs another example, here is a portion of a DataSet\u00a0used to divide a map of Canada into western, central, and eastern regions:\n\nThis DataSet\u00a0matches regions with three-digit FSA codes, though you could also use provinces if you wanted.\u00a0\nThe resulting map would look as follows (with shading based on a value column\u00a0applied afterward in the Analyzer):\u00a0\u00a0\n\nAdding Regions to a Map\nOnce you have all of the necessary columns in your DataSet, you are ready to create a new regional map in your Domo instance. You must have an \"Admin\" default security profile or a custom role with \"Manage All Company Settings\" to add maps to Domo. For more information about default security roles, see\u00a0Managing Custom Roles.", "source": "../../raw_kb/article/creating_a_map_with_regions/index.html", "title": "Creating a Map with Regions"}, {"objectID": "5b64c0511aa4-2", "text": "Note: If you want to regionalize a map for a country\u00a0not available in Domo,\u00a0you must first upload an SVG\u00a0map for that country. You can download SVG country\u00a0maps\u00a0from any of a number of websites, such as\u00a0http://www.amcharts.com/svg-maps/\u00a0and\u00a0https://simplemaps.com/resources/svg-maps. These sites also include maps for overseas territories such as French Guiana, Aruba, and so on.\u00a0For more information about downloading SVG maps, see\u00a0Obtaining an SVG Chart.", "source": "../../raw_kb/article/creating_a_map_with_regions/index.html", "title": "Creating a Map with Regions"}, {"objectID": "5b64c0511aa4-3", "text": "To regionalize a map,", "source": "../../raw_kb/article/creating_a_map_with_regions/index.html", "title": "Creating a Map with Regions"}, {"objectID": "5b64c0511aa4-4", "text": "Select More\u00a0> Admin in the toolbar at the top of the screen.In Admin Settings, select\u00a0Company Settings > Custom Charts.Click\u00a0Add Chart.Click\u00a0Create regions.Click on the map you want to add regions to, then click\u00a0Next. \tIf the map you want to regionalize\u00a0has been added to Domo\u00a0using the Custom Charts tool, you can find it by switching to \"Custom Charts\" in the menu at the top of the dialog.   \tIf the country map you want is not in Domo, you must first upload it. (See the note at the top of this section.)\u00a0In the DataSet\u00a0dialog, use the search and filter options to locate the DataSet\u00a0with your regions data, then click\u00a0Select DataSet. \tIf the DataSet you need is not yet available in Domo, you can upload it by clicking\u00a0Add DataSet\u00a0then configuring the connector you want to use to pull data. For more information, see\u00a0Adding a DataSet Using a Data Connector. \tAfter you click\u00a0Select DataSet, a new window opens in which you can select your region and sub-region columns.In the\u00a0Select new custom", "source": "../../raw_kb/article/creating_a_map_with_regions/index.html", "title": "Creating a Map with Regions"}, {"objectID": "5b64c0511aa4-5", "text": "your region and sub-region columns.In the\u00a0Select new custom region\u00a0menu, select the name of the column containing your regions\u00a0(i.e. the larger geographical areas your sub-regions will be divided between).\u00a0  In the\u00a0Select column for subregions\u00a0menu, select the name of the column containing your subregions\u00a0(i.e. the smaller geographical areas that will be divided across the larger areas).\u00a0   \tAfter you make selections in both menus, your map should update to show the new regional areas.\u00a0Click\u00a0Next. \tA new window now opens in which you can choose from two configuration options.Uncheck the Show lines separating sub-regions\u00a0box if you want to remove the borders between your subregions; otherwise leave this box checked. \tThe following screenshot shows an example of this, in which the lines bordering the U.S. states have been removed:  Uncheck the\u00a0Show sub-regions not included in DataSet\u00a0box if you want to hide regions not included in the DataSet; otherwise leave this box checked. \tThe following screenshot shows an example of this, in which the", "source": "../../raw_kb/article/creating_a_map_with_regions/index.html", "title": "Creating a Map with Regions"}, {"objectID": "5b64c0511aa4-6", "text": "screenshot shows an example of this, in which the \"Northeastern\" and \"Southeastern\" regions do not appear because there is no data for them in the DataSet. This is an excellent option to use when you want to show map data only for specific regions of a given country.\u00a0  Click\u00a0Next.In the new window that appears, enter a name for the map.\tThis is the name of the map as it will appear in the Chart Picker.\u00a0(Optional) Enter a description for the map if desired.Click\u00a0Save.", "source": "../../raw_kb/article/creating_a_map_with_regions/index.html", "title": "Creating a Map with Regions"}, {"objectID": "5b64c0511aa4-7", "text": "The map should now be available in the Chart Picker in Analyzer, under\u00a0Custom Charts.\nBuilding Your Map in Analyzer\u00a0\nAfter you upload your map to Domo, you can build a card from it in Analyzer just as you would any other country map. You can access the new map in Custom Charts\u00a0in the Chart Picker. For instructions about building country maps, see\u00a0Country Map.\u00a0\nChart properties for custom maps are the same as those used in most other geographical maps. For a list of these properties, see\u00a0Properties for Maps.", "source": "../../raw_kb/article/creating_a_map_with_regions/index.html", "title": "Creating a Map with Regions"}, {"objectID": "3a50d796031e-0", "text": "Title\n\nCreating a New List in the Customer Portal\n\nArticle Body\n\nIn the Domo Customer Portal, you can create lists of cases based on different criteria. This is especially useful for Admin-level users who want to be able to view just their own cases instead of the entire list of cases in their Domo instance.\u00a0This walkthrough shows you how to do this.\nTo create a list in the Customer Portal,\nAccess the Customer Portal through support.domo.com.In the Portal, click the gear icon in the button bar in the top right corner.  Select\u00a0New\u00a0in the gear menu.  Enter a name for the new list.Leave the default setting of Only I can see this list view.Click\u00a0Save.  Add the desired filters.    In this example, an Admin sets a filter allowing them to see only the cases they created as a specific user.  Click\u00a0Done.  Click\u00a0Save.\u00a0\u00a0  \nYour new list is now created.\nFor more information about the Support Portal, see\u00a0Getting Help.", "source": "../../raw_kb/article/creating_a_new_list_in_the_customer_portal/index.html", "title": "Creating a New List in the Customer Portal"}, {"objectID": "491d4e00411f-0", "text": "Title\n\nCreating a Project\n\nArticle Body\n\nYou can add a new project in the\u00a0Projects & Tasks\u00a0page. Once you have specified the basic information for the project, including the name, team members, privacy level (either public or private), and the optional description and due date, you can begin populating the project with tasks and giving assignments to team members.\nVideo\u00a0- Creating and Managing Projects\u00a0\n\nTo create a new project,\nOpen the\u00a0Projects & Tasks\u00a0page by selecting\u00a0Projects and Tasks\u00a0in the\u00a0More menu at the top of the screen.Click\u00a0Add Project. A\u00a0Create New Project\u00a0dialog opens.\n\n\n \n\n\nTip: You can also open this dialog from anywhere in Domo by selecting  in the app toolbar and selecting Tasks > Project.\n\n\nEnter the project name.Enter the team members for the project. As you enter letters, user names containing matching letters appear in a pop-up list. When you select a name, it is added to the field. You can add as many names to the field as you want.Select a privacy setting. If you select \"Public,\" anyone in your company can view, access, and edit this project. If you select \"Private,\" only the team members you selected for the project in step 4 can view and access the project.(Optional) Enter a project description.(Optional) Enter a due date for the project.Click\u00a0Done.", "source": "../../raw_kb/article/creating_a_project/index.html", "title": "Creating a Project"}, {"objectID": "e687acd74955-0", "text": "TitleCreating a Rank or Row CountArticle BodyTo\u00a0create a rank or row count for a DataSet, you can\u00a0use a MySQL or RedShift DataFlow.\nIn MySQL\nType the following code in your DataFlow:\nSelect\n\u00a0a.*\n,@rank:= @rank + 1 AS `Rank` \nFROM your_dataset a,(SELECT @rank:=0) b\nORDER BY ___  \u00a0\nIn\u00a0MySQL Windowed\nYou can also create a windowed row count using variables. MySQL evaluates the FROM and ORDER BY portions of the query before evaluating the SELECT portion. Each column in the SELECT is then evaluated in order. You can take advantage of this order of evaluation to replicate the ROW_NUMBER and other windowing functions.\nType the following code in your DataFlow:\nSELECT\na.`Name`,\na.`Amount`,\nCASE WHEN a.`Name` = @PriorRecordName then @RowNumber := @RowNumber + 1\n\u00a0WHEN a.`Name` <> @PriorRecordName then @RowNumber := 1\nEND AS 'Windowed Row Number',\n@PriorRecordName := a.`Name`\nFROM `mydatatable` a, (SELECT @PriorRecordName := '', @RowNumber := 0) AS b\nORDER BY a.`Name, a.`Amount\nIn this query, the variables @PriorRecordName and @RowNumber are initialized before the SELECT portion of the query. The variables are initialized with the empty string and 0, respectively. Then, each column in the SELECT statement is evaluated in order. This method relies on the data being sorted correctly, so it is essential to include the `Name` and `Amount` columns in the ORDER BY of the query.", "source": "../../raw_kb/article/creating_a_rank_or_row_count/index.html", "title": "Creating a Rank or Row Count"}, {"objectID": "e687acd74955-1", "text": "The CASE statement determines how the `Windowed Row Number` is set. If the value of `Name` matches the value of the @PriorRecordName, then @RowNumber is incremented by one and that value is returned as the `Windowed Row Number` column. However, if `Name` does not match @PriorRecordName, then @RowNumber receives the value 1 and that value is returned as the `Windowed Row Number`. Finally, the @PriorRecordName variable is set to the `Name` field value.\nIn its entirety, the query is returning rows in order, checking to see if the value of `Name` in the current row is the same as that in the prior row, and assigning a `Windowed Row Number` value accordingly.\u00a0\nBy performing the query above, your output should look like this:", "source": "../../raw_kb/article/creating_a_rank_or_row_count/index.html", "title": "Creating a Rank or Row Count"}, {"objectID": "e687acd74955-2", "text": "You can create\u00a0 a windowed row count using variables without a CASE statement by typing the following code in your DataFlow:\nSelect\na.*\n,@rank:=\u00a0 IF(@prev = a.`column`,@rank + 1,1) AS `Rank` \n,@prev:= a.`column`\nFROM your_dataset a,(SELECT @rank:=0, @prev:=\u2019\u2019) b \n\nIn RedShift\nEnter the following input code:\nROW_NUMBER () OVER\n(\n[ PARTITION BY expr_list ]\n[ ORDER BY order_list ]\n)", "source": "../../raw_kb/article/creating_a_rank_or_row_count/index.html", "title": "Creating a Rank or Row Count"}, {"objectID": "3b78e8edbd48-0", "text": "TitleCreating a Recursive/Snapshot DataFlow in Magic ETLArticle BodyIntro\nA \"recursive\" or \"snapshot\" DataFlow is a DataFlow that uses itself as an input.\u00a0\nDataFlows (both SQL and Magic ETL) cannot append data natively like Connectors. However, if you need to create a DataFlow that appends data, you can do so by running it once and then using the output as part of the input for the next run. By doing this, every time the DataFlow runs, it includes the data from before and also appends the new data onto itself.\n\n\n\n\n\nImportant: There are significant behavioral differences in Magic ETL. Read the Behavior Changes and Feature Updates in Magic ETL\u00a0article before converting mission-critical Magic ETL DataFlows.\u00a0Failure to do so may cause an unintended change to your DataFlow\u2019s behavior.\n\n\n\n\nVideo\u00a0- What is a Recursive DataFlow?\n\n \n\n\u00a0\n\n\n \n\n\nImportant: If a recursive DataFlow is edited incorrectly, you could lose ALL historical data. To avoid this any time you are editing, create an additional DataSet that is a copy of your historical DataSet. This DataSet will remain static. If anything happens to your historical DataSet, you will have a backup from before you began editing.", "source": "../../raw_kb/article/creating_a_recursivesnapshot_dataflow_in_magic_etl/index.html", "title": "Creating a Recursive/Snapshot DataFlow in Magic ETL"}, {"objectID": "3b78e8edbd48-1", "text": "To create a recursive DataFlow in Magic ETL:", "source": "../../raw_kb/article/creating_a_recursivesnapshot_dataflow_in_magic_etl/index.html", "title": "Creating a Recursive/Snapshot DataFlow in Magic ETL"}, {"objectID": "3b78e8edbd48-2", "text": "Create and run a Magic\u00a0ETL DataFlow.Once the DataFlow has finished running, load the output DataSet as an input DataSet.The Output tile\u00a0will show the output DataSet name followed by \"1.\"You should now have two DataSets in the DataFlow\u2014the updating original DataSet and the historical DataSet.You now need to find a column to use as a constraint. This helps determine when to replace data in your historical DataSet with new data. Constraint columns are normally ID columns or date columns or have other unique identifiers. In this example, we use the `Date` column as a constraint.Use Select Columns to select only the constraint column.Use Remove Duplicates to return a unique list of constraints.Use an Outer Join to combine your new data to the historical DataSet. If you selected the historical DataSet on the left side of the join, as shown below, use a Left Outer Join. If you select the historical DataSet on the right side of the join, use a Right Outer Join. (Do not select Inner Join, as this could result in a loss of data.) If the", "source": "../../raw_kb/article/creating_a_recursivesnapshot_dataflow_in_magic_etl/index.html", "title": "Creating a Recursive/Snapshot DataFlow in Magic ETL"}, {"objectID": "3b78e8edbd48-3", "text": "this could result in a loss of data.) If the date columns are named the same you will notice a warning in Step 3. If you are doing a Left Outer Join then you can auto-fix the right table, and if you are doing a Right Outer Join you can auto-fix the left table. This will add the conflicting column to the Alterations section and will allow you to rename the column if you would like to keep it or drop it altogether. In this situation, we will rename it to \u201cDelete if not null\u201d for our next step.Filter any rows from your DataSet\u00a0where the new date column is not null.This returns only rows from the historical DataSet that do not exist in the new updating DataSet. You can do this by adding a Filter Rule or by adding a Formula Rule as shown in the examples below.Add Filter Rule:Add Formula Rule:Use Select Columns to remove the extra date column.Use Append to join the historical DataSet and new updating DataSet. If all steps have been done correctly, both DataSets will show \"No changes.\"Make sure", "source": "../../raw_kb/article/creating_a_recursivesnapshot_dataflow_in_magic_etl/index.html", "title": "Creating a Recursive/Snapshot DataFlow in Magic ETL"}, {"objectID": "3b78e8edbd48-4", "text": "correctly, both DataSets will show \"No changes.\"Make sure not to forget to choose how you would like the data to be handled if the data types don't match between columns that should be combined. For more information about this option you can find it here under the \"Append Rows (Union)\" section: Behavior Changes and Feature Updates in Magic ETL.Connect the Append tile to your output. Once complete, your ETL should look like the following:", "source": "../../raw_kb/article/creating_a_recursivesnapshot_dataflow_in_magic_etl/index.html", "title": "Creating a Recursive/Snapshot DataFlow in Magic ETL"}, {"objectID": "3b78e8edbd48-5", "text": "Troubleshooting/FAQ\nSee Top 5 issues Users Experience with DataFlows\u00a0to see common issues and errors seen when building DataFlows.", "source": "../../raw_kb/article/creating_a_recursivesnapshot_dataflow_in_magic_etl/index.html", "title": "Creating a Recursive/Snapshot DataFlow in Magic ETL"}, {"objectID": "77628eff55d2-0", "text": "Title\n\nCreating a Recursive/Snapshot Old Magic ETL DataFlow\n\nArticle Body\n\nA \"recursive\" or \"snapshot\" DataFlow is a DataFlow that uses itself as an input.\u00a0\nDataFlows (neither SQL nor Magic ETL types) cannot append data natively like connectors. However, if you need to create a DataFlow that appends data, you can do so by running it once and then using the output as part of the input for the next run. This way, every time the DataFlow runs, it includes the data from before and also appends the new data onto itself.\u00a0\n\n\n \n\nImportant: If a recursive DataFlow is edited incorrectly, you could lose ALL historical data. To avoid this any time you are editing, create an additional DataSet that is a copy of your historical DataSet. This DataSet will remain static. If anything happens to your historical DataSet, you will have a backup from before you began editing.\u00a0 \n\n\n\nVideo - What is a Recursive DataFlow?", "source": "../../raw_kb/article/creating_a_recursivesnapshot_old_magic_etl_dataflow/index.html", "title": "Creating a Recursive/Snapshot Old Magic ETL DataFlow"}, {"objectID": "77628eff55d2-1", "text": "To create a recursive DataFlow in Magic ETL,\n\u00a0\nCreate and run a Magic ETL DataFlow.Once the DataFlow has finished running, load the output DataSet as an input DataSet.The DataSet name will show the output DataSet name followed by \"1.\"You should now have two DataSets in the DataFlow\u2014the updating original DataSet and the historical DataSet.You now need to find a column to use as a constraint. This helps determine when to replace data in your historical DataSet with new data. Constraint columns are normally ID columns or date columns or have other unique identifiers. In this example we use the `Date` column as a constraint.\u00a0Use Select Columns to select only the constraint column.Use Remove Duplicates to return a unique list of constraints.\u00a0Use Add Constants to create a new column that tells you when a row needs to be deleted.Use an Outer Join to join your deletion-identifying column to the unique constraint column in your historical DataSet.\u00a0 If you selected the historical DataSet on the left side of the join, as shown below, use a left outer join. If you select the historical DataSet on the right side of the join, use a right outer join. (Do not select\u00a0Inner Join, as this could result in a loss of data.)Filter any rows from your DataSet that contain the value \"Delete Me\" (or whatever identifier you came up with in step 5).This returns only rows from the historical DataSet that do not exist in the new updating DataSet.Use Select Columns to remove the additional unique constraint column (`Date`) and the unique deletion-identifying column.Use Append to join the historical DataSet and new updating DataSet.If all steps have been done correctly, both DataSets will show \"No changes.\"Connect the Append tile to your output.\nOnce complete, your Magic ETL should look like the following:\nTroubleshooting/FAQ", "source": "../../raw_kb/article/creating_a_recursivesnapshot_old_magic_etl_dataflow/index.html", "title": "Creating a Recursive/Snapshot Old Magic ETL DataFlow"}, {"objectID": "77628eff55d2-2", "text": "Once complete, your Magic ETL should look like the following:\nTroubleshooting/FAQ\nSee Top 5 issues Users Experience with DataFlows\u00a0to see common issues and errors seen when building DataFlows.", "source": "../../raw_kb/article/creating_a_recursivesnapshot_old_magic_etl_dataflow/index.html", "title": "Creating a Recursive/Snapshot Old Magic ETL DataFlow"}, {"objectID": "711ed0d13f6e-0", "text": "TitleCreating a Recursive/Snapshot SQL DataFlowArticle BodyA \"recursive\" or \"snapshot\" DataFlow is a DataFlow that uses itself as an input.\nDataFlows (neither SQL nor ETL types) cannot append data natively like connectors. However, if you need to create a DataFlow that appends data, you can do so by running it once and then using the output as part of the input for the next run. This way, every time the DataFlow runs, it includes the data from before and also appends the new data onto itself.\u00a0\n\n\n \n\nImportant: If a recursive DataFlow is edited incorrectly, you could lose ALL historical data. To avoid this any time you are editing, create an additional DataSet that is a copy of your historical DataSet. This DataSet will remain static. If anything happens to your historical DataSet, you will have a backup from before you began editing.\u00a0 \n\n\n\nVideo - What is a Recursive DataFlow?", "source": "../../raw_kb/article/creating_a_recursivesnapshot_sql_dataflow/index.html", "title": "Creating a Recursive/Snapshot SQL DataFlow"}, {"objectID": "711ed0d13f6e-1", "text": "Video - What is a Recursive DataFlow?\n\n\u00a0 \n\n\n\u00a0\n\u00a0\nTo create a recursive SQL DataFlow,\nCreate and run an SQL DataFlow.The query for the output DataSet will be SELECT * FROM your-input-DataSetOnce the DataFlow has finished running, load the output DataSet as an input DataSet.The DataSet name will show the output DataSet name followed by \"1.\"You should now have two DataSets in the DataFlow\u2014the updating original DataSet and the historical DataSet.Locate a column to use as a constraint. This helps determine when to replace data in your historical DataSet with new data. Constraint columns are normally ID columns or date columns or have other unique identifiers. In this example we use the `Date` column as a constraint.\u00a0Your query should look similar to the following:Filter your data so you are pulling historical data where it does not exist in the original DataSet.Combine your new and historical DataSets.\u00a0Append the new data with the historical DataSet.Output the data using the same output you created back in Step 1. You don't need to create a brand new output.", "source": "../../raw_kb/article/creating_a_recursivesnapshot_sql_dataflow/index.html", "title": "Creating a Recursive/Snapshot SQL DataFlow"}, {"objectID": "55686b985a05-0", "text": "Title\n\nCreating a Request Template\n\nArticle Body\n\nIn Approvals, you can create request templates that anyone can use to submit requests. Whether it\u2019s a new budget proposal, a time off request, or something in between, Approvals is designed to allow you to incorporate all of your company\u2019s unique approval processes. When creating a template, you determine the information that must be provided before submitting a request, and the individuals or groups will review it.\nThis topic discusses the specific steps for configuring a request template. For general information about the layout of Approvals, see\u00a0Submitting a Request for Approval.\nTo create a request template,\nSelect \u00a0> Approvals.In the left-hand navigation pane, click\u00a0Manage templates.Click\u00a0Create New\u00a0in the top-right corner of the screen.Enter a unique name for the template in the\u00a0Template title\u00a0area at the top of the screen, also assign it an icon and a tag. (Optional) If you want to include specific instructions for users filling out this form, enter those instructions in the\u00a0Instructions\u00a0field.Click Add Field to add fields for users to enter information when submitting requests with this template.\n\n\n \n\nNote: The information that a user provides to the first field will be used as the title of the request.", "source": "../../raw_kb/article/creating_a_request_template/index.html", "title": "Creating a Request Template"}, {"objectID": "55686b985a05-1", "text": "For each additional field you want to include in this template, do the following:In the\u00a0Type\u00a0menu, select the appropriate field type for the desired response.  The following table lists and describes all of the available field types:Field TypeDescriptionExampleShort AnswerAllows users to enter a single line of text as a response. Also appropriate for times (as we currently do not have a field specific to times).ParagraphAllows users to enter a paragraph (multiple lines of text) as a response. Also appropriate for lists.\u00a0NumberAllows users to enter a string of numbers as a response.CurrencyAllows users to enter currency values as a response as well as select from four different currency symbols. Note that users do not need to include currency symbols in their responses, as they are applied automatically.EmailAllows users to enter an email address as a response.DateAllows users to select a date from a calendar as a response.AttachmentAllows users to attach a Domo\u00a0Card, DataSet, or a file as a response.(Optional) If you want this field to be optional for users, turn off the\u00a0Required\u00a0toggle switch. (If", "source": "../../raw_kb/article/creating_a_request_template/index.html", "title": "Creating a Request Template"}, {"objectID": "55686b985a05-2", "text": "for users, turn off the\u00a0Required\u00a0toggle switch. (If a field is required, a user cannot submit the request without providing a response in the field.)(Optional) To move a field up or down in the order of fields in the request template, click \u00a0and drag the field to wherever you want.(Optional) If desired, you can remove a field\u00a0by clicking the trash can icon next to that field.Under\u00a0Approval Chain, select the individuals or groups that need to review the requests submitted using this template. Requests will be sent to users for approval in the order provided here. For example, if the first user in the chain was \"Brad Storch\" and the second user was \"Betty Symington,\" any requests made using this form would first be sent to Brad for his approval. Once Brad gave his approval, they would then be sent to Betty. If Betty then gave her approval, the submitter would then be notified that the request had been successfully approved at both levels.(Optional) If the request will need to be approved by more than one", "source": "../../raw_kb/article/creating_a_request_template/index.html", "title": "Creating a Request Template"}, {"objectID": "55686b985a05-3", "text": "request will need to be approved by more than one user, click Add Approver Step and select the new approving user.(Optional) If you do not want individuals modifying the approval chain when using this template, use the toggle switch next to Lock Chain. When a template has a locked chain, it means that the user submitting a request with this template cannot add/remove approvers or modify the order.(Optional) If desired, change the approver order by clicking \u00a0for a given approver and dragging his/her name up or down in the chain.(Optional) Add observers to this request template. Observers have access to review the request and add comments, but cannot act on the request, such as approving or denying. When adding an observer to a template, the observer will have access to every request submitted with this template.When you are satisfied with the layout of your template, click\u00a0Save.", "source": "../../raw_kb/article/creating_a_request_template/index.html", "title": "Creating a Request Template"}, {"objectID": "767943334854-0", "text": "TitleCreating a Rolling Average Period-over-Period DataFlowArticle BodyAt times you may want to create a card with, for example, a three-month moving average for this year compared to the same period last year, to see if they are trending up or down in comparison to the previous year.\nTo create the DataFlow for this calculation, create a transform like:\nSELECT\u00a0 `DateField`\u00a0 ,`ValueField`\u00a0 ,(SELECT\u00a0 \u00a0 \u00a0 SUM(`ValueField`) / COUNT(*)\u00a0\u00a0\u00a0 \u00a0 FROM\u00a0 \u00a0 \u00a0 TABLE\u00a0t2\u00a0 \u00a0 WHERE\u00a0 \u00a0 \u00a0 t2.`DateField` > LAST_DAY(DATE_SUB(t1.`DateField`, INTERVAL 3 MONTH))\u00a0 \u00a0 \u00a0 AND t2.`DateField` < LAST_DAY(t1.`DateField`) ) AS `Rolling_Average`\nFROM\u00a0 TABLE\u00a0t1\nThis transform assumes that the \"DateField\" column contains a date representing the month, e.g. 2015-01-01 for January.\nThe subquery retrieves the sum of the last 3 months\u2019 worth of values and then divides them by the number of records found in the range, creating the \"Rolling_Average\" field.\nYou need to replace your field names for \u201cDateField\" and \u201cValueField\u201d and add any others you want in the resulting DataSet. Also replace TABLE with the DataSet table you\u2019re using as your input.\nIt is often beneficial to create a snapshot of values, so that as of any date chosen, you can look at that day\u2019s values compared against other time periods (the day before, week before, month before, etc.). To do this in a DataFlow, the data must be placed side-by-side so the card can read it properly. The following example query does this. Each section should be its own transform. You should also add indexing to the query for MySQL DataFlows.", "source": "../../raw_kb/article/creating_a_rolling_average_periodoverperiod_dataflow/index.html", "title": "Creating a Rolling Average Period-over-Period DataFlow"}, {"objectID": "767943334854-1", "text": "Select\u00a0 \u00a0 \u00a0 \u00a0today.date,\u00a0 \u00a0 \u00a0 \u00a0today.value,\u00a0 \u00a0 \u00a0 \u00a0yesterday.value,\u00a0 \u00a0 \u00a0 \u00a0lastweek.value,\u00a0 \u00a0 \u00a0 \u00a0lastmonth.valuefrom\u00a0 \u00a0 \u00a0 \u00a0rawdata today\u00a0left join\u00a0 \u00a0 \u00a0 \u00a0(select date_add(datevalue, INTERVAL 1 DAY) as date, value from rawdata) yesterday\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on today.date = yesterday.dateleft join\u00a0 \u00a0 \u00a0 \u00a0(select date_add(datevalue, INTERVAL 7 DAY) as date, value from rawdata) lastweek\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on today.date = lastweek.dateleft join\u00a0 \u00a0 \u00a0 \u00a0(select date_add(datevalue, INTERVAL 1 MONTH) as date, value from rawdata) lastmonth\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on today.date = lastmonth.date\nThe following table shows the logic behind this query:", "source": "../../raw_kb/article/creating_a_rolling_average_periodoverperiod_dataflow/index.html", "title": "Creating a Rolling Average Period-over-Period DataFlow"}, {"objectID": "767943334854-2", "text": "Some additional work may be needed in order to pull in the correct columns and values (such as using a Group By action).", "source": "../../raw_kb/article/creating_a_rolling_average_periodoverperiod_dataflow/index.html", "title": "Creating a Rolling Average Period-over-Period DataFlow"}, {"objectID": "4439175e54e9-0", "text": "TitleCreating a Rolling Average Using DataFlowsArticle BodyIntro\nYou can create a rolling average for a specified number of days\u00a0using\u00a0a SQL DataFlow or\u00a0Magic ETL. To do this, you will need to\u00a0join the data back onto itself multiple times.\nSolution Details and Steps\nMySQL\nCreate a\u00a0MySQL DataFlow.Add your input DataSet.\u00a0Create a transform using the following query:\nSELECT\u00a0a.`value`,(SELECT AVG(b.`value`) FROM input_DataSet b\u00a0WHERE b.`date` <= a.`date` AND b.`date` >= SUBDATE(a.`date`, INTERVAL 5 DAY)\u00a0) AS \u2018Rolling 5 Day\u00a0Average\u2019\u00a0,a.`date`\u00a0FROM input_DataSet\n(Optional) If you would like to change the day comparison (changing 5 day average to 7 days), you can change the number in your SUBDATE() [SUBDATE(a.`date`, INTERVAL 5 DAY)].If you would like to do a rolling sum or another calculation you can change your AVG() to the proper aggregation. [SELECT AVG(b.`value`) FROM input_DataSet b ].You will need to change `date` to your date column.You will need to change `value` to your value column.Create an output to the DataFlow. The query should be\u00a0SELECT * FROM transform_data_1.Name the output and your DataFlow.  \nRedShift\nCreate a\u00a0RedShift DataFlow.\u00a0Add your input DataSet.Create a transform using the following query:\nSELECT\u00a0a.\u201dvalue\u201d,(SELECT AVG(b.\u201dvalue\u201d) FROM input_DataSet b\u00a0WHERE b.\u201ddate\u201d <= a.\u201ddate\u201d AND b.\u201ddate\u201d >= DATEADD(DAY, -5, a.\u201ddate\u201d) ) AS \u201cRolling 5 Day Average\u201d,a.\u201ddate\u201d\u00a0FROM input_DataSet", "source": "../../raw_kb/article/creating_a_rolling_average_using_dataflows/index.html", "title": "Creating a Rolling Average Using DataFlows"}, {"objectID": "4439175e54e9-1", "text": "If you would like to change the day comparison (changing 5 day average to 7 days), you can change the number in your DATEADD() [DATEADD(DAY, -5, a.\u201ddate\u201d)].If you would like to do a rolling sum or another calculation you can change your AVG() to the proper aggregation. [SELECT AVG (b.\u201dvalue\u201d) FROM input_DataSet b ].You will need to change \u201cdate\u201d to your date column.You will need to change \u201cvalue\u201d to your value column.Create an output to the DataFlow. The query should be\u00a0SELECT * FROM transform_data_1.Name the output and your DataFlow.", "source": "../../raw_kb/article/creating_a_rolling_average_using_dataflows/index.html", "title": "Creating a Rolling Average Using DataFlows"}, {"objectID": "4439175e54e9-2", "text": "Magic ETL\nCreate a new Magic ETL.Add \u201cInput DataSet\u201d tile.Add \u201cSelect Columns\u201d tile.\u00a0Select only your date and value columns.You are going to create a \u201cDate Operations\u201d tile for every day you wish to compare.\u00a0If you are showing 5 day average you will need to create 5 tiles.You will select for all of these to use your \u201cSelect Columns\u201d tile as their input.  Your \u201cDate Operations\u201d tiles will be adding a day to your original date column. Your first tile will add 1 day, your second will add 2 and this will continue.  You now want to add 5 different \u201cJoin Data\u201d tiles.  Your joins are going to pull from the previous join and your date operation.The join should be formatted with your previous join (original data) as your left table. Your right table will be your \u201cDate Operations.\"You should join on the original \u201cDate\u201d = \u201cDate + _.\"  Your data should now look like the following screenshot if you \u201cRun Preview.\u201d  You can use a \u201cSelect Columns\u201d to pull your original \u201cDate\u201d and \u201cValue\u201d columns. You will also pull all other value columns. (Value_1 \u2013 Value_5)  You can now start to add your values together to create your average.You will need to add all of your value columns together.Using the \u201cCalculator\u201d tile we can add two columns together at a time.  You now can divide by the number of value columns you\u00a0have, using the \u201ccalculator\u201d tile. In this example it is 6.  You\u00a0can now clean up your columns using the \u201cSelect Columns\u201d tile.You only need to keep your \u201cDate\u201d, \u201cValue\u201d, and \u201cRolling Average\u201d columns.  \u00a0Your data should now look similar to: You can now add an output to your Magic ETL.", "source": "../../raw_kb/article/creating_a_rolling_average_using_dataflows/index.html", "title": "Creating a Rolling Average Using DataFlows"}, {"objectID": "4ac960ee98fa-0", "text": "TitleCreating a Schedule Group in Workbench 4Article Body\n\n\n\n\nImportant: Support for Workbench 4 ended on April 15, 2021. Workbench may continue to run on installed machines, but it will no longer receive feature enhancements and security updates. When issues are encountered in Workbench 4, the recommended course of action from the Domo Support team will be to upgrade to the latest version of Workbench 5. To see this article for Workbench 5, click here.", "source": "../../raw_kb/article/creating_a_schedule_group_in_workbench_4/index.html", "title": "Creating a Schedule Group in Workbench 4"}, {"objectID": "4ac960ee98fa-1", "text": "You can create groups of DataSet jobs that run on the same schedule. Unlike scheduling for an individual DataSet job, which is done from within Workbench and provides few options, scheduling for a DataSet job group is\u00a0done in Windows Task Scheduler (after you create the group in Workbench), so advanced options are available.\u00a0You can also make edits to your groups from within Workbench.\nFor information about basic scheduling, see\u00a0Scheduling a Job in Workbench 4.\nTo create a schedule group,\nClick\u00a0Tools > Schedule in the Buttons toolbar at the top of the Workbench window.Select\u00a0New\u00a0Scheduled Group\u00a0then click\u00a0Next.Enter a name for your group in the\u00a0Group Name field.Enter your Windows credentials in the required fields,\u00a0then click\u00a0Next.Check the boxes for all of the DataSet jobs you want\u00a0in this group, then click\u00a0Next.This creates a group with default schedule settings.(Conditional) To configure advanced settings in Windows Task Scheduler, click\u00a0Modify Schedule, then continue on to the next step.\u00a0Otherwise click Finish.When you click Modify Schedule, Windows Task Scheduler opens automatically\u00a0so you can complete the scheduling process.In Task Scheduler Library, click DomoV4.\u00a0In the pane in the top center of the screen, double-click the DataSet job you want to schedule.Click the Triggers tab.Select the default trigger then click Edit.Set up the scheduling for the group in the Edit Trigger dialog.For more information about the options in this dialog, see Windows' help documentation.", "source": "../../raw_kb/article/creating_a_schedule_group_in_workbench_4/index.html", "title": "Creating a Schedule Group in Workbench 4"}, {"objectID": "7d5bca226bfa-0", "text": "Title\n\nCreating a SQL DataFlow\n\nArticle Body\n\nIntro\nCreate DataFlows to combine and transform your data using SQL queries. This is a more technical DataFlow solution than using Magic ETL but may provide a greater breadth of options. For information about creating a Magic ETL DataFlow, see Creating a Magic ETL DataFlow.\n\n\n\u00a0\n\n\nImportant:\u00a0Input DataSets\u00a0in a DataFlow\u00a0cannot be restricted by PDP policies\u2014all available rows\u00a0must\u00a0pass through the DataFlow. Because of this, you must apply PDP policies to the output DataSets generated by a DataFlow.\u00a0\nWhen you build a DataFlow using an input DataSet\u00a0with PDP\u00a0policies in place,\u00a0the DataFlow breaks unless\u00a0at least one of the following criteria\u00a0applies:\nYou have an Admin security profile or a custom role with the Manage DataFlows grant enabled.You are\u00a0the DataSet owner.You are\u00a0part of the \"All Rows\" policy. This gives you access to all of the rows in the DataSet.\nFor more information about using PDP with DataFlows, see\u00a0PDP and DataFusions/DataFlows.\u00a0\n\n\n\n\nYou access the interface for creating a SQL\u00a0DataFlow from the Data Center. For more information about the Data Center, see Data Center Layout.\nVideo - MySQL DataFlow Overview\n \nVideo - Combining DataSets Using DataFlows\n\n\u00a0\nCreating a SQL DataFlow\n\u00a0\n\n\n\u00a0\n\n\nNote: Redshift does\u00a0not\u00a0support stored procedures, as documented here:\u00a0http://docs.aws.amazon.com/redshift/latest/dg/c_unsupported-postgresql-features.html. The alternative is to use a MySQL DataFlow, as MySQL does support stored procedures.\u00a0\u00a0\n\n\n\n\nTo create a SQL\u00a0DataFlow,\nIn Domo,\u00a0click\u00a0Data\u00a0in the toolbar at the top of the screen.Click SQL in the Magic Transform toolbar at the top of the window.", "source": "../../raw_kb/article/creating_a_sql_dataflow/index.html", "title": "Creating a SQL DataFlow"}, {"objectID": "7d5bca226bfa-1", "text": "Tip: You can also open the SQL DataFlow editor from anywhere in Domo by selecting \u00a0in the app toolbar and selecting Data > SQL.\n\n\nSelect the type of DataFlow you want to create.\tThis opens the\u00a0Create DataFlow view.Enter a name and description for the DataFlow.Select the input DataSets you want in the DataFlow by doing the following:Click Select DataSet.\n\n\n\u00a0\n\nNote:\u00a0Input DataSets in a DataFlow must already exist in Domo; you cannot upload new DataSets in the Create DataFlow view. For information about uploading new DataSets to Domo, see Connecting to Data with Connectors.\n\n\n\n\u00a0\n\n\n\u00a0\n\nImportant: The name of the DataSet and each column's name in the DataSet MUST be 64 characters or less (including spaces.) This is a limitation of the MySQL engine but is not seen in Magic ETL.\n\n\nSelect the desired DataSet.\t\t  appears for the DataSet you selected, including the owner of the DataSet, details of its run history, column names, etc.  Click\u00a0Choose DataSet.\t\tA\u00a0tile for the selected DataSet\u00a0appears on the screen. You can click the DataSet tile to view the preview window.(Optional) Under the\u00a0Select Columns\u00a0tab, you can remove columns you don't want to include in the DataSet by clicking the \"X\" to the right of the column name, or remove all columns by clicking None. \t\t\tYou can also add individual columns back into the DataSet by clicking\u00a0Add Column\u00a0and selecting the columns, or add all columns back into the DataSet by clicking\u00a0All.  (Select how the DataSet will be processed. Options include processing the Entire DataSet or Only new rows appended since the last DataFlow run.\n\n\n\u00a0\n\nNote: This feature is currently in Beta. To be added to the Beta, please reach out to your Customer Success Manager (CSM).", "source": "../../raw_kb/article/creating_a_sql_dataflow/index.html", "title": "Creating a SQL DataFlow"}, {"objectID": "7d5bca226bfa-2", "text": "Important: You can only use the DataFlow append processing method if that input DataSet is configured to update with an append method. For more information on scheduling your DataSet with an append type update, see Basic Scheduling of a DataSet.\n\n\n\n\n\n\u00a0\n\nTip: If your input DataSet has a large number of rows, try using the append processing method. This will allow your DataFlow to run much quicker if it's only processing the new rows.\n\n\n\n\u00a0Select as many additional input DataSets as you want by repeating step 5.(Optional) Add transforms by doing the following:Click Add Transform.\t\tA Transform dialog appears with various options. For more detailed information about these options, see Understanding Transform and Output DataSet options.Enter SQL code to make the desired transformations to the input DataSet(s).Click Apply.Click Add Output DataSet.\tAn Output DataSet dialog appears with various options. For more detailed information about these options, see Understanding Transform and Output DataSet options.Enter SQL code to indicate how you want the input DataSets to be combined.\t\tIf you have transformed the input DataSets in the Transform dialog, those transformations are applied here.(Optional) Add additional output DataSets by repeating the previous two steps.Click Done.Schedule your DataFlow in the Settings\u00a0pane. For more information on the different scheduling options, see Scheduling DataFlows.\n\n\n\u00a0\n\nNote:\u00a0If a schedule hasn't been specified, your DataFlow will default to a manual\u00a0schedule.", "source": "../../raw_kb/article/creating_a_sql_dataflow/index.html", "title": "Creating a SQL DataFlow"}, {"objectID": "7d5bca226bfa-3", "text": "(Conditional) If you want to run the DataFlow in Strict Mode, click the Settings\u00a0button at the top of the screen then toggle the\u00a0Strict Mode\u00a0option to on.\tFor more information about this option, see\u00a0Understanding Strict Mode.\u00a0  (Conditional) Do one of the following to save your DataFlow:If you want to run the script that outputs this DataFlow to a usable DataSet in Domo, click the orange down arrow in the upper right corner of the screen, select Save and Run,\u00a0enter a version description if desired, then click\u00a0Save to confirm. \t\tThis starts the process of generating DataSets from the DataFlow. This generation process may take from a minute to an hour or more, depending on the size of the input DataSets. In addition to generating DataSets, a card for the DataFlow is added to the DataFlows listing in the Data Center.If you want to save this DataFlow without outputting it to DataSets at this time, click Save, enter a version description if desired, then click Save again to confirm. A card for the DataFlow is added to the DataFlows listing in the Data Center, but no DataSets are generated. You can run the DataFlow to output DataSets at any time by mousing over the card for the DataFlow in the DataFlows listing, clicking , and selecting Run. This option and the other options available in this menu are discussed later in this topic.\nWhen you save a DataFlow, an entry for this version is added to the\u00a0Versions tab in the Details view for the DataFlow. If you entered a description when saving, that description is shown in the entry for the DataFlow. For more information about versions, see Viewing the Version History for a DataFlow.", "source": "../../raw_kb/article/creating_a_sql_dataflow/index.html", "title": "Creating a SQL DataFlow"}, {"objectID": "7d5bca226bfa-4", "text": "Note: Many users ask why output DataSets for a DataFlow are not marked as \"Updated\" when the DataFlow runs successfully. This is usually because the data has not actually changed\u2014no update has occurred. Therefore, the DataSets do not show as updated.   \u00a0\n\n\n\nUnderstanding Transforms and Output DataSet options\nWhen creating a new transform, you can choose to create either a table-\u00a0or SQL-type transform.\nA Table Transform\u00a0creates a new table using a SELECT statement and will always generate an output table. Due to an output table being generated, you can create easy indexes based on these tables.\u00a0A SQL Transform\u00a0creates a table that typically doesn't include a SELECT statement such as a stored procedure. This type of transform does not generate an output table.\nBoth Transforms and output DataSets support the Run to here\u00a0option. With Run to here, you can choose to run the DataFlow\u00a0only up to the selected transform.\u00a0In the case of very large DataFlows with numerous transforms or outputs, this may help you save time when testing your DataFlow's code. To use this option, select\u00a0Run to here\u00a0in the wrench menu for the transform or output DataSet, or open the editor for the transform or output DataSet, click the arrow next to the\u00a0Run SQL\u00a0menu, and select\u00a0Run to here.\u00a0\nYou can also add transforms either above or below a given transform in your DataFlow. To do this, select\u00a0Add transform above\u00a0or\u00a0Add transform below\u00a0in the wrench menu for the transform, then select whether you want to create a\u00a0table- or SQL-type transform.\u00a0\nThe following screenshot shows you the main components of the Transform dialog.", "source": "../../raw_kb/article/creating_a_sql_dataflow/index.html", "title": "Creating a SQL DataFlow"}, {"objectID": "7d5bca226bfa-5", "text": "You can use the following table to learn more about these components:\nComponentDescriptionInputs/Transforms ListShows the names of all input DataSets you have selected for this DataFlow for your convenience in writing SQL statements. In addition, any previously created transforms appear here. These names appear in both the Transform and Output DataSet dialogs. If you transform one or more input DataSets in the Transform dialog, those transforms are applied to the input DataSets when you refer to them in the Output DataSet dialog.\nClicking on an input DataSet\u00a0opens a list of all columns in the DataSet and their data types (decimal, text, date/time, etc.).Run SQL menuProvides access to several options:Run SQL. Lets you run a test to determine whether your SQL is valid.Run to here. Runs all previous transforms to this point.\u00a0Explain SQL. Shows the explain plan for the SQL\u2014an ordered list of the steps the database will make when executing the query. This can be used to help you optimize your queries. However, because these steps are highly complex and technical, this feature is recommended for advanced users only.\u00a0\u00a0\n\n\n\u00a0\n\nNote:\u00a0Because MySQL determines which indexes are needed to improve run-time efficiency, not all of your index transforms may show up in the Explain SQL if MySQL determined that running the index would cause inefficiencies.\n\n\nSQL fieldLets you create SQL code to transform and/or combine your input DataSets. The field contains an autocomplete feature for your convenience in writing code.Preview buttonExpands/Collapses the preview area at the bottom of the pane after you have chosen\u00a0Run SQL\u00a0or\u00a0Run to here.\nPreviewing the data\nThe preview will display in the transform after you have chosen\u00a0Run SQL\u00a0or\u00a0Run to here. You can select options in the preview menu \u00a0to allow you to change how the results are displayed in the preview table for easier viewing.", "source": "../../raw_kb/article/creating_a_sql_dataflow/index.html", "title": "Creating a SQL DataFlow"}, {"objectID": "7d5bca226bfa-6", "text": "Note: These settings do not affect the transform itself or how the data will be output. It\u00a0only affects\u00a0how the results are displayed in the preview table.\n\n\n\n\nThe following components are available:\nComponentDescriptionDisplay TextDefault - Displays the text in the standard font.Monospace - Displays the text with more spacing in between the letters.Null HandlingAdvanced - Displays the word null or empty string\u00a0to specify which type of value is being displayed in the column.Basic - Shows blank cells in the table. (This is the default setting.)Decimal PrecisionDefault - Displays the number as it appears in the DataSet.Set - Displays a specified amount of decimal places.\nEasy indexing\nAn index is a data structure that improves the speed of operations in a table. With\u00a0Easy Indexing, you can quickly add an index on one or more columns right to your input DataSet or\u00a0table transform.\nTo create an index in an input DataSet or transform,\nOpen the editor for the input DataSet or transform.Select the Indexing tab.  Choose your Index Type.Select which column to apply the index.Click\u00a0Done.\u00a0\nVideo - SQL Explain Plans\n \nUnderstanding Strict Mode\nStrict Mode controls how MySQL handles invalid or missing values in data-change statements and also affects the\u00a0handling of division by zero, zero dates, and zeroes in dates. You can find more info at https://dev.mysql.com/doc/refman/5.6/en/sql-mode.html#sql-mode-strict.\nBest Practices for Creating DataFlows\nEach DataFlow should...\ninclude descriptive names for each step of the transformation.include a description of the input DataSets being merged or manipulated and the DataSet being created, and should also indicate the owner of the data.be named the same as the output DataFlow\u2014Because the outputs of a Dataflow become their own DataSet in the Data Center, this allows for easy identification of which DataSets are produced by which DataFlows.", "source": "../../raw_kb/article/creating_a_sql_dataflow/index.html", "title": "Creating a SQL DataFlow"}, {"objectID": "dfb1ac91dce6-0", "text": "Title\n\nCreating Badges for Achievements\n\nArticle Body\n\nYou can create a\u00a0badge\u00a0and designate users who can award the\u00a0badge to others in their\u00a0Profile\u00a0pages. You can do this in More > Admin > Company settings > Achievements. You can only create badges and designate users to award them\u00a0if you have an \"Admin\" default security role or a custom role with \"Assign Achievements\" enabled. For more information about default security roles, see\u00a0Managing Custom Roles.\u00a0\nFor information about awarding a badge, see\u00a0Viewing and Awarding Badges.\n\n\n\nTo create a badge,\nClick\u00a0 More\u00a0> Admin.The\u00a0Admin Settings\u00a0page appears.Select Company settings > Achievements.Click\u00a0New Achievement, then define the badge.Enter the name of the badge.Enter the description of the badge.Enter the names of users who can award the\u00a0badge to others\n\n\n \n\n\nNote:\u00a0If you have permission to award an badge, the Give a Badge\u00a0button appears in the\u00a0Badges section when viewing a user profile.\n\n\nUpload an image of the badge.Click\u00a0Add.", "source": "../../raw_kb/article/creating_badges_for_achievements/index.html", "title": "Creating Badges for Achievements"}, {"objectID": "dc88e9a2ea00-0", "text": "TitleCreating Columns in Workbench 4 Using CalculationsArticle Body\n\n\n\n\nImportant: Support for Workbench 4 ended on April 15, 2021. Workbench may continue to run on installed machines, but it will no longer receive feature enhancements and security updates. When issues are encountered in Workbench 4, the recommended course of action from the Domo Support team will be to upgrade to the latest version of Workbench 5. To see this article for Workbench 5, click here.\n\n\n\nYou can create new columns in DataSets in Workbench 4 by building calculations based on the data in other columns. For example, you\u00a0might\u00a0have a column containing\u00a0values with one or more decimal places.\u00a0You could create a new column in which\u00a0all of the values in the original column are rounded up or down to the nearest whole number,\u00a0using the\u00a0CEILING or\u00a0FLOOR functions, respectively. Workbench 4 contains over 80 such functions you can use in constructing calculations. Categories of\u00a0functions include logical (such as AND, NOT, IF, etc.), mathematical (such as ACOS, RND, and SUMLIST), statistical (such as AVERAGE, MAX, and STDEV), text (such as CONCATENATE, MID, and SUBSTITUTE), and date and time (such as DAY, EOMONTH, and TODAY). You can find descriptions of all of the functions in the Workbench 4 user interface.\nTraining Video - Creating Columns in Workbench Using Calculations\nLearn how to create new Workbench columns using calculations.\n\nYou build calculated fields for a selected DataSet job in the Calculation Builder.\u00a0You access the Calculation Builder for a DataSet by adding a Calculation transform.", "source": "../../raw_kb/article/creating_columns_in_workbench_4_using_calculations/index.html", "title": "Creating Columns in Workbench 4 Using Calculations"}, {"objectID": "dc88e9a2ea00-1", "text": "To add a calculated field in Workbench 4,\nIn the Accounts pane, select\u00a0the DataSet job you want\u00a0to add a calculated field\u00a0to.In the Transforms grouping in the Buttons toolbar at the top of the Workbench window, click Add New.In the Transform Type menu, select Calculation then click Next.Click Finish.A Calculation item is added under Transforms for this DataSet job.Click the new Calculation item under Transforms.The Calculation Builder opens in the Dynamic Options panel.Enter a name for the calculation in the Column Name field.Build your calculation in the\u00a0Calculation field using the column names in the\u00a0Available Fields listing and the functions in the\u00a0Available  Functions listing.You can add column names and functions to a calculation by double-clicking them in the list. \u00a0You can filter the functions by category by selecting the desired category in\u00a0the\u00a0Category menu. All functions\u00a0are accompanied by descriptions; you can see\u00a0the description for a function by clicking that function. In addition, each function shows its proper usage. For example, the SUM function is represented as SUM (p1, [p2],...).\u00a0This means that you\u00a0are summing the values in all referenced columns, which are indicated by p1, p2, and so on.\u00a0\u00a0Click\u00a0Validate to ensure that\u00a0your calculation is valid.\n\n\n \n\n\nNote:\u00a0In calculations, column names cannot contain non-ASCII characters. You can reference a column in a calculation using Column# where # is the 1-based column order. For example: CONCATENATE(Column2,Column3)\n\n\n\nClick\u00a0Save in the DataSet Jobs grouping in the Buttons toolbar at the top of the Workbench window.\n\u00a0Your DataSet\u00a0now contains the new\u00a0column based on your calculation.", "source": "../../raw_kb/article/creating_columns_in_workbench_4_using_calculations/index.html", "title": "Creating Columns in Workbench 4 Using Calculations"}, {"objectID": "fb581a005f6e-0", "text": "Title\n\nCreating Columns in Workbench 5.1 Using Calculations\n\nArticle Body\n\nYou can create new columns in DataSets in Workbench 5.1 by building calculations based on the data in other columns. For example, you might have a column containing values with one or more decimal places. You could create a new column in which all of the values in the original column are rounded up or down to the nearest whole number, using the CEILING or\u00a0FLOOR functions, respectively. Workbench 5.1 contains over 80 such functions you can use in constructing calculations. Categories of functions include logical (such as AND, NOT, IF, etc.), mathematical (such as ACOS, RND, and SUMLIST), statistical (such as AVERAGE, MAX, and STDEV), text (such as CONCATENATE, MID, and SUBSTITUTE), and date and time (such as DAY, EOMONTH, and TODAY). You can find descriptions of all of the functions in the Workbench 4 user interface.\nYou build calculated fields for a selected DataSet job in the Calculated\u00a0Field Transform Editor.\u00a0You access the Calculated\u00a0Field Transform Editor\u00a0for a DataSet by adding a Calculation transform.\n\nVideo - Calculated Field Transform in Workbench 5", "source": "../../raw_kb/article/creating_columns_in_workbench_51_using_calculations/index.html", "title": "Creating Columns in Workbench 5.1 Using Calculations"}, {"objectID": "fb581a005f6e-1", "text": "Video - Calculated Field Transform in Workbench 5\n\nTo add a calculated field in Workbench 5.1,\nIn Workbench, click \u00a0in the left-hand icon bar.In the Jobs listing, double-click\u00a0the job for which you want to create a calculated column.Click Transforms\u00a0to expand that section of the pane.In the\u00a0Add a transform\u00a0menu, select\u00a0Calculated Field Transform.Click the \u00a0button.The\u00a0Calculated Field Transform Editor\u00a0appears.Enter a name for the new column in the Column Name field.Build your calculation in the\u00a0Calculation field using the column names in the\u00a0Available columns\u00a0listing and the functions in the\u00a0Available  functions listing.You can add column names and functions to a calculation by double-clicking them in the list. \u00a0You can filter the functions by category by selecting the desired category in\u00a0the\u00a0Category menu. All functions\u00a0are accompanied by descriptions; you can see\u00a0the description for a function by clicking that function. In addition, each function shows its proper usage. For example, the SUM function is represented as SUM (p1, [p2],...).\u00a0This means that you\u00a0are summing the values in all referenced columns, which are indicated by p1, p2, and so on.\u00a0\u00a0Click\u00a0Validate to ensure that\u00a0your calculation is valid.\n\n\n \n\n\nNote:\u00a0In calculations, column names cannot contain non-ASCII characters. You can reference a column in a calculation using Column# where # is the 1-based column order. For example: CONCATENATE(Column2,Column3)\n\n\n\nClick\u00a0Apply\u00a0when finished.Click \u00a0at the top of the pane to save your transform.\n\u00a0The next time you run the job, the newly calculated column will be added to your DataSet.", "source": "../../raw_kb/article/creating_columns_in_workbench_51_using_calculations/index.html", "title": "Creating Columns in Workbench 5.1 Using Calculations"}, {"objectID": "5cea051cf356-0", "text": "Title\n\nCreating Columns in Workbench 5 Using Calculations\n\nArticle Body\n\nYou can create new columns in DataSets in Workbench 5 by building calculations based on the data in other columns. For example, you\u00a0might\u00a0have a column containing\u00a0values with one or more decimal places.\u00a0You could create a new column in which\u00a0all of the values in the original column are rounded up or down to the nearest whole number,\u00a0using the\u00a0CEILING or\u00a0FLOOR functions, respectively. Workbench 5 contains over 80 such functions you can use in constructing calculations. Categories of functions include logical (such as AND, NOT, IF, etc.), mathematical (such as ACOS, RND, and SUMLIST), statistical (such as AVERAGE, MAX, and STDEV), text (such as CONCATENATE, MID, and SUBSTITUTE), and date and time (such as DAY, EOMONTH, and TODAY). You can find descriptions of all of the functions in the Workbench 4 user interface.\nYou build calculated fields for a selected DataSet job in the Calculated\u00a0Field Transform Editor.\u00a0You access the Calculated\u00a0Field Transform Editor\u00a0for a DataSet by adding a Calculation transform.\n\nVideo - Calculated Field Transform in Workbench 5", "source": "../../raw_kb/article/creating_columns_in_workbench_5_using_calculations/index.html", "title": "Creating Columns in Workbench 5 Using Calculations"}, {"objectID": "5cea051cf356-1", "text": "Video - Calculated Field Transform in Workbench 5\n\nTo add a calculated field in Workbench 5,\nIn Workbench, click \u00a0in the left-hand icon bar.In the Jobs listing, double-click\u00a0the job for which you want to create a calculated column.Click Transforms\u00a0to expand that section of the pane.In the\u00a0Add a transform\u00a0menu, select\u00a0Calculated Field Transform.Click the\u00a0\u00a0button.The\u00a0Calculated Field Transform Editor\u00a0appears.Enter a name for the new column in the Column Name field.Build your calculation in the\u00a0Calculation field using the column names in the\u00a0Available columns\u00a0listing and the functions in the\u00a0Available  functions listing.You can add column names and functions to a calculation by double-clicking them in the list. \u00a0You can filter the functions by category by selecting the desired category in\u00a0the\u00a0Category menu. All functions\u00a0are accompanied by descriptions; you can see\u00a0the description for a function by clicking that function. In addition, each function shows its proper usage. For example, the SUM function is represented as SUM (p1, [p2],...).\u00a0This means that you\u00a0are summing the values in all referenced columns, which are indicated by p1, p2, and so on.\u00a0\u00a0Click\u00a0Validate to ensure that\u00a0your calculation is valid.\n\n\n \n\n\nNote:\u00a0In calculations, column names cannot contain non-ASCII characters. You can reference a column in a calculation using Column# where # is the 1-based column order. For example: CONCATENATE(Column2,Column3)\n\n\n\nClick\u00a0Apply\u00a0when finished.Click\u00a0\u00a0at the top of the pane to save your transform.\n\u00a0The next time you run the job, the newly calculated column will be added to your DataSet.", "source": "../../raw_kb/article/creating_columns_in_workbench_5_using_calculations/index.html", "title": "Creating Columns in Workbench 5 Using Calculations"}, {"objectID": "e6929862d1ee-0", "text": "Title\n\nCreating County and Zip Code Drilldown in a U.S. Map\n\nArticle Body\n\nIntro\nYou can create U.S. maps in which users can drill down on states to see values by both county and zip code. For example, a U.S. map may show sales for western states, as follows:\nIn this map, a user could click on the state of Oregon to show the sales for all of the counties in Oregon:\nHe could then click on Multnomah County (the small dark red county containing the city of Portland) to show sales by zip code within that county:\nTo enable state and county drilldown, you must first create a DataFlow data source that joins two data sources: the data source with the values you want to show and a data source with state, county, and zip code information. With this data source, you can create a U.S. map with state and county drilldown enabled.\u00a0\nVideo\u00a0- Creating a Map Drilldown to Zip", "source": "../../raw_kb/article/creating_county_and_zip_code_drilldown_in_a_us_map/index.html", "title": "Creating County and Zip Code Drilldown in a U.S. Map"}, {"objectID": "e6929862d1ee-1", "text": "Creating the DataSet\nThe first step in building a map with drilldown is creating a DataSet with your values matched to the appropriate states, counties, and zip codes. You can do this\u00a0by using DataFlows to join the following input DataSets:\nThe master DataSet that contains the values you want to display. The DataSet must contain a column with zip codes.A reference DataSet containing columns with states, FIPS county codes, and zip codes. The names of the columns in this DataSet must be different from those in the master DataSet, or the DataFlow will not run successfully.\nTo create this DataSet you will need to combine your data with Domo's\u00a0\"Zips2Fips\" DataSet, which is available via the Domo Dimensions connector.\u00a0After you acquire the Domo \"Zips2Fips\" DataSet, you can create a DataFlow DataSet that joins the master and \"Zips2Fips\" DataSets by matching the zip code columns.\nTo join the master and \"Zip2Fips\" DataSets,", "source": "../../raw_kb/article/creating_county_and_zip_code_drilldown_in_a_us_map/index.html", "title": "Creating County and Zip Code Drilldown in a U.S. Map"}, {"objectID": "e6929862d1ee-2", "text": "To join the master and \"Zip2Fips\" DataSets,\nIn Domo,\u00a0click\u00a0Data\u00a0in the toolbar at the top of the screen.Click SQL\u00a0in the Magic Transform\u00a0area at the top of the screen.\tThe Create DataFlow page opens.Enter a name for the DataFlow in the Name field.(Optional) Enter a description for the DataFlow in the Description field.Under \"Input DataSets,\" click Select a DataSet.Locate and select the master DataSet (the DataSet that contains your values).Click Select a DataSet.Locate and select the \"Zip2Fips\" DataSet.Click Add Output DataSet.\tAn Output DataSet dialog appears.Enter a name for the output DataSet in the Output DataSet Name field.In the Output SQL field, enter the following SQL, replacing master with the name of your master DataSet and zipcode_column_name with the name of the zip code column in your master DataSet. Also, if you named the reference DataSet anything besides \"zip2fips\" in the previous set of steps, you will need to change all instances of \"zip2fips\" to that name.  select master.*, zip2fips.`STATE` as 'State', zip2fips.`FIPS`, zip2fips.`CNTY_NAME` as 'County', zip2fips.`ZIP` as 'ZipCode'from master join zip2fips on zip2fips.`ZIP` = master.`zipcode_column_name`Click Done.Click Save and Run. \tDomo\u00a0now\u00a0attempts to create an output DataSet that joins the master and \"Zips2Fips\" DataSets. If the attempt is successful, the new DataSet is added to your DataSets listing. If any problems are found (such as duplicate column names), a message appears indicating that the run was not successful.\nCreating map drilldown\nWith your new DataSet, you can build a U.S. map with state- and county-level drill path.", "source": "../../raw_kb/article/creating_county_and_zip_code_drilldown_in_a_us_map/index.html", "title": "Creating County and Zip Code Drilldown in a U.S. Map"}, {"objectID": "e6929862d1ee-3", "text": "Note: You cannot drill directly from the U.S. map into a zip code map; you must include the state level as an intermediary step. For example, you would not be able to click on Oregon and bring up a map with zip codes; you would first need to open a view showing the counties.\u00a0\u00a0\n\n\n\nTo create a map with drill path,\nIn the page where you want the new card to appear, click\u00a0then select\u00a0Create New Card.Click Existing Data.Locate and select the DataSet you created in the previous set of steps.Click Select DataSet.\tThe Visualize your Data screen appears,Select United States as your chart type.In the value menu, select the column that contains the values you want to display.In the category menu, select State.Click Save & Finish.\tThe card is saved to the card page.Click the new card to open its Details view.In the\u00a0Options menu, select Edit Drill Path.Click Add a view.In the category menu, select FIPS.Click Save this View.Click Add a view.In the category menu, select ZipCode.Click Save this View.\nThe drilldown map is complete. You can test it by returning to the Details view and clicking on a state. If the drill path was implemented correctly, a state-level view appears with counties showing. You can then click a county to show the zip code areas within that county.\n\n\n\u00a0\n\nNote:\u00a0If you have just state level information and use the US map card to\u00a0filter to a specific state, the entire US map will appear, but only the state you have filtered on will be\u00a0highlighted.", "source": "../../raw_kb/article/creating_county_and_zip_code_drilldown_in_a_us_map/index.html", "title": "Creating County and Zip Code Drilldown in a U.S. Map"}, {"objectID": "e6929862d1ee-4", "text": "To view a map of the state you have filtered to, you need the FIPS view saved as the top level US County card.\u00a0 The map recalibrates\u00a0the view to show only the state you have FIPS values for.\u00a0Then, as in the Oregon example above, you would just see the specific state\u00a0but filtered down to the county level.\u00a0\n\n\n\u00a0\n\nNote: For a US map to use\u00a0State/County/Zip Code drilldown, it is important that the state column in the\u00a0DataSet\u00a0for the\u00a0top-level card use a two letter state abbreviation category. The state column MUST be named \"State\" or \"STATE.\"", "source": "../../raw_kb/article/creating_county_and_zip_code_drilldown_in_a_us_map/index.html", "title": "Creating County and Zip Code Drilldown in a U.S. Map"}, {"objectID": "378770b027f4-0", "text": "Title\n\nCreating Domo Stories\n\nArticle Body\n\nIntro\nDomo Stories\u00a0allow you to tell targeted stories by creating custom Pages\u00a0that translate smoothly across devices and retain the same Card orientation and size.\u00a0Because each layout has been optimized individually for web, mobile, tablet, print, and Scheduled Reports, Domo Stories tell\u00a0a consistent story no matter where the story ends up being consumed.\u00a0With hundreds of layout templates built individually for dozens of use cases, you\u00a0can create Domo Stories that bridge the gap between understanding the data and taking action.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-1", "text": "Domo Stories differ from standard Pages in the following ways:\nIn standard Pages, individual users can move and resize Cards. With Domo Stories, however, only Page owners and \"Admin\" users can make changes to Card order and size. This provides a single source of truth by ensuring that all users see the same thing in a Page.Cards in\u00a0standard Pages\u00a0may appear differently across different devices or platforms (i.e. different desktop window sizes, email reports, print, etc.).\u00a0Cards in a Domo Stories Page, however,\u00a0appear the same no matter what device or platform you are using.\u00a0In standard Pages, all Cards are the same size and dimensions by default. If you want to change the size or dimensions of a Card, you must do it manually. In a Domo Stories Page, you can choose from hundreds of\u00a0different templates, each of which contains slots appropriate for different Card sizes and dimensions. You can also resize templates from the Page level, which changes the dimensions of the Cards in the templates.\u00a0Domo Stories cannot be collapsed as Collections in standard Pages can.You can export Domo Stories Pages to PDFs. You cannot do this with standard Pages. For more information, see\u00a0Exporting Domo Stories Pages to PDF.\nYou can use Card-to-Card Filters in Domo Stories and in standard Pages. These Filters allow you to filter all Cards on a Page by setting a Filter on a single Card. Interaction Filters are automatically enabled in Domo Stories but must be turned on manually in standard Pages. For more information, see\u00a0Using Filter Cards to Filter Card Content.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-2", "text": "Cards in a Domo Stories Page have all of the same options as Cards in a standard Page EXCEPT the sizing options. If you want to resize a Card in a Domo Stories Page, you drop it into a different-sized slot or resize the template from the Page view while editing the Page. There are no height restrictions for Cards in Domo Stories; they can be as tall as you want.\u00a0\u00a0\nIn Domo Stories, you can change the background color of Pages as well as Cards. This is not possible in standard Pages.\u00a0\nYou cannot have Domo Stories and Card Collections in the same Page.\u00a0If you have any Collections in your Page when you turn on Domo Stories, all of the Collections are converted into Domo Stories Pages. Likewise, if you convert a Domo Stories Page back into a standard Page, all of the Domo Stories are turned back into Collections. If you convert a Page with Collections into a Domo Stories Page and there are any Cards not in a Collection, these Cards are added to an \"Appendix\" section at the bottom of the Page.\u00a0\nConverting a standard Page to a Domo Stories\u00a0Page\nTo convert a standard Page (with or without Collections) to a Domo Stories Page, you must be the Page owner or have an \"Admin\" default security role. For more information about security roles, see Default Security Role Reference.\nTo convert a standard Page to a Domo Stories\u00a0Page,", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-3", "text": "To convert a standard Page to a Domo Stories\u00a0Page,\nSelect\u00a0Design Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Click\u00a0Design Dashboard.If the Page contains Collections, all of the Collections become Domo Stories Pages, and any extra Cards not in Collections are added into the \"Appendix\" section at the bottom of the Page. If the Page does not contain Collections, all of the Cards are added to a single default Domo Stories Page with an \"Example Header\" heading.\u00a0(Conditional)\u00a0If you are already satisfied with the Domo Stories layout on the Page, click\u00a0Save\u00a0then Close. Otherwise, first edit the Domo Stories Page\u00a0layout following the instructions under Editing Pages, below.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-4", "text": "Note:\u00a0When you convert a standard Page to a Domo Stories\u00a0Page, the\u00a0Edit Dashboard\u00a0and\u00a0Convert to Standard Page\u00a0options are grayed out while the Pages and Cards load.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-5", "text": "Converting a Domo Stories Page back to a standard Page\nTo convert a Domo Stories Page (with or without Collections) back to a standard\u00a0Page, you must be the Page owner or have an \"Admin\" default security role. For more information about security roles, see\u00a0Default Security Role Reference.\u00a0\nTo convert a Domo Stories\u00a0Page back to a standard Page,\nSelect\u00a0Convert to Standard Page\u00a0in the\u00a0\u00a0menu in the top right corner of the Page. (If the Domo Stories\u00a0edit mode is open, you must first close it for this menu to appear.)Click\u00a0Convert Page\u00a0to confirm.\nIf the Page contains Domo Stories, all of the Domo Stories are converted into Collections.\nAdding Domo Stories and layout components\nThe Domo Stories\u00a0edit mode contains various options for editing your Domo Stories. In this mode you can do all of the following:\nAdd new Card layouts based on a variety of Domo Stories templates.Drag and drop Visualization Cards into the slots in the layouts you have added.Resize templates and the Cards within them by dragging their borders\u00a0Swap Cards in a layout by dragging one Card on top of another.Add headers to your Domo Stories.Drag and drop layouts and headers wherever you want them in the Page.Delete layouts or headers as desired.\u00a0Add borders between Domo Stories.\u00a0Add images and\u00a0notebook Card content inline, without having to first open the notebook Card editor.Change the background color of your Page and/or individual Cards.\u00a0Specify which Card elements appear (for example, you could hide the summary number for a Card).\nAdding a Domo Stories Page\nYou can add a new Domo Stories Page to your Domo instance by selecting\u00a0New Story\u00a0in the\u00a0Add to Domo\u00a0menu.\nTo add a Domo Stories Page to your instance,", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-6", "text": "To add a Domo Stories Page to your instance,\nClick\u00a0\u00a0in the top right corner of the screen.Select\u00a0New Story.Enter a title for the new Stories Page.\nA new Domo Stories template Page is added, and the edit mode is opened by default. You can now begin adding and customizing Domo Stories layouts as described in the following sections.\u00a0\nAdding a Domo Stories layout\u00a0\nYou can add a Domo Stories layout by clicking the\u00a0\u00a0icon\u00a0that appears on the right side of the Page view when the Domo Stories\u00a0edit mode is open, then dragging it to the desired location on the Page. You can then select the desired Domo Stories layout template. The template you select will then be added to this location in the Page.\nTo add a Domo Stories\u00a0layout,", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-7", "text": "To add a Domo Stories\u00a0layout,\nOn the Page where you want to add a Domo Stories layout, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Click and drag the \u00a0icon from the toolbar on the right side of the Page to the location on the Page where you want the new layout.\u00a0When you drag the\u00a0layout icon over a\u00a0border, it highlights to indicate you can put the layout there. In the example below, the user is dragging the icon over the line directly beneath the \"Group 1\" layout.\u00a0Once you drop the \u00a0icon onto a border, the\u00a0Choose a layout\u00a0dialog opens.Use the filter options to locate the desired template.\u00a0The\u00a0Categories\u00a0menu lets you filter by template category. Categories are as follows:TemplateDescriptionAllThe top templates are shown for all categories. You can show all of the templates for a category by clicking\u00a0View All\u00a0for that category.With bannerAll of these templates include a long, narrow slot at the top where you can insert a Card to use as a banner.\u00a0HeroAll of these templates include at least one very large slot for Cards that should immediately draw viewers' attention (\"hero Cards\").\u00a0\nQuick summaryAll of these templates consist of small slots for Cards that are intended to be glanced at.\u00a0\nGeneral purposeAll of these templates consist of a mix of small and large Cards.\nThe\u00a0Number of Cards\u00a0menu lets you filter by the number of Card slots in the template.\u00a0Once you locate the desired template, click on it.\u00a0The template for the layout is now added to the designated location in the Page.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-8", "text": "Tip:\u00a0You can return to the\u00a0Choose a layout\u00a0dialog to change your template by mousing over the layout and clicking the pencil icon\u00a0\u00a0that appears on the left. Note that when switching a template, the new template does\u00a0not\u00a0need to contain the same amount of slots as the current template. If the new template has fewer slots available than the current template, the extra Cards are moved to the \"Appendix\" section at the bottom of the Page. If the new template has more slots available than the current template, the new slots appear as empty in the template.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-9", "text": "Do any of the following to add Cards to the layout:Drag Cards from elsewhere in the Page into the appropriate slots in the template.\u00a0You can drag Cards from other Domo Stories in the Page, from the \"Appendix\" area, etc. When you drag a Card (represented by a \u00a0icon) over a border line in a layout, it highlights\u00a0to indicate the Card will end up there when you drop it.\u00a0Note that when you move a Card into a different-sized slot, the Card automatically resizes itself to fit the slot.If you drag a Card onto another Card, the two Cards switch places. So if you dragged \"Finance: AR Aging\" from \"Group 2\" onto \"\"Finance: COGS\" in \"Group 1,\" \"Finance: AR Aging\" would take over the spot in Group 1, and \"Finance: COGS\" would appear in the spot formerly containing the other Card in Group 2.Add an existing Visualization Card from your Domo instance into an empty slot by doing the following:In the slot where you want to add the Card, click\u00a0Add", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-10", "text": "slot where you want to add the Card, click\u00a0Add Content.Select\u00a0Add existing Card.Locate the Card you want to add by entering its name in the\u00a0Search Cards\u00a0field and clicking on it.Add an existing Visualization Card from your Domo instance to a layout without an empty slot by doing the following:Click and drag the \u00a0icon from the toolbar on the right side of the Page to the spot where you want the Card to appear.\u00a0When you drag the Card\u00a0icon over a\u00a0Page, borders between layout slots highlight to indicate\u00a0the spots where you can insert the Card.\u00a0In the example below, the user is dragging the icon over the line between the \"Finance: COGS\" and \"Outstanding Debt\" Cards.When you drop the icon onto a line, a slot appears for the Card with an\u00a0Add Content\u00a0button.Click\u00a0Add Content.Select\u00a0Add existing Card.Locate the Card you want to add by entering its name in the\u00a0Search Cards\u00a0field and clicking on it.Add an image Card (essentially a Document Card containing an image) inline to a layout by doing the", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-11", "text": "an image) inline to a layout by doing the following:Click and drag the \u00a0icon from the toolbar on the right side of the Page to the spot where you want the image Card\u00a0to appear.\u00a0When you drag the image icon over a\u00a0Page, borders between layout slots highlight to indicate\u00a0the spots where you can insert the Card.Click\u00a0Choose Image.Select the image you want to upload from your computer or network.Add a notebook Card inline to a layout by doing the following:Click and drag the \u00a0icon from the toolbar on the right side of the Page to the spot where you want the image Card\u00a0to appear. Alternatively, if your layout already contains a blank slot, you can select\u00a0Add Content > Insert text\u00a0to open the editor.Enter the desired notebook Card content.\u00a0The interface options are the same as those in the full notebook Card editor. For more information, see\u00a0Adding a Notebook Card.Click anywhere outside of the Card to save it.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-12", "text": "Adding headers\nYou can insert additional header text\u00a0in your Domo Stories Pages\u00a0to divide up content and draw attention to the most important Cards. Headers can be placed before or after layouts as well as other headers.\u00a0\u00a0\nTo add a header,\nOn the Page where you want to add header text,\u00a0select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Click and drag the \u00a0icon from the toolbar on the right side of the Page to the location on the Page where you want the new header.\u00a0When you drag the\u00a0header icon over a\u00a0Page, borders highlight to indicate\u00a0the spots where you can insert the Card.Replace the word \"Title\" with the desired header name.(Optional) Use the\u00a0\u00a0icon to the left of the header to drag it where you want in the Page.\u00a0\nAdding borders\nYou can add horizontal borders to your Domo Stories\u00a0to better break up the content for your viewers. You can insert borders before or after layouts, headers, or other borders.\u00a0\nTo add a border,\nOn the Page where you want to add a border,\u00a0select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Click and drag the \u00a0icon from the toolbar on the right side of the Page to the location on the Page where you want the new border.\u00a0When you drag the\u00a0border icon over a\u00a0Page, borders highlight to indicate\u00a0the spots where you can insert the Card.(Optional) Use the\u00a0\u00a0icon to the left of the border to drag it where you want in the Page.\u00a0\nEditing Domo Stories layouts\u00a0\nA variety of options is available for editing Domo Stories, such as swapping Cards, replacing Cards with other Cards from your Domo instance, moving Cards to the Appendix,\u00a0deleting entire layouts or individual Cards from a layout, etc.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-13", "text": "With all Domo Stories editing options, make sure to click\u00a0Save\u00a0after you make changes so you don't lose your work.\nSwapping Cards\nTo swap Cards, just drag one onto the other.\u00a0\nTo swap two Cards,\nOn the Page where you want to swap Cards, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Drag one Card on top of the other.\u00a0When you drag a Card (indicated by a \u00a0icon) over another Card, a heavy\u00a0line appears around the second Card\u00a0to indicate the first Card will appear in that slot when you drop it.\u00a0Note that when two Cards swap places, they both automatically resize\u00a0themselves\u00a0to fit the new slots.\u00a0\nReplacing Cards\nYou can replace a Card in a Domo Story\u00a0with any existing Card you have access to in your Domo instance. Cards you replace are sent to the \"Appendix\" section at the bottom of the Page.\nTo replace a Card,\nOn the Page where you want to replace a Card, select\u00a0Edit Dashboard\u00a0in the \u00a0menu in the top right corner of the Page.Mouse over the Card you want to replace.\u00a0In the\u00a0Edit Content\u00a0menu that appears, click\u00a0Choose different Card.Locate the Card you want to add by entering its name in the\u00a0Search Cards\u00a0field and clicking on it.\nThe new Card fills the old Card's slot, and the old Card is sent to the \"Appendix\" section at the bottom of the Page.\nMoving Cards to the appendix\nIf you want to move a Card to the Appendix without replacing it with another Card (as explained in the previous steps), you can do so easily.\nTo move a Card to the Appendix,", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-14", "text": "To move a Card to the Appendix,\nOn the Page where you want to move a Card, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Mouse over the Card you want to move.\u00a0In the\u00a0Edit Content\u00a0menu that appears, select Move to appendix.\nThe Card is moved to the Appendix, and the template slot is now empty.\nDeleting individual Cards\nYou can remove a Card from a Domo Story, in which case it is no longer found anywhere in that Page. Note that these instructions show how to do this in the Story Editing mode, but you can also do this when the Editing mode is closed by selecting\u00a0Remove\u00a0in the Card menu (just as you can for any normal\u00a0Card).\u00a0\nTo delete a Card,\nOn the Page where you want to delete a Card, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Mouse over the Card you want to delete.\u00a0In the\u00a0Edit Content\u00a0menu that appears, select Remove from Dashboard.\nDeleting a Domo Story\nYou can delete a Domo Story\u00a0from a Page. When you do this, all of the Cards in the Domo Story\u00a0are moved to the \"Appendix\" section at the bottom of the Page.\nTo delete a Domo Story,\nOn the Page where you want to delete a Domo Story, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Locate the Domo Story you want to delete.\u00a0Click the\u00a0\u00a0icon to the left of the Domo Story.Click\u00a0Remove Layout\u00a0to confirm.\nThe Domo Story is removed, and all Cards are sent to the \"Appendix.\"\nEditing or deleting a header", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-15", "text": "Editing or deleting a header\u00a0\nTo edit a header, you simply click in the header box and replace the name with the name you want. To delete a header, you click the\u00a0\u00a0icon for that header.\nTo edit a header,\u00a0\nOn the Page where you want to edit or delete a header, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Locate the header you want to edit or delete.Do one of the following:To enter a new header, click on the existing header and replace it with the desired header text.To delete the header, click the\u00a0 icon for the header.\nDeleting a border\nTo delete a border, you just click the\u00a0\u00a0icon for that border.\nTo delete a border,\nOn the Page where you want to delete a border, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Locate the border you want to delete.Click the\u00a0 icon for the border.\nChanging the background color for a Domo Story\nYou can change the background color for all Cards in a given Domo Story. This changes is applied to all Cards except for any in which you have already set an individual background color (discussed more in the next section).\nTo change the background color for a Domo Story,\n\u00a0On the Page where you want to change a Story background color, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Locate and hover over the Domo Story you want to change the background color for.Click the palette icon\u00a0\u00a0.Click one of the provided colors from the palette.(Optional) To remove a fill from a Card, click\u00a0No Fill\u00a0in the color palette.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-16", "text": "Tip:\u00a0When setting the background color of a Card, be sure it doesn't interfere with the colors already set in your chart. Remember that your primary goal is to communicate information to your viewers, not impress them with your flashy color choices!\n\n\n\nCustomizing individual Cards\nIn Domo Stories, there are a number of ways to customize Cards that are not available in standard Pages. These include changing Card background colors, showing or hiding Card elements such as titles and summary numbers, and configuring the actions that take place when users click on Cards.\u00a0\nChanging the background color for a Card\nYou can select a different background color for a Card in a Domo Story by selecting\u00a0Edit Content > Change background.\u00a0This change takes precedence over any background colors you have set for an entire Story\u00a0(discussed in the previous section). So, for example, let's assume you have a Domo Story called \"Street Metrics.\" Within that Story is a Card called \"Net Income.\" You set the background color for that Card to red to draw attention to it. If you then change the overall color scheme of the \"Street Metrics\" Story to blue, all of the Cards in the Story take on blue backgrounds\u00a0except\u00a0for the Card you already set to red.\u00a0\u00a0\nTo change the background color for a Card,\nOn the Page containing the Card you want to change the background color for, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Locate the Card in the Page.Select\u00a0Edit Content > Change background.Click one of the provided colors from the palette.(Optional) To remove a fill from a Card, click\u00a0No Fill\u00a0in the color palette.\u00a0\n\n\n \n\nTip:\u00a0When setting the background color of a Card, be sure it doesn't interfere with the colors already set in your chart. Remember that your primary goal is to communicate information to your viewers, not impress them with your flashy color choices!", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-17", "text": "Showing or hiding Card elements\nFor any given Card, you can hide Card elements such as titles, timeframes (i.e. date grains, such as \"By day\"), Summary Numbers, and even charts themselves. For example, you could hide the chart for a Card entirely and show just the Summary Number for the card. You do this by selecting\u00a0Edit Content > Display settings\u00a0then checking or unchecking the elements you want to show or hide.\nVideo - Domo Stories: Summary Metric View\n\n\u00a0\nTo show or hide elements for a Card,\nOn the Page containing the Card you want to show or hide elements for,\u00a0select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Locate the Card in the Page.Select\u00a0Edit Content > Display settings.Check or uncheck the elements you want to show or hide.\nSetting Card interactions\nFor Cards in Domo Stories, you can define the actions that take place when a user clicks on a Card. These actions include the following:\nOpening the Card details view in a new browser tabEnabling interactions Filters\u00a0for all Cards powered by the same DataSet as this Card, either for all such Cards on the Page or for selected CardsOpening other content in Domo, either in the same browser tab or in a different tabOpening a specified web PageEnabling \"Drill in place\" for the Card, which allows users to drill down into the card without having to first open the Details viewDisabling interactions entirely (essentially making the Card a \"flattened\" version of the Card)\nVideo -\u00a0Domo Stories: Drill in Place\n\nCard interactions are\u00a0only\u00a0available for Cards in Domo Stories. They do not work in standard Pages.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-18", "text": "To configure a Card's Details view to open in another browser tab,\nOn the Page with the Card you want to open in another browser tab when clicked, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Mouse over the Card and click\u00a0Edit Content.Select\u00a0Change Interaction.In Section 2, check the\u00a0Open Card details in a new tab\u00a0box.Click\u00a0Save.\nTo configure interaction Filters for a Card,\nOn the Page with the Card you want to configure interaction Filters for, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Mouse over the Card and click\u00a0Edit Content.Select\u00a0Change Interaction.(Optional) If you want to the Details view for the Card to open in a new tab when a user clicks on the Card, check the\u00a0Open Card details in a new tab\u00a0box in Section 2.(Conditional) Check the\u00a0Enable interaction Filters\u00a0box if it is not checked already.Do one of the following:If you want interaction Filters to be applied to all Cards on the Page powered by the same DataSet as this Card, select the Apply to all Cards on Dashboard\u00a0radio button.If you want interaction Filters to be applied to\u00a0selected\u00a0Cards on the Page powered by the same DataSet as this Card, select the\u00a0Apply to selected Cards\u00a0radio button, click Change, select the Cards you want to apply the Filters to, click\u00a0Save, then click\u00a0Save again.\nTo enable Drill in Place for the Card,\nOn the Page with the Card you want to enable Drill in Place on, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Mouse over the Card and click\u00a0Edit Content.Select\u00a0Change Interaction.Select the\u00a0Drill in place\u00a0radio button.Click\u00a0Save.\nFor more information about creating drilldown on a card, see\u00a0Adding Drill Path to Your Chart.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-19", "text": "For more information about creating drilldown on a card, see\u00a0Adding Drill Path to Your Chart.\nTo configure specified Domo content to open when a Card is clicked,\nOn the Page with the Card you want to link to Domo content, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Mouse over the Card and click\u00a0Edit Content.Select\u00a0Change Interaction.In Section 1, select the\u00a0Link to anything in Domo\u00a0radio button.In Section 2, click the\u00a0Select Content\u00a0button.\u00a0Locate the content you want to open when this Card is clicked.Click\u00a0Save.(Optional) If you want the URL to open with a custom name, enter the name in the\u00a0Display as\u00a0field.(Optional) If you want the new content to open in a new browser tab when the Card is clicked, check the\u00a0Open link in a new tab\u00a0box.Click\u00a0Save\u00a0again.\nTo configure an external web Page to open when a Card is clicked,\nOn the Page with the Card you want to link to external\u00a0content, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Mouse over the Card and click\u00a0Edit Content.Select\u00a0Change Interaction.In Section 1, select the\u00a0Link to external web Page\u00a0radio button.In Section 2, in the Web Page address\u00a0field, enter the URL of the Page you want to open when the Card is clicked.(Optional) If you want the URL to open with a custom name, enter the name in the\u00a0Display as\u00a0field.(Optional) If you want the new content to open in a new browser tab when the Card is clicked, check the\u00a0Open link in a new tab\u00a0box.Click\u00a0Save\u00a0again.\nTo disable Card interactions,", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-20", "text": "To disable Card interactions,\nOn the Page with the Card you want to disable interactions for, select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Mouse over the Card and click\u00a0Edit Content.Select\u00a0Change Interaction.In Section 1, click the radio button reading\u00a0Disable Card interactions.\u00a0Click\u00a0Save.\nSetting a Page background color or image\nIf you want, you can set either a background color or image on a Domo Stories Page. You cannot set both a background color and image on the same Page.\u00a0\nSetting a Page background color\nYou can choose colors from a palette or specify a custom color by inputting a hexadecimal value.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-21", "text": "Note:\u00a0When you add a background color, the Card content shrinks slightly to allow for padding between the Card edges and content.\u00a0\u00a0\n\n\n\nVideo - Domo\u00a0Stories Background Colors\n\n\u00a0\nTo change the Page background color,\nOn the Page you want to change the background color of,\u00a0select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.In the\u00a0Options\u00a0menu at the top of the screen, select\u00a0Change background.(Conditional) If\u00a0Image Fill\u00a0is selected in the menu at the top of the dialog, choose\u00a0Color Fill.Choose the color you want from the palette OR specify a custom color by entering a hexadecimal value into the field at the bottom of the dialog.\nSetting a Page background image\nYou can specify a background image to appear in your Domo Stories Pages. You can also choose a background color to be used while your image is loading. You also have the option to set the text color for headers and cards without a background color. Background images are recommended to have a size with a 2:1 ratio (for example, 1000 x 500).\u00a0\n\nVideo - Adding\u00a0Background Images to Domo Stories", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-22", "text": "Video - Adding\u00a0Background Images to Domo Stories\n\n\u00a0\nTo set the background image for a Page,\nOn the Page you want to change the background color of,\u00a0select\u00a0Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.In the\u00a0Options\u00a0menu at the top of the screen, select\u00a0Change background.(Conditional) If\u00a0Color\u00a0Fill\u00a0is selected in the menu at the top of the dialog, choose\u00a0Image\u00a0Fill.Click\u00a0Choose Image.Locate and select your image on your machine.(Optional) To change the default background color (which is only used before the image has loaded on the Page), click\u00a0Change\u00a0then choose the new color from the palette (or enter a hexadecimal code in the field at the bottom of the palette).(Optional) To change the default text color used on the Page, select the new color in the menu under \"Default text color.\"\u00a0Currently only black and white are available as text colors.\nIf you want to change your background image, click\u00a0Choose New Image\u00a0at the bottom of the dialog then locate and select the new image on your machine.\nTurning on fullscreen mode\nIn Domo Stories Pages, a fullscreen mode is available. When this mode is active, all Cards in the Page are visible, while all other interface elements such as navigation components, menus, etc. are hidden.\u00a0\nTo turn on fullscreen mode,\nSelect\u00a0Full Screen\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.To exit fullscreen mode, press Escape.\nTurning on mobile preview\nIn Domo Stories Pages, you can turn on a preview showing how your Stories will appear when viewed on smartphones. This preview will be the same across all major smartphone brands, notwithstanding a few UI elements specific to certain brands. There is no preview for tablets because the tablet layout is nearly the same as the standard web layout.\u00a0\nVideo - Using the Mobile Preview in Domo Stories", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "378770b027f4-23", "text": "To turn on the mobile preview,\nOn the Page you want to change the view of, select Edit Dashboard\u00a0in the\u00a0\u00a0menu in the top right corner of the Page.Click the\u00a0Desktop\u00a0menu in the top right area of the screen.Select\u00a0Mobile.\nTo switch back to\u00a0Desktop, select\u00a0Desktop\u00a0in the same menu.\nTurning on auto width\nWith the Auto-Width Dashboard setting, Dashboard content will expand to fill all the available window space. This will allow for more rows/columns to be visible from the Dashboard view. This is enabled on a per-Dashboard basis to allow you to customize your Stories.\nTo turn on auto width,\nOn the Page you want to enable auto width, select Edit Dashboard in the details_gear_icon.png menu in the top right corner of the Page.Click the Options menu in the top right area of the screen.Select Auto width.", "source": "../../raw_kb/article/creating_domo_stories/index.html", "title": "Creating Domo Stories"}, {"objectID": "24bd41433b95-0", "text": "Title\n\nCreating Segments in Analyzer\n\nArticle Body\n\nIntro\nThe Segments feature allows you to create better comparisons of a Segment versus another group. It also allows you to dynamically filter one item and not have the filter impact the Segment so that you can compare the two results. You can add a Segment to any multi-series chart, period over period chart, or multi-value chart. You can also add Segments to the Card Details view of a Card and to Dashboards.\nVideo - Segments\n\n\nParts of the Segment Creation Window\nFieldDescriptionSegment nameAdd the name of the Segment.\n\n\n\n\n\nNote: Ensure that the name of the Segment matches the item you are filtering in your data. For example, if you are selecting where the Product column equals \"Toys\" then you would name the Segment Toys. Otherwise, your chart will display both the original data for \"Toys\" and the new Segment for \"Toys\" instead of the Segment replacing the existing data for \"Toys\".\n\n\nDescriptionAdd an optional description of what the Segment contains.Filter columns to create a segmentAdd the columns that define the Segment. These will filter the Segment.Do not filter this segment by the following columnsAdd columns that will not filter the Segment.Segment Color Rules - Card ColorChoose what color the Segment value will show on the chart. The color selected will only display on this specific Card.\n\n\n\n\n\nNote: If a Card Color is selected it will override the DataSet Color.", "source": "../../raw_kb/article/creating_segments_in_analyzer/index.html", "title": "Creating Segments in Analyzer"}, {"objectID": "24bd41433b95-1", "text": "Note: If a Card Color is selected it will override the DataSet Color.\n\n\nSegment Color Rules - Dataset ColorChoose what color the Segment value will show on the DataSet. The Segment will always display this unless a Card Color has been set.Segment previewThis will display the columns defined in the Segment.\nCreating a Segment\nOpen a Card in Analyzer.Select the Add Dynamic Segment button under the Dimensions and Measures OR select Segments from the Analyzer Toolbar.Enter the Segment name.(Optional) Enter a Description of the Segment. This will help users to determine what Segments they should use if there are multiple on the DataSet.Click the + icon and add the column that you want to filter the Segment to the Filter columns to create a segment section.Select the value in the column that you wish to filter the Segment by and click Apply.Repeat steps 5 and 6 until all of the desired columns are added.Click the + icon and add the column that you do not want to filter the Segment to the Do not filter this segment by the following columns section.Repeat step 8 until all of the desired columns are added.(Optional) Select either a Card Color or a Dataset Color to apply to the Segment.Click Save & Close to save the Segment.\nBelow is an example Segment that was used in the corresponding screenshots:", "source": "../../raw_kb/article/creating_segments_in_analyzer/index.html", "title": "Creating Segments in Analyzer"}, {"objectID": "24bd41433b95-2", "text": "Using a Segment in a Card\nOpen a Card in Analyzer.Select an existing Segment from the list of columns under the Beast Modes & Segments section OR create a new Segment by following the steps in the Creating a Segment section.Drag the Segment to the Segments section next to the Series in the Card.Repeat steps 2 and 3 until all the desired Segments are added.Save the Card.\nUsing a Segment in the Card Details view of a Card\nSelect the Segments tab from the menu on the right-hand side of the chart.Select the Segment(s) you want applied to the Card. If you hover over the Segment name, and there is a Description entered for the Segment, it will be displayed. Up to three Segments can be selected at a time.\nUsing a Segment on a Dashboard\nClick the Filter icon  if the Filter Bar is not currently displayed on the Dashboard.Click the Segments dropdown menu.Select the Segment(s) you want applied to the Dashboard. If you hover over the Segment name, and there is a Description entered for the Segment, it will be displayed.Click Apply.\nFAQ's\nSegments are saved to the DataSet. If a DataSet that has Segments is used on a Dashboard, then the Segments option will display.There can be up to 20 Segments saved to a DataSet.You can disable Segments on Dashboards by going to the Filter Options and toggling off Segments for that Dashboard.", "source": "../../raw_kb/article/creating_segments_in_analyzer/index.html", "title": "Creating Segments in Analyzer"}, {"objectID": "89144918459e-0", "text": "TitleCrimson Hexagon ConnectorArticle BodyIntro\nCrimson Hexagon is an enterprise social media analytics company providing insights for brand strategy, market research, and more. To learn more about the Crimson Hexagon API, visit their page ( http://apidocs.crimsonhexagon.com/re...ce#results-api ).\nPrimary Use CasesThis connector helps you answer business questions regarding consumer trends, sentiment, competitors, and category-level conversations.Primary MetricsMentions volume over timeNet sentimentEmotions breakdownSocial engagementMentions by geographyDemographic breakdowns.Primary Company RolesMarketing VPMarketing directorMarketing managerAverage Implementation Time~1 hourEase of Use (on a 1-to-10 scale with 1 being easiest)3\nYou connect to your Crimson Hexagon account in the Data Center. This topic discusses the fields and menus that are specific to the Crimson Hexagon connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\n\n \n\nNote: Access to the Crimson Hexagon Monitor API is required to use this connector.", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-1", "text": "Best Practices\nIt's recommended that you deploy reports for the same monitor for comparison.Deploy reports for a monitor over a consistent time period.Social reports only pertain to monitors of the specified social platform (i.e. the Facebook reports will only return data for Facebook monitors).\nPrerequisites\nTo connect to your Crimson Hexagon account and create a DataSet, you must have your Crimson Hexagon username and password. You must also reach out to Crimson Hexagon support to enable API access for your username.\u00a0\nConnecting to Your Crimson Hexagon Account\nThis section enumerates the options in the\u00a0 Credentials \u00a0and\u00a0 Details \u00a0panes in the Crimson Hexagon Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Crimson Hexagon account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter your Crimson Hexagon username.PasswordEnter your Crimson Hexagon password.\nOnce you have entered valid Crimson Hexagon credentials, you can use the same account any time you go to create a new Crimson Hexagon DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-2", "text": "MenuDescriptionReportSelect the Crimson Hexagon report you want to run.\u00a0The following reports are available:AuthorsReturns information related to the Twitter authors who have posted in a given monitor. Author information is currently only available for monitors with Twitter content.Day and TimeReturns volume information aggregated by time of day or day of the week, depending on the parameters set by the requesting user. The \"numberOfDocuments\" field shows the total volume of posts for each date.\u00a0Demographics AgeReturns age information about authors in a monitor.Demographics EthnicityReturns ethnicity information (calculated using US census data and definitions) about authors in a monitor. The information in this endpoint is only available to US-based customers. Users from non-US customers attempting to access this endpoint will receive an error response.Facebook Admin PostsReturns posts made by the administrator(s) of a targeted Facebook page in a Facebook Social Account Monitor.Facebook Page LikesReturns the cumulative daily like totals for a targeted Facebook page in a Facebook Social Account Monitor as of the selected dates.Facebook Total ActivityReturns information about actions made by users and admins for", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-3", "text": "ActivityReturns information about actions made by users and admins for that page.Geography All ResourcesReturns all the geographical locations that you may use to filter monitor results. For API endpoints that take a filter parameter, you may use the information from this endpoint to filter results based on the geolocation fields included in this response. Each of the other geography resources endpoints provides more granular views of information that appears in this endpoint.Geography CitiesReturns all the cities or urban areas defined in the given country that you may use to filter monitor results. Cities and urban areas may span multiple states or regions.Geography StatesReturns all the states or country regions defined in the passed in country that you may use to filter monitor results.Instagram FollowersReturns the cumulative daily follower count for a targeted Instagram account in an Instagram Social Account Monitor as of the selected dates.Instagram Sent MediaEquivalent to Twitter Sent Posts and Facebook Admin Posts, but for media sent by admins in a targeted Instagram account.Instagram Total ActivityReturns information about actions made by users and admins for the selected account. The requested", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-4", "text": "by users and admins for the selected account. The requested monitor must be an Instagram Social Account Monitor.Interest AffinitiesReturns information about the authors in a monitor and their affinity with a range of pre-defined topics. Errors specific to this endpoint are related to the number of posts required to calculate affinities. There must be at least 250 tweets in the selected date range to use Affinities. Each interest must have a minimum of 5 posts and average at least 5 authors per day.Monitor AuditReturns audit information about the selected monitor, sorted from most to least recent.Monitor DetailReturns detailed metadata about the selected monitor, including category information.Monitor ListReturns a list of monitors accessible to the requesting (default) or selected user along with metadata related to those monitors. This endpoint provides the most convenient access to the unique monitor IDs used for requests to other endpoints.Monitor ResultsReturns the results of a given monitor's analysis.Monitor Results by CityReturns the selected monitor's result volume grouped by the cities in the selected country or the whole world.Monitor Results by CountryReturns the selected", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-5", "text": "or the whole world.Monitor Results by CountryReturns the selected monitor's result volume grouped by country.Monitor Results by StateReturns the selected monitor's result volume grouped by the states in the selected country. State information requires the Country parameter to return results.Monitor Training PostsReturns a list of the training posts for a given opinion monitor. The selected monitor must be an opinion monitor; requests for other monitor types will return an error. By default, all training posts for all categories in a monitor will be returned; however you may pass a category ID in your request to get training posts from a specific category. This endpoint is subject to the same content restrictions as other endpoints that return posts.PostsReturns information about posts in a monitor. For Twitter and Google+ content, we are currently unable to show title and contents information via the API.Team ListReturns a list of teams accessible to the requesting user.Top Sites and Content ResourcesReturns volume information related to the sites and content sources (e.g. Twitter, Forums, Blogs, etc.) in a monitor.Twitter Engagement MetricsReturns information about the", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-6", "text": "in a monitor.Twitter Engagement MetricsReturns information about the top hashtags, mentions, and retweets in a monitor. Whereas the Twitter Total Engagement endpoint requires a Twitter Social Account Monitor, this endpoint requires a standard Buzz or Opinion monitor with Twitter content.Twitter FollowersReturns the cumulative daily follower count for a targeted Twitter account in a Twitter Social Account Monitor as of the selected dates.Twitter Sent PostsReturns information about posts sent by the owner of a target Twitter account in a Twitter Social Account Monitor.Twitter Total EngagementReturns information about retweets, replies, and @mentions (collectively, engagement) for a targeted Twitter account in a Twitter Social Account monitor.Word CloudReturns an alphabetized list of the top 300 words in a monitor. This data is generated using documents randomly selected from the pool defined by the submitted start, end, and filter parameters. The words included in the response are the 300 words most frequently used in the documents, excluding common stopwords. Note that we are counting the total number of times each word appears in the documents, not the number of posts", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-7", "text": "appears in the documents, not the number of posts in which each word appears.Monitor NameSelect the name of the monitor you want to pull data for.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Specific Start DateSelect\u00a0the first date in your date range.\u00a0Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-8", "text": "Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-9", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Use Local TimeSelect True to use the time local to the publishing author of a post instead of converting that time to the time zone of the selected monitor.Report Daily ResultsSelect True to have results aggregated by day of week instead of time of day.Country NameSelect the country you want to pull data for.DailySelect True to have results aggregated daily instead of across the selected date range.Team NameSelect the team you want to pull data for.Hide ExcludedIf you select True, categories set as hidden will not be included in category proportion calculations.Geo TaggedSelect True to return only geo-tagged documents.Extended LimitSelect True to increase the returned posts limit from 500 to 10,000.FilterEnter a pipe-separated list of field:value pairs to filter results.\nFor example: \u00a0 'site:blogspot.com,wordpress.com|keywords:android|geolocation:USA.NY.New York;GBR.Greater London.London'\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nTroubleshooting\nAfter selecting the report and monitor type, if you are not able to see the desired monitor, confirm the monitor type in your Crimson Hexagon account.\nFAQ", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "89144918459e-10", "text": "FAQ\nDo I need a certain kind of account with the data service to set up the connector?\nNo. Any Crimson Hexagon account with access to collected data should be usable to set up this connector.\nHow often can the data be updated?\nDatasets should be set to update once a day.\nAre there any API limits that I need to be aware of?\nThe Crimson Hexagon API limits each authenticated user to 120 requests per minute. Note that a single dataset run may make multiple requests to the API.", "source": "../../raw_kb/article/crimson_hexagon_connector/index.html", "title": "Crimson Hexagon Connector"}, {"objectID": "0478468e7856-0", "text": "TitleCriteo Advertising ConnectorArticle BodyIntro\nCriteo's state-of-the-art technology transforms digital advertising into a personal experience that drives better results.\u00a0 To learn more about the Criteo API, visit their page (https://support.criteo.com/s/article?article=API-Getting-Started).\nYou connect to your Criteo Advertising account in the Data Center. This topic discusses the fields and menus that are specific to the Criteo Advertising connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Criteo Advertising account and create a DataSet, you must have the following:\nYour Criteo Advertising username and passwordYour Criteo Advertising app token\nFor information about generating an app token, reach out to your Criteo Advertising client representative.\u00a0\nConnecting to Your Criteo Advertising Account\nThis section enumerates the options in the Credentials and Details panes in the Criteo Advertising Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Criteo Advertising account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter your Criteo\u00a0Advertising username.PasswordEnter your Criteo Advertising password.App TokenEnter your Criteo Advertising app token.\nOnce you have entered valid Criteo Advertising credentials, you can use the same account any time you go to create a new Criteo Advertising DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane", "source": "../../raw_kb/article/criteo_advertising_connector/index.html", "title": "Criteo Advertising Connector"}, {"objectID": "0478468e7856-1", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus for configuring your report.\nMenuDescriptionReportSelect the Criteo Advertising report you want to run. Currently only a single report is available.\u00a0Advertising ReportsReturns available performance reports.Report TypeSelect the desired report type, either Banner, Campaign, or Category.Banner IDs (Optional)Enter a comma-separated list of banner IDs you want to pull data for.Campaign IDsSelect all of the campaigns you want to retrieve data for.Category IDsSelect all of the categories you want to retrieve data for.Report ColumnsSelect the columns you want to appear in your report.Aggregation TypeSelect whether you want your report dates to be aggregated by hour or by day.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/criteo_advertising_connector/index.html", "title": "Criteo Advertising Connector"}, {"objectID": "0478468e7856-2", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nCan I use the same account to create multiple DataSets?\nYes.\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits I should be aware of?\nNo.", "source": "../../raw_kb/article/criteo_advertising_connector/index.html", "title": "Criteo Advertising Connector"}, {"objectID": "72e36dced6c3-0", "text": "TitleCriteo ConnectorArticle BodyIntro\nCriteo is a personalized retargeting company that works with Internet retailers to serve personalized online display advertisements to consumers who have previously visited the advertiser's website.\u00a0To learn more about the Criteo API, visit their API documentation.\nYou connect to your Criteo account in the Data Center. This topic discusses the fields and menus that are specific to the Criteo connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Criteo account and create a DataSet, you must have the Client ID and Client Secret associated with your Criteo account.\nConnecting to Your Criteo Account\nThis section enumerates the options in the Credentials and Details panes in the Criteo Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Criteo account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionClient IDEnter your Criteo account client ID.Client SecretEnter your Criteo account client secret.\nOnce you have entered valid Criteo credentials, you can use the same account any time you go to create a new Criteo DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Report menu, along with various other menus for configuring your report.", "source": "../../raw_kb/article/criteo_connector/index.html", "title": "Criteo Connector"}, {"objectID": "72e36dced6c3-1", "text": "This pane contains a primary\u00a0Report menu, along with various other menus for configuring your report.\nMenuDescriptionReportSelect the Criteo report you want to run. Currently only one report is available.Ad SetsRetrieves the list of ad sets.AudiencesRetrieves the list of audiences.Campaigns And Categories BidsRetrieves the bids for campaigns and their categories.CampaignsRetrieves the list of campaigns.Campaigns V2Retrieves a list of the latest campaigns.StatisticsGenerates statistics report.Advertisers\u00a0selectionSpecify whether you want to select all advertisers or specific advertisers.Advertiser IDSelect the advertisers.Campaign StatusSelect whether to retrieve data for all campaigns or only active campaigns.Bid TypeSelect the bid type you want to retrieve data with.DimensionsSelect the dimensions to retrieve.MetricsSelect the metrics to retrieve.CurrencySelect the currency to use for the report.Date selectionSelect whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/criteo_connector/index.html", "title": "Criteo Connector"}, {"objectID": "72e36dced6c3-2", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/criteo_connector/index.html", "title": "Criteo Connector"}, {"objectID": "09e7ab9deab0-0", "text": "TitleCriteo Marketing ConnectorArticle BodyIntro\nCriteo is a personalized re-targeting company that works with Internet retailers to serve personalized online display advertisements to consumers who have previously visited the advertiser's website. Criteo was built on the open Internet and believes in the opportunity, choice, and freedom that it offers to everyone. Use DOMO's Criteo Marketing connector for better advertising solutions, and to get access to the data and technology needed to attract and maintain your customers. To learn more about the Criteo Marketing\u00a0API, visit https://support.criteo.com/s/article?article=Criteo-Marketing-API-Intro&language=en_US.\nYou connect to your Criteo account in the Data Center. This topic discusses the fields and menus that are specific to the Criteo connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding\u00a0a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Criteo Marketing account and create a DataSet, you must have the following:\nYour Criteo\u00a0Client IDYour Criteo\u00a0Client Secret\nConnecting to Your Criteo Marketing\u00a0Account\nThis section enumerates the options in the Credentials and Details panes in the Criteo Marketing Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Criteo Marketing account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionClient IDEnter the Client ID associated with your\u00a0Criteo account.Client SecretEnter the Client Secret associated with your\u00a0Criteo account.", "source": "../../raw_kb/article/criteo_marketing_connector/index.html", "title": "Criteo Marketing Connector"}, {"objectID": "09e7ab9deab0-1", "text": "Once you have entered valid Criteo credentials, you can use the same account any time you go to create a new Criteo Marketing DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Criteo Marketing\u00a0report you want to run.\u00a0The following reports are available:Advertisers CategoriesRetrieves all advertisers categories.AudiencesRetrieves the list of audiences.BudgetsRetrieves the list of budgets.Campaigns And Categories BidsRetrieves the bids for campaigns and their categories.CampaignsRetrieves the list of campaigns.Campaigns CategoriesRetrieves the list of categories associated with the campaign.CategoriesRetrieves the list of categories with specified filters.SellersRetrieves the list of sellers.Sellers CampaignsRetrieves the list of campaigns with the associated sellers.StatisticsGenerates statistics report.Advertisers\u00a0SelectionSelect whether you want to retrieve data for all advertisers or specific advertisers.Advertiser IDSelect the Advertiser ID(s).Bid TypeSelect the bid type you want to retrieve data with.Campaign StatusSelect the campaign status you want to retrieve data with.Campaigns SelectionSelect whether you want to retrieve data for all campaigns or a specific campaigns.Campaign IDSelect the Campaign ID.Report TypeSelect the report type you want to retrieve data with.Ignore Cross DeviceSelect True if you want to ignore cross device data.DimensionsSelect the dimensions. You can select between one to three dimensions at a time.Metricsselect the metrics.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/criteo_marketing_connector/index.html", "title": "Criteo Marketing Connector"}, {"objectID": "4b86a649adff-0", "text": "Title\n\nCSV Advanced Connector\n\nArticle Body\n\nIntro\nA character-separated values (CSV) file stores tabular data in plain text form. The Domo CSV Advanced connector allows you to access CSV files that you have uploaded to an SFTP server.\u00a0You can also import files via HTTPS calls.\n\n\n\u00a0\n\nNote: Due to security concerns, Domo no longer permits uploading data using FTP. If necessary, please update your server settings to support SFTP instead of FTP.\u00a0\n\n\n\nYou connect to CSV files in the Data Center. This topic discusses the fields and menus that are specific to the CSV Advanced connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesUse this connector to access CSV files from an SFTP server.Primary MetricsN/APrimary Company RolesAnalystsAverage Implementation Time1 hourEase of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices\nIf you are using SFTP, make sure your credentials are ready and verified via your SSH key. The accepted formats for the key are either DES or RSA. If you are using HTTPS, you only need to verify that your resource is protected.\nPrerequisites\nIf you plan to access CSV files by connecting to an SFTP server, you must have a username and password to authenticate to that server. If you plan to access CSV files over HTTPS, credentials are required only if the resource is protected.\nYou need the username and password associated with your CSV file. You also your SSH key and the passphrase for your ssh key.\n\n\n\u00a0\n\nNote: Domo does not support the SSH keys generated using ssh-keygen. The SSH keys need to be the DES or RSA keys (in PEM format) generated by OpenSSL.", "source": "../../raw_kb/article/csv_advanced_connector/index.html", "title": "CSV Advanced Connector"}, {"objectID": "4b86a649adff-1", "text": "Connecting to CSV Files\nThis section enumerates the options in the Credentials, Details, and Advanced panes in the CSV Advanced Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nIn this pane you\u00a0may need to enter a username and password, depending on what protocol you are using to access CSV data and whether the data is protected.\u00a0If you plan to access CSV files by connecting to an SFTP server, you must enter the credentials for that server here (username, password, and host name). If you plan to access files over HTTPS, you only need to enter credentials if the resource is protected. You must also whitelist the IP addresses from the following article on the server:\u00a0Whitelisting IP Addresses for Connectors.\nIn either case, once you click Next, this CSV account is saved and you can use the same account any time you go to create a new\u00a0CSV Advanced\u00a0DataSet. You can manage connector accounts in the Accounts tab in the\u00a0Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nYou may also have the option to import the CSV file from a URL. If this is the case, you need the URL and potentially a username and password, depending on whether the source requires them.\nDetails Pane\nIn this pane you choose a protocol (HTTP request or SFTP file transfer) then fill in the details for that protocol. This pane includes the following fields and menus:\n\n\n\u00a0\n\nNote: Due to security concerns, Domo no longer permits uploading data using FTP. If necessary, please update your server settings to support SFTP instead of FTP.", "source": "../../raw_kb/article/csv_advanced_connector/index.html", "title": "CSV Advanced Connector"}, {"objectID": "4b86a649adff-2", "text": "MenuDescriptionProtocolSelect\u00a0a protocol for accessing the CSV file.\nUse SFTP\u00a0to access a\u00a0CSV file\u00a0located on an SFTP server.\nUse HTTP Request to access a\u00a0CSV\u00a0file using an HTTP GET request. The request is triggered by\u00a0the use of a unique URL parameter defined by the service providing the CSV file.URLEnter the URL for the CSV file.\nIf using the File Transfer protocol, enter the URL to the folder containing the CSV file. For example: sftp://hostname/home/test. You'll also have to include the port in the URL if the SFTP server is not configured to run on the standard port 22. The 1234 port specific URL would look like sftp://hostname:1234/home/test\n\n\n\u00a0\n\nNote: Due to security concerns, Domo no longer permits uploading data using FTP. If necessary, please update your server settings to support SFTP instead of FTP.", "source": "../../raw_kb/article/csv_advanced_connector/index.html", "title": "CSV Advanced Connector"}, {"objectID": "4b86a649adff-3", "text": "If using the HTTP Request protocol, enter the full URL for the CSV file. You must provide the URL as well as the fully qualified path, including the filename. For example: http://www.ferc.gov/docs-filing/eqr/...v/contract.txt\nIf you are using the SFTP protocol, you can request a file for a specific date. You need to name your file using the format file_[yyyyMMdd]_suffix.csv. You can also select the most recent file using the format filenameprefix_[latest].File Location MethodSelect whether you want to select a specific file from the\u00a0specified directory or search for a file name.File NameEnter the name of the file to search for.Directory FileSelect a file from the specified directory. You can select a CSV or GZ file to parse or a ZIP file to unzip.File TypeSelect the file type for the file you want to access.HTTP HeadersEnter additional headers for\u00a0your HTTP/HTTPS CSV URL, if necessary. Use the format\u00a0header Name1 = 'headerValue1' and headerName2 = 'headerValue2'.HTTP BodyEnter additional request body for\u00a0your HTTPS CSV URL, if necessary.\nAdvanced Pane\nIn this pane you select the delimiter for your CSV file.\nMenuDescriptionDelimiter CharacterSelect the delimiter character used to separate columns in your CSV file.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nTroubleshooting\nVerify that your credentials are correct.If you cannot connect using SFTP, check your firewall settings.\nFAQ\nHow frequently will my data update?\nData will update every hour.\nAre there any API limits that I need to be aware of?\nNo.\nCan I use the same CSV Advanced account\u00a0for multiple datasets?\nYes.\nWhat do I do if I cannot connect using SFTP?", "source": "../../raw_kb/article/csv_advanced_connector/index.html", "title": "CSV Advanced Connector"}, {"objectID": "4b86a649adff-4", "text": "Yes.\nWhat do I do if I cannot connect using SFTP?\nIf you cannot connect using SFTP, check your firewall settings.", "source": "../../raw_kb/article/csv_advanced_connector/index.html", "title": "CSV Advanced Connector"}, {"objectID": "a24fd7999758-0", "text": "TitleCSV SFTP Advanced Security ConnectorArticle BodyIntro\nA character-separated values (CSV) file stores tabular data in plain-text form. Domo provides connectors that allow you to access CSV files you have uploaded to an SFTP (Secure File Transfer Protocol) server:\nThe Domo CSV SFTP\u00a0Pull connector (formerly called the \"CSV-SFTP\" connector) allows you to access CSV files that you have uploaded to your own SFTP server.The Domo CSV SFTP Advanced Security connector allows you to access CSV files from a directory or\u00a0a zip file\u00a0that you have uploaded to your own SFTP server.The Domo CSV SFTP Push connector allows you to capture and process CSV files that have been uploaded to Domo's own SFTP. With this connector, Domo gives you credentials, then you push your data to Domo.\u00a0\nThis article discusses the CSV SFTP\u00a0Advanced Security connector. For information about the CSV SFTP Push connector, visit\u00a0CSV SFTP Push Connector.\u00a0\nThe CSV SFTP\u00a0Advanced Security connector is\u00a0a\u00a0\"File\" connector, meaning it\u00a0retrieves files and output\u00a0them to Domo. In the Data Center, you can access the connector page for this\u00a0and other File connectors by clicking\u00a0File\u00a0in the toolbar at the top of the window.\nYou connect to CSV files in the Data Center. This topic discusses the fields and menus that are specific to the\u00a0CSV SFTP Advanced Security connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo authenticate to the\u00a0SFTP server, you must have the following:\nSFTP Host nameSFTP Port numberSFTP Server UsernamePublic Key\nTo obtain your Public Key:", "source": "../../raw_kb/article/csv_sftp_advanced_security_connector/index.html", "title": "CSV SFTP Advanced Security Connector"}, {"objectID": "a24fd7999758-1", "text": "To obtain your Public Key:\u00a0\nEnter the host name, port number, and SFTP server username then click Generate Key.\u00a0Copy the Public key on to the SFTP server.\nYou must also whitelist the IP addresses from the following article on the server: Whitelisting IP Addresses for Connectors.\nConnecting to CSV\u00a0Files\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes for\u00a0the\u00a0CSV SFTP\u00a0Advanced Pull connector.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your SFTP server. The following table describes what is needed for each field:\nFieldDescriptionHostEnter the name of the SFTP host you want to connect to. This\u00a0must\u00a0begin with\u00a0sftp://. For example:\u00a0sftp://hostname.PortEnter the port number for your SFTP server.UsernameEnter the username you use to\u00a0authenticate to the SFTP server.\u00a0\nAfter entering the host, port, and username, click the Generate Key button. Copy your Public Key to the SFTP server.\nLooking to upload a CSV file but do not have an SFTP server? Take a look at\u00a0our\u00a0Workbench solution\u00a0available for\u00a0download in your\u00a0Domo instance.\nDetails Pane\nIn this pane you choose a CSV report from the SFTP server you've connected to.", "source": "../../raw_kb/article/csv_sftp_advanced_security_connector/index.html", "title": "CSV SFTP Advanced Security Connector"}, {"objectID": "a24fd7999758-2", "text": "In this pane you choose a CSV report from the SFTP server you've connected to.\u00a0\nMenuDescriptionDirectorySelect the directory that contains the file you want to access.File TypeSpecify file type of the selected file from the directory.CSVSelect CSV if you want to access a CSV file.ZIPSelect ZIP if you want to access a ZIP file.How would you like to choose your CSV file?Select how would you like to choose the file name.Select fileSelect the file name from the drop-down that you want to pull.Enter file nameEnter the file name manually.Select CSV file nameSelect the CSV file from the drop-down.Enter CSV file nameEnter the CSV file name manually.How would you like to choose your zip file?Select\u00a0how would you like to choose the zip file.Select zip fileSelect a CSV file to parse from the unzip folder.Enter zip file nameEnter a file name to parse from inside the zip archive.How would you like to choose your subfile from the zip file?Select how would you like to choose the subfile in the zip file.Select subfile from zip fileSelect a CSV file to parse from the unzipped folder.Enter the subfile name from zip fileEnter a file name to parse from inside the zip archive.Delimiting characterSelect the delimiting character used in your file. If your delimiter is not listed, select 'Other.'Specify your delimiterEnter the character used to delimit your character separated values (CSV) text.Quote CharacterSelect the desired quote character for parsing CSV files. Double quote is the CSV standard default quote character.Custom Quote CharacterEnter the desired CSV Quote character.Escape CharacterSelect the desired escape character for parsing CSV files.Custom Escape CharacterEnter the desired CSV escape character.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/csv_sftp_advanced_security_connector/index.html", "title": "CSV SFTP Advanced Security Connector"}, {"objectID": "c3c1489f60f5-0", "text": "Title\n\nCSV SFTP Pull Connector\n\nArticle Body", "source": "../../raw_kb/article/csv_sftp_pull_connector/index.html", "title": "CSV SFTP Pull Connector"}, {"objectID": "c3c1489f60f5-1", "text": "Intro\nA character-separated values (CSV) file stores tabular data in plain text form. Domo provides two connectors that allow you to access CSV files you have uploaded to an SFTP\u00a0(Secure File Transfer Protocol) server:\nThe Domo CSV SFTP\u00a0Pull connector (formerly called the \"CSV-SFTP\" connector) allows you to access CSV files that you have uploaded to your own SFTP server.The Domo CSV SFTP Push connector allows you to capture and process CSV files that have been uploaded to Domo's own SFTP. With this connector, Domo gives you credentials, then you push your data to Domo.\u00a0\nThis article discusses the CSV SFTP\u00a0Pull connector. For information about the CSV SFTP Push connector, visit\u00a0CSV SFTP Push Connector.\u00a0\nIf you need to access files located on a directory, use the CSV Advanced connector.\u00a0\nThe CSV SFTP\u00a0Pull connector is\u00a0a\u00a0\"File\" connector, meaning it\u00a0retrieves files and output\u00a0them to Domo. In the Data Center, you can access the connector page for this\u00a0and other File connectors by clicking File in the toolbar at the top of the window.\nYou connect to CSV files in the Data Center. This topic discusses the fields and menus that are specific to the\u00a0CSV connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nYou must have an SFTP host name and a username and password to authenticate to the SFTP\u00a0server.\u00a0You also need to whitelist the appropriate IP addresses for your region. You can access the list of IP addresses, by region, in\u00a0Whitelisting IP Addresses for Connectors & Federated Adapters.\nConnecting to CSV Files", "source": "../../raw_kb/article/csv_sftp_pull_connector/index.html", "title": "CSV SFTP Pull Connector"}, {"objectID": "c3c1489f60f5-2", "text": "Connecting to CSV Files\nThis section enumerates the options in the\u00a0Credentials and Details\u00a0panes for\u00a0the\u00a0CSV SFTP\u00a0Pull connector.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your SFTP server. The following table describes what is needed for each field:\nFieldDescriptionSFTP Host NameEnter the name of the SFTP host you want to connect to. This must begin with sftp://. For example: sftp://hostname.Username\u00a0Enter the username you use to\u00a0authenticate to the SFTP server.\u00a0Password\u00a0Enter the password you use to authenticate to the SFTP server.\u00a0Head Start RowSignifies the row where the header should start.Data Start Rowsignifies the row where the data should be fetched from the CSV file.\nLooking to upload a CSV file but do not have an SFTP server? Take a look at\u00a0our Workbench solution available for\u00a0download in your\u00a0Domo instance.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\n\u00a0\nWhat kind of credentials do I need to power up this connector?\nYou need the username, password and host name of the SFTP server.\nHow does this connector handle the data?\nThe CSV-SFTP connector pulls the data from customer's SFTP server into Domo.\nHow often can the data be updated?\nData will update every hour.\nAre there any API limits that I need to be aware of?\nNo", "source": "../../raw_kb/article/csv_sftp_pull_connector/index.html", "title": "CSV SFTP Pull Connector"}, {"objectID": "ed1a8f9b5ae3-0", "text": "Title\n\nCSV SFTP Push Connector\n\nArticle Body", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-1", "text": "Intro\nA character-separated values (CSV) file stores tabular data in plain text form. Domo provides two connectors that allow you to access CSV files you have uploaded to an SFTP\u00a0(Secure File Transfer Protocol) server:\nThe Domo CSV SFTP Pull connector (formerly known as the \"CSV-SFTP\" connector) allows you to access CSV files that you have uploaded to your own SFTP server.The Domo CSV SFTP\u00a0Push connector\u00a0allows you to capture and process CSV files that have been uploaded to Domo's own SFTP. With this connector, Domo gives you credentials, then you push your data to Domo.\nThis article discusses the CSV SFTP Push connector. For information about the CSV SFTP\u00a0Pull connector, visit\u00a0CSV SFTP\u00a0Pull Connector.\nThe CSV SFTP Push connector is\u00a0a\u00a0\"File\" connector, meaning it\u00a0retrieves files and output\u00a0them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking File in the toolbar at the top of the window.\nYou connect to CSV files\u00a0in the Data Center. This topic discusses the fields and menus that are specific to the CSV SFTP Push connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to a Domo-side SFTP server\u00a0and import a CSV file, you must have an access key in .pem format. If you do not have a key, Domo allows you to generate one.\u00a0\nSending Your CSV File to an SFTP Server", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-2", "text": "Sending Your CSV File to an SFTP Server\nThis section shows you how to set up a connection and configure CSV options in the Credentials and Details panes in the CSV-SFTP Push\u00a0connector page.\u00a0The components of the other panes in this page, Update Mode\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\u00a0\nTo add a CSV DataSet by sending a file to a Domo-hosted SFTP server,", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-3", "text": "Important:\u00a0Intacct limits the number of concurrent connections using one company id. The default limit is 2 connections at a time but may vary based on your level of service and contractual terms with Intacct. If you have a service level agreement that allows for a greater number of concurrent connections, you will need to go into your Domo account in the account center (https://[yourinstance].domo.com/datacenter/accounts) and update the value in your Intacct account. Otherwise, Domo will simply default your account to the normal limit of 2 concurrent requests imposed by Intacct.This change will improve performance and help increase the stability of the Intacct Advance connector. Overall, this should have very little impact on your DataSets, however, the downside of this change is that you may see an increase in the amount of time it takes to retrieve your data into Domo if you do not update your account to reflect the appropriate limit.Additional steps can be taken to mitigate any potential slowdown by scheduling your Intacct Advance connector DataSets in such a way that they do not overlap each other.", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-4", "text": "Select\u00a0\u00a0> Data Center.Click\u00a0File\u00a0in the tab row at the top of the window.Locate\u00a0CSV SFTP\u00a0in the connector list and click it.Enter a name for the new account in the\u00a0Name your account\u00a0field.\tThis is the name that this account will be known by in the connector Accounts page.\u00a0Do one of the following:If you want to generate a key...Select\u00a0Generate a key for me.Check the box acknowledging that you understand you will be able to download your key file only once.Click\u00a0Create Key.\t\t\tA key is downloaded in\u00a0.pem\u00a0format.If you want to enter your own key...Select\u00a0Use my own key.Paste your RSA public key into the empty box.Click\u00a0Save Key.(Conditional) If the first row of your CSV file is not the header row, enter the header row number in the\u00a0Header Row\u00a0field; otherwise leave this field blank.Select the delimiter for your CSV file in the\u00a0Delimiter\u00a0menu (or leave this set to\u00a0Detect delimiter\u00a0if you want this to be determined automatically).\u00a0(Conditional) If there are one or more rows at the bottom of the file you do not want to import, enter the number of rows to skip in the\u00a0Skip Footer Rows\u00a0field. Otherwise leave this blank.(Conditional) If the data in your CSV file starts on a different row than the row following the header row, enter the row number in the\u00a0Starting Data Row\u00a0field. Otherwise leave this blank.Select the desired escape character in the\u00a0Escape Character\u00a0menu.Select the desired quote character in the\u00a0Quote Character\u00a0menu.(Conditional) If you would like Domo to detect your encoding type automatically, leave the\u00a0Detect Encoding\u00a0box checked. Otherwise, uncheck the box then select the desired encoding type in the\u00a0Encoding\u00a0menu.(Conditional) If you would like Domo to keep leading zeros in numeric values like '0123', select this checkbox.", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-5", "text": "Note:\u00a0This will parse the values as text.Select the desired date format in the\u00a0Date Format\u00a0field.Click\u00a0Next.\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nUploading a CSV File Using an SFTP Client\nYou can upload a CSV file to a Domo SFTP server using an SFTP client. These instructions show you how to do this using the FileZilla SFTP client (https://filezilla-project.org). For instructions about uploading files using other SFTP clients, read the documentation for your chosen client or do a Google search.\nTo send a CSV file to a Domo SFTP server using FileZilla,", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-6", "text": "To send a CSV file to a Domo SFTP server using FileZilla,\nConfigure the CSV connector in Domo by following the instructions under Sending Your CSV File to an SFTP Server.\u00a0Move your\u00a0.pem\u00a0file into a directory on your machine.\tIn this example we use the\u00a0/sftp\u00a0directory.Create a CSV file in the\u00a0/sftp\u00a0directory with the same name as the new DataSet's ID (which you can find in the\u00a0How to Upload\u00a0tab in the details view for the DataSet).\tFor example, if your DataSet ID was\u00a0cb4ba6a0-0934-440f-8572-1253b9f7525a, the CSV filename would be\u00a0cb4ba6a0-0934-440f-8572-1253b9f7525a.csv.\u00a0In FileZilla, create a new site in the Site Manager.\u00a0  In the\u00a0General\u00a0tab, configure settings as follows:For this setting......do thisHostEnter the\u00a0Host name\u00a0URL found in the\u00a0How to Upload\u00a0tab in the details view for the DataSet.PortEnter\u00a022.ProtocolSelect\u00a0SFTP.Logon TypeSelect\u00a0Key file.UserEnter the value next to\u00a0User name\u00a0in the\u00a0How to Upload\u00a0tab in the details view for the DataSet.Key fileLocate your\u00a0.pem\u00a0key in the file browser.Open the connection to the site you just configured.Drag the CSV file you want to upload from the Local file window to the Remote site window.  \t\u00a0Close the connection or quit FileZilla.\nYou should now see the file being uploaded into Domo.\nUploading a CSV File Using the Command Line\nYou can send a CSV file to a Domo SFTP server using a command line client. These instructions are specific to Mac/Linux systems and may not work on Windows.", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-7", "text": "To send a CSV file to a Domo SFTP server using the command line,\nConfigure the CSV connector in Domo by following the instructions under\u00a0Sending Your CSV File to an SFTP Server.\u00a0Move your\u00a0.pem\u00a0file into a directory on your machine.\tIn this example we use the\u00a0/sftp\u00a0directory.Create a CSV file in the\u00a0/sftp\u00a0directory with the same name as the new DataSet's ID (which you can find in the\u00a0How to Upload\u00a0tab in the details view for the DataSet).\tFor example, if your DataSet ID was\u00a0cb4ba6a0-0934-440f-8572-1253b9f7525a, the CSV filename would be\u00a0cb4ba6a0-0934-440f-8572-1253b9f7525a.csv.\u00a0In the\u00a0/sftp\u00a0directory, connect to the SFTP server using this command:  sftp \u2013i\u00a0domosftpkey.pem\u00a0username@mycompany.import.domo.com \twhere\u00a0username\u00a0is the value corresponding to\u00a0User name\u00a0in the\u00a0How to Upload\u00a0tab,\u00a0domosftpkey.pem\u00a0is the name of your key, and\u00a0mycompany\u00a0is your Domo domain (i.e. the portion of the URL of your Domo instance immediately following\u00a0http://).\u00a0Use this command to upload the CSV file:  put\u00a0cb4ba6a0-0934-440f-8572-1253b9f7525a.csvExit the server by entering\u00a0quit. \tWhen you exit the server, you will see your file being processed in Domo.\nThere is a possibility you will see the following message when you try to connect to the server:", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-8", "text": "If this occurs, run the following command then try connecting again:\nchmod 600 /sftp/domosftpkey.pem\nwhere\u00a0domosftpkey.pem\u00a0is the name of your key.\nUploading a CSV File Using a Third-Party Service\nYou can send a CSV file to a Domo SFTP server using any of various third-party services. These instructions show you how to deliver analytics reports using Adobe Analytics. For instructions about uploading files using other services, peruse the documentation for those services or do a Google search.\nTo send an Adobe Analytics CSV file to a Domo SFTP server,", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-9", "text": "Configure the CSV connector in Domo by following the instructions under\u00a0Sending Your CSV File to an SFTP Server. Make sure you set the following options:Under\u00a0Generate or Select Key, choose the\u00a0Use my own key\u00a0option. Then paste the RSA public key you received from Adobe in the text field.Under\u00a0Parsing Options...Set the delimiter to\u00a0Comma.Leave\u00a0Includes Header Row\u00a0checked unless you have it set up to not include one in Adobe.Switch to Adobe Analytics and log into your account.Navigate to\u00a0Tools > Data Warehouse.  \t\u00a0In the\u00a0Data Warehouse Request\u00a0tab, enter all the parameters needed to define the report you want to send to Domo. These parameters include\u00a0Request Name,\u00a0Reporting Date,\u00a0Segments,\u00a0Items,\u00a0Breakdowns,\u00a0Metrics, etc.  \t\u00a0In the\u00a0Schedule Delivery\u00a0section at the bottom of the page, click\u00a0Advanced Delivery Options.  \t\u00a0Set delivery options as follows:For this option......do thisReport file nameSelect\u00a0Custom, then enter the Domo DataSet ID in the text field with a\u00a0.csvextension.\u00a0Report FormatSelect\u00a0CSV.Report DestinationSelect\u00a0FTP. Then enter information in the fields as", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-10", "text": "DestinationSelect\u00a0FTP. Then enter information in the fields as follows:Hostsftp://hostname\u00a0(as listed on the\u00a0How to Upload\u00a0tab in Domo; for example\u00a0sftp://modocorp.import.domo.com)Port22DirectoryEnter the\u00a0Username\u00a0value found in the\u00a0How to Upload\u00a0tab in Domo.UsernameEnter the\u00a0Username\u00a0value found in the\u00a0How to Upload\u00a0tab in Domo.PasswordLeave this blank. Adobe will use the RSA public key you entered in step 1 above.Optionally, you can also click the\u00a0Scheduling Options\u00a0tab to specify a one-time immediate delivery or a recurring schedule. \t\u00a0Once you have specified all advanced delivery options, click\u00a0Send.\tYou are taken back to the\u00a0Data Warehouse Request\u00a0page.Click\u00a0Request this Report\u00a0to save all your settings.", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "ed1a8f9b5ae3-11", "text": "Adobe now queues the report and delivers it to Domo when ready. It may take some time for Adobe to complete the request.", "source": "../../raw_kb/article/csv_sftp_push_connector/index.html", "title": "CSV SFTP Push Connector"}, {"objectID": "06ce6251c99a-0", "text": "TitleCSV SFTP Traverse ConnectorArticle BodyIntro\nThe CSV SFTP Traverse Connector is capable of parsing any valid CSV file and retrieving data from files where the file pattern name matched with the filename at the server; It is also able to connect to different data sources or any CSV resource that is hosted remotely and available over FTP protocol.\nA character-separated values (CSV) file stores tabular data in plain text form. Domo provides two connectors that allow you to access CSV files you have uploaded to an SFTP\u00a0(Secure File Transfer Protocol) server:\nThe Domo CSV SFTP Traverse connector (formerly called the \"CSV-SFTP\" connector) allows you to access CSV files that you have uploaded to your own SFTP server.The Domo CSV SFTP Push connector allows you to capture and process CSV files that have been uploaded to Domo's own SFTP. With this connector, Domo gives you credentials, then you push your data to Domo.\u00a0\nThis article discusses the CSV SFTP Traverse connector. For information about the CSV SFTP Push connector, visit CSV SFTP Push Connector.\u00a0\nIf you need to access files located on a directory, use the\u00a0CSV Advanced\u00a0connector.\u00a0\nThe CSV SFTP Traverse connector is a \"File\" connector, meaning it retrieves files and output them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking\u00a0File\u00a0in the toolbar at the top of the window.\nYou connect to CSV files in the Data Center. This topic discusses the fields and menus that are specific to the\u00a0CSV connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites", "source": "../../raw_kb/article/csv_sftp_traverse_connector/index.html", "title": "CSV SFTP Traverse Connector"}, {"objectID": "06ce6251c99a-1", "text": "Prerequisites\nYou must have an SFTP host name and a username and password to authenticate to the SFTP\u00a0server.\u00a0You also need to whitelist the appropriate IP addresses for your region.\u00a0You can access the list of IP addresses, by region, in\u00a0Whitelisting IP Addresses for Connectors & Federated Adapters.\nConnecting to CSV Files\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details panes for the CSV SFTP Traverse connector. The components of the other panes in this page, Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your SFTP server. The following table describes what is needed for each field:\nFieldDescriptionSFTP Host NameEnter the name of the SFTP host you want to connect to. This\u00a0must\u00a0begin with\u00a0sftp://. For example:\u00a0sftp://hostname.Username\u00a0Enter the username you use to\u00a0authenticate to the SFTP server.\u00a0Password\u00a0Enter the password you use to authenticate to the SFTP server.\u00a0Head Start RowSignifies the row where the header should start.Data Start Rowsignifies the row where the data should be fetched from the CSV file.\nLooking to upload a CSV file but do not have an SFTP server? Take a look at\u00a0our\u00a0Workbench solution\u00a0available for\u00a0download in your\u00a0Domo instance.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\n\u00a0\nWhat kind of credentials do I need to power up this connector?\nYou need the username, password and host name of the SFTP server.\nHow does this connector handle the data?", "source": "../../raw_kb/article/csv_sftp_traverse_connector/index.html", "title": "CSV SFTP Traverse Connector"}, {"objectID": "06ce6251c99a-2", "text": "How does this connector handle the data?\nThe CSV-SFTP connector pulls the data from customer's SFTP server into Domo.\nHow often can the data be updated?\nData will update every hour.\nAre there any API limits that I need to be aware of?\nNo", "source": "../../raw_kb/article/csv_sftp_traverse_connector/index.html", "title": "CSV SFTP Traverse Connector"}, {"objectID": "e8ab9402e1c0-0", "text": "Title\n\nCSV Upload Connector\n\nArticle Body\n\nThis connector is now DEPRECATED.\nTo upload CSV files from your local machine or network to Domo, use our File Upload connector.\nTo upload CSV files to Domo from an external SFTP\u00a0server, use\u00a0either our CSV Advanced Connector.\nTo upload CSV files to Domo using Domo's own SFTP server, use the\u00a0CSV SFTP Push Connector.\nTo upload CSV files to Domo from a website, use the\u00a0CSV Advanced Connector.\nTo upload CSV files to Domo from a third-party service, use the connector corresponding to that service, such as Amazon S3, etc.", "source": "../../raw_kb/article/csv_upload_connector/index.html", "title": "CSV Upload Connector"}, {"objectID": "645578dd0fca-0", "text": "TitleCustomizing or Branding Your CourseBuilder AppArticle BodyUse the following graphic to learn how to customize or brand your CourseBuilder app:", "source": "../../raw_kb/article/customizing_or_branding_your_coursebuilder_app/index.html", "title": "Customizing or Branding Your CourseBuilder App"}, {"objectID": "4d04cf715cef-0", "text": "Title\n\nCustom Charts\n\nArticle Body\n\nIntro\nYou can add custom charts via the following formats\u00a0to your Domo instance in the Admin Settings or the Analyzer.\nSVG\u00a0(Scalable Vector Graphics) -\u00a0Are an XML-based markup language for describing two-dimensional based vector graphics.KML (Keyhole Markup Language) - Is a file format used to display geographic data in an Earth browser such as Google Earth.GeoJSON - Is an open standard format designed for representing simple geographical features, along with their non-spatial attributes.TopoJSON - Is an extension of GeoJSON that encodes geospatial topology and that typically provides smaller file sizes.Shape -\u00a0ESRI standard file format. Includes geospatial vector data (import Google Earth and export as KML).\nOnce you have added a\u00a0chart, it becomes available for users to select in the \"Custom Charts\" menu in the Chart Picker in Analyzer. This allows your users\u00a0to build visualizations pertinent to your business that are not available by default in Domo. Depending on the complexity of your charts, you may be able to upload charts without having to edit the SVG, or you may have to make extensive changes, which will most likely require a working knowledge of XML.\nYou can also use the Custom Regions tool to upload a DataSet\u00a0that includes a column for the new custom regions matched with the sub-regions within them. This is not as robust as building your own SVG files but does not require a knowledge of XML. For more information, see\u00a0Creating a Map with Regions.\n\n\n\n \n\n\nNote:\u00a0You must have an \"Admin\" default security role or a custom role with \"Manage All Cards and Pages\" enabled to add custom charts to your Domo instance. For more information about default security roles, see\u00a0Managing Custom Roles.", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-1", "text": "As a simple example, let's say user JJ, an executive\u00a0at\u00a0a doll factory in Hong Kong, wants to build a card showing sales of a new line of dolls throughout all the districts of Hong Kong. With the set of preinstalled maps available in Domo, she would have no way to do this. However, with Domo's custom charts feature, all JJ has to do is go online, download an SVG map of Hong Kong, then add it to her Domo in the Admin Settings or in Analyzer. The map is now available for JJ to use as well as anyone else with card-building\u00a0permissions. JJ creates her card in the same way she would create any other map card\u2014by uploading the DataSet\u00a0with her sales data, opening the Analyzer, selecting the \"Hong Kong\" map type, and plugging in the columns from the DataSet. The resulting map looks as follows:\n\u00a0 \u00a0 \u00a0\nAs a slightly more complex example, let's say user Rodrigo manages a number of time share properties in the United States. These properties are divided across five distinct regions\u2014West, Southwest, Midwest, Southeast, and Northeast. Rodrigo wants to be able to see at a glance the revenue gained from each region. Again, Domo does not include a preinstalled map for regions of the United States (and if it did, the states in each grouping would probably not exactly correspond to the states in the regions in Rodrigo's company). In this case, because custom grouping of regions is required, Rodrigo will need to customize an SVG\u00a0map of the United States to incorporate grouping. Luckily he has a basic knowledge of XML, so without too much trouble he is able to add the required grouping tags in order to divide up the map as needed:\n\u00a0 \u00a0\nThese are two relatively simplistic examples of how Domo users might implement custom charts. More complex examples might include non-geographical maps such as college campuses...\u00a0 \u00a0\u00a0...layout charts for airplane seats...", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-2", "text": "...and even assembly instructions, like the ones for this car bumper:With custom charts, you are limited only by your imagination, expertise, and amount of free time. Be aware that charts like those shown in the previous three examples are extremely complex and will require an extensive knowledge of XML to build. Also be aware that some XML tags are not supported at this time. These will be listed later in this article.", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-3", "text": "Important: The maximum size of an SVG, KML, GeoJSON, TopoJSON, and Shape file\nthat can be imported and converted is 25 MB.\n\n\n\n\n\u00a0\nVideo - Custom Maps and Charts", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-4", "text": "Summary of steps\nIn general, the process for adding custom charts to your Domo\u00a0and building cards from them will go as follows:\nObtain the chart you want to add.Edit the XML if necessary.Add the chart to Domo in Admin Settings or Analyzer.\u00a0Build your chart card in Analyzer.\nObtaining an SVG chart\nIn most cases, you can obtain an SVG chart for use in Domo\u00a0by either a) downloading it from the Internet, or b) building it yourself.\u00a0\nIf you want to install a map for a country not available in Domo, you can download it from any of a number of websites, such as http://www.amcharts.com/svg-maps/ and\u00a0https://simplemaps.com/resources/svg-maps. These sites also include maps for overseas territories such as French Guiana, Aruba, and so on. Once you have downloaded one of these maps, you can immediately upload it to Domo\u2014there is no need to manipulate the XML (unless you are planning to divide it into subregions).\u00a0\nIf you want to build an SVG chart yourself, you can do so using any of a number of vector-based drawing applications such as Adobe Illustrator or Inkscape. For information about these or other applications, please refer to their documentation.\u00a0\nEditing the XML for your chart\nIf your SVG\u00a0is not a simple regional map or requires any customization, you will need to edit the XML to incorporate the necessary changes. Based on the desired customizations and the complexity of the chart, these edits may be miniscule or extremely extensive. It is possible that you may attempt to upload an SVG\u00a0chart\u00a0to Domo and encounter an error; this is usually the result of unsupported tags in your XML. In this case you will need to either strip out the problematic tags or replace them with supported tags.", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-5", "text": "This section provides lists of tags supported by Domo's SVG converter along with those that are not currently supported.\nSupported XML tags\nSupported shape tags\nThe following shape tags are supported:\npath (all path commands are supported except rotating an arc)rectcircleellipselinepolylinepolygon\nOf these, you can include data for all except line and polyline.\u00a0\nSupported style tags\nThe following style tags are supported, either with a tag, in the style attribute, or from CSS embedded in the SVG. (These tags cannot include data.)\nfillstrokestroke-widthfill-opacitystroke-opacityopacity\u00a0\nSupported transforms\nDomo provides LIMITED support for the following transforms:\nmatrix()translate()scale()rotate()\nText tag\nThe\u00a0text\u00a0tag has LIMITED support. You cannot set the font, but Domo will attempt to use whatever font color and size you specify.\u00a0\nrotate() is supported with text tags. You can also use the matrix() transform to rotate a text tag.\u00a0\nUnsupported XML tags\nDomo's SVG converter does\u00a0not\u00a0currently support any of the following:\nGradient paintsPatternsFiltersOther complex fillsEmbedded images\nUnderstanding SVG\u00a0file structure\nThe following XML code was taken from an SVG file for a country map of Colombia.\u00a0This shows the basic format for the file, with extraneous elements removed for simplicity and most path data elements\u00a0condensed (indicated by ellipses). All of the tags used in this file are supported in Domo.", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-6", "text": "path\u00a0tags always include the d\u00a0subtag, which indicates the path data elements\u00a0for a particular\u00a0region, along with at least one other subtag\u00a0to indicate a unique ID or name for\u00a0a region. In the above example, each region path includes an id and name subtag. These\u00a0tags are important because they also correspond to the region columns in your DataSet. When you build a map in Analyzer, you do so by dragging a region column into the \"State Name\" field above your map preview. (For more information about applying columns, see Applying DataSet Columns to Your Chart.) So in this example, your DataSet would need to include at least one of the columns referenced above\u2014either a column with region IDs or names\u2014to match up with your SVG map.\n\n\n \n\n\nNote:\u00a0If an attribute of data-name is found, this will be the name used to target data. If this attribute does not exist, the next in priority is name. If neither of this is specified, id will be used.\u00a0\n\n\n\nYou could also add other subtags\u00a0as necessary to account for other possible names that may be used in a DataSet. For example, DataSets for most country maps include a column for ISO 3166 regional codes. So in the \"Colombia\" example above, the XML\u00a0could\u00a0include a third\u00a0subtag\u00a0with the code for each region. We can simply call this tag code.\u00a0So under the name=\"Amazonas\" tag\u00a0you could add something like code=\"01\"\u00a0(because the ISO 3166 code for Amazonas is \"01\"), and then follow up in similar fashion with the other regions.\u00a0\u00a0\nThe following example shows a simple DataSet\u00a0with columns that correspond to the previous code example (with an \"ISO 3166\" column added as suggested in the preceding paragraph):", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-7", "text": "If you want to combine\u00a0regions in a map into larger regions, you can do so by grouping all <path>\u00a0tags in your SVG file into their appropriate groups then enclosing each group\u00a0within\u00a0<g> and </g> tags. All <g> tags in your file must include an id\u00a0and/or name attribute.\u00a0\nThe following code sample shows a portion of an\u00a0SVG file with grouping included. This code is for a U.S. map divided into a number of different groups. Two of these groups, \"West\" and \"Southwest,\" are represented in this screenshot.", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-8", "text": "To access the full code sample so you can start building USA region maps, click here:\u00a0usa_regions.svg. Remember that you can customize regions as desired just by cutting and pasting <path> tags for states into the appropriate group. For example, this file places Oklahoma into the \"Southwest\" group, but if you wanted to move it into \"Midwest,\" you could do so just by copying and pasting into the group with the id of \"Midwest.\" You can also add or delete groups as necessary.\u00a0\nAdding custom charts to Domo\nOnce you have acquired the chart you want to add to your Domo instance\u2014and edited the XML if necessary\u2014the next step is to upload it to your Domo. You can do this in either of the following locations:\nIn\u00a0Admin Settings > Company Settings > Custom Charts. You cannot access\u00a0Company Settings\u00a0unless you have an \"Admin\" default security profile or a custom role with \"Manage All Company Settings\" enabled.\u00a0In the Chart Picker in Analyzer, by choosing\u00a0Custom Charts\u00a0then clicking the \"+\" button at the bottom of the pane. This button does not appear unless you have an \"Admin\" security profile or a custom role with \"Manage All Cards and Pages\" enabled.\nFor more information about default security profiles, see\u00a0Managing Custom Roles.\nTo add a custom chart to Domo in the Admin Settings,\nNavigate to\u00a0More\u00a0> Admin Settings > Company Settings > Custom Charts.Click\u00a0Add Chart.Do one of the following:Drag and drop your\u00a0file into the \"Drop file here\" field.Click\u00a0Select File, navigate to the desired\u00a0file, and select it.(Optional) Rename the file and enter a description if desired.Click\u00a0Save.\nTo add a custom chart\u00a0to Domo in the Analyzer,", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-9", "text": "To add a custom chart\u00a0to Domo in the Analyzer,\nOpen the Analyzer for a card.In the Chart Picker, select\u00a0Custom Charts.Click the \"+\" button at the bottom of the\u00a0Custom Charts\u00a0pane.\u00a0Do one of the following:Drag and drop your\u00a0file into the \"Drop file here\" field.Click\u00a0Select File, navigate to the desired\u00a0file, and select it.\u00a0(Optional) Rename the file and enter a description if desired.Click\u00a0Save.\nThe chart is now available in the\u00a0Custom Charts\u00a0dropdown in the Chart Picker in Analyzer. If at any time you want to delete this chart from Domo, you can do so by going into\u00a0Admin Settings > Company Settings > Custom Charts, selecting the chart, then choosing\u00a0Delete\u00a0from the wrench menu in the top right corner. Similarly, if you want to switch the\u00a0file for a chart, you can do so by choosing\u00a0Change file\u00a0from this same menu.\nBuilding your chart in Analyzer\nAfter you upload your chart to Domo, you can build a card from it in Analyzer just as you would any country map. For instructions about building country maps, see\u00a0Country Map.\u00a0Note that these instructions hold true for any custom chart you upload into Domo, whether or not it is a geographical map.\u00a0\u00a0\nChart properties for custom charts are the same as those used in most geographical maps. For a list of these properties, see\u00a0Properties for Maps.\nAdding drill path to custom charts", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "4d04cf715cef-10", "text": "Adding drill path to custom charts\nYou can add drill path to custom charts so you can drill from one custom chart to another. You do this the same way as you would for any other chart. Note that for this to work, you must give the custom chart you are drilling to the same name\u00a0as that of the group on the top chart you are drilling on. For example, if you had a custom regional map of America, and you wanted to drill to another custom chart on the \"Western Region,\" then the next custom map would\u00a0need to be called \"Western Region.\"", "source": "../../raw_kb/article/custom_charts/index.html", "title": "Custom Charts"}, {"objectID": "007a0f075217-0", "text": "Title\n\nCvent Connector\n\nArticle Body", "source": "../../raw_kb/article/cvent_connector/index.html", "title": "Cvent Connector"}, {"objectID": "007a0f075217-1", "text": "Intro\nCvent, Inc. is a publicly held software-as-a-service company that specializes in meetings management technology. To learn more about the Cvent API, visit their page (http://www.cvent.com/en/event-manage...grations.shtml).\nYou connect to your Cvent account in the Data Center. This topic discusses the fields and menus that are specific to the Cvent connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\u00a0\nPrimary Use CasesUse this connector to analyze and report conference metrics to learn from and optimize these large meetings.Primary MetricsHow many people signed up for a meeting?How many people actually showed up?How many total people attended each meeting?What survey feedback are we getting?Primary Company RolesIt varies based on the organization, but most companies will want C-level roles to have access to the data at some point.Average Implementation Time1 hourEase of Use (on a 1-to-10 scale with 1 being easiest)2\nPrerequisites\nTo connect to your Cvent account and create a DataSet, you must have the following:\nYour Cvent account number. You can find this in your Cvent Administration Profile page.Your Cvent API username and password. You can request these credentials from Cvent.\nBefore you connect to Cvent in Domo, you should whitelist\u00a0the following IP addresses to your Cvent account:\n54.208.95.237\n54.208.87.122\n54.208.95.167\n54.208.94.194\n50.207.240.162\n50.207.241.62\n52.18.90.222\n52.62.103.83\n34.198.214.100\n\u00a0\nConnecting to Your Cvent Account", "source": "../../raw_kb/article/cvent_connector/index.html", "title": "Cvent Connector"}, {"objectID": "007a0f075217-2", "text": "34.198.214.100\n\u00a0\nConnecting to Your Cvent Account\nThis section enumerates the options in the Credentials and Details panes in the Cvent Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Cvent account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionCvent Account NumberEnter your Cvent account number.Cvent API UsernameEnter your Cvent API username.Cvent API PasswordEnter your Cvent API password.Cvent EnvironmentSelect respective Environment from the dropdown. i.e. Production Account, Sandbox Account and EU Production Account.\nFor information about obtaining these credentials, see \"Prerequisites,\" above.\nOnce you have entered valid Cvent credentials, you can use the same account any time you go to create a new Cvent DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a single menu from which you select a Cvent report.", "source": "../../raw_kb/article/cvent_connector/index.html", "title": "Cvent Connector"}, {"objectID": "007a0f075217-3", "text": "MenuDescriptionReportSelect the Cvent report you want to run.\u00a0The following reports are available:AppointmentReturns a list of appointments for the authenticated user's account.Appointment AttendeeReturns a list of appointment attendees for the authenticated user's account.ApproverReturns a list of approvers for the authenticated user's account.BudgetReturns budget details for the authenticated user's account, such as tax type, gratuity type, etc.Budget ItemReturns a list of budget items for the authenticated user's account.CampaignReturns a list of campaigns for the authenticated user's account.ContactsReturns a list of contacts for the authenticated user's account.Contacts Custom DetailsReturns a list of contact custom details for the authenticated user's account.Contact GroupReturns a list of contact groups for the authenticated user's account.Conversion RateReturns conversion rate information for the authenticated user's account.eMarketing Email HistoryReturns the eMarketing email history for the authenticated user's account.Event\u00a0Returns a list of events for the authenticated user's account.Event CardsReturns a list of event cards for the authenticated user's account.Event DetailsReturns event details for the authenticated user's account.Event Email HistoryReturns the event email history for", "source": "../../raw_kb/article/cvent_connector/index.html", "title": "Cvent Connector"}, {"objectID": "007a0f075217-4", "text": "user's account.Event Email HistoryReturns the event email history for the authenticated user's account.Event QuestionReturns event questions for the authenticated user's account.GuestReturns a list of guests for the authenticated user's account.InviteeReturns a list of invitees for the authenticated user's account.Meeting RequestReturns a list of meeting requests for the authenticated user's account.Meeting Request Custom DetailsReturns meeting request custom details for the authenticated user's account.Meeting Request Sleeping Room DetailsReturns meeting request sleeping room details for the authenticated user's account.Meeting Request UserReturns a list of meeting request users for the authenticated user's account.Meeting Request SurveyReturns a list of meeting request surveys for the authenticated user's account.ProposalReturns a list of proposals for the authenticated user's account.Proposal Sleeping RoomReturns a list of proposal sleeping rooms for the authenticated user's account.Proposal Estimated CostsReturns a list of guests for the authenticated user's account.Rate HistoryReturns the rate history for the authenticated user's account.RegistrationReturns registration information for the authenticated user's account.Registration with Order DetailsReturns registration information with order details for the authenticated user's account.Registration", "source": "../../raw_kb/article/cvent_connector/index.html", "title": "Cvent Connector"}, {"objectID": "007a0f075217-5", "text": "information with order details for the authenticated user's account.Registration with Survey DetailsReturns registration with survey details for the authenticated user's account.RespondentReturns a list of respondents for the authenticated user's account.ResponseReturns a list of responses for the authenticated user's account.RFPReturns RFP details for the authenticated user's account.RFP Meeting Room DetailsReturns RFP meeting room details for the authenticated user's account.RFP Question DetailsReturns RFP question details for the authenticated user's account.RFP Sleeping Room DetailsReturns RFP sleeping room details for the authenticated user's account.RFP Supplier DetailsReturns RFP supplier details for the authenticated user's account.SessionReturns session details for the authenticated user's account.SpeakerReturns a list of speakers for the authenticated user's account.SupplierReturns a list of suppliers for the authenticated user's account.Supplier ProposalReturns a list of supplier proposals for the authenticated user's account.Supplier RFPReturns supplier RFP details for the authenticated user's account.SurveyReturns a list of surveys for the authenticated user's account.Survey Email HistoryReturns the survey email history for the authenticated user's account.Survey QuestionReturns a list of survey questions", "source": "../../raw_kb/article/cvent_connector/index.html", "title": "Cvent Connector"}, {"objectID": "007a0f075217-6", "text": "user's account.Survey QuestionReturns a list of survey questions for the authenticated user's account.Table AssignmentReturns a list of table assignments for the authenticated user's account.TransactionReturns a list of transactions for the authenticated user's account.UserReturns a list of users for the authenticated user's account.User GroupReturns a list of user groups for the authenticated user's account.User RoleReturns a list of user roles for the authenticated user's account.", "source": "../../raw_kb/article/cvent_connector/index.html", "title": "Cvent Connector"}, {"objectID": "007a0f075217-7", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nTroubleshooting\nCheck your credentials to make sure you have the proper access rights.", "source": "../../raw_kb/article/cvent_connector/index.html", "title": "Cvent Connector"}, {"objectID": "5f2b3eac0a1e-0", "text": "TitleCvent REST ConnectorArticle BodyIntro\nCvent, Inc. is a publicly held software-as-a-service company that specializes in meetings, events, and hospitality management technology. Use Domo's Cvent REST Connector to retrieve data for various insights and metrics about attendees, contacts, events, surveys, and so on.\nExperience enhanced performance and make faster decisions by integrating Cvent data with your existing CRM and Marketing Automation systems. By combining your Cvent data with Domo\u2019s leading business intelligence platform, you\u2019ll develop comprehensive strategies to fully optimize your business. Within minutes you\u2019ll be able to create customized reports for your most important KPIs. To learn more the API, visit Cvent's website.\nYou connect to your Cvent account in the Data Center. This topic discusses the fields and menus that are specific to the Cvent connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\u00a0\nPrimary Use CasesUse this connector to analyze and report conference metrics to learn from and optimize these large meetings.Primary MetricsHow many people signed up for an event?How many people actually showed up?How many total people attended each meeting, survey, event?What survey feedback are we getting?Primary Company RolesIt varies based on the organization, but most companies will want C-level roles to have access to the data at some point.Average Implementation Time1 hourEase of Use (on a 1-to-10 scale with 1 being easiest)2\nPrerequisites\nTo connect to your Cvent account and create a DataSet, you must have the following:\nYour Cvent account nameYour client ID and client secret associated with your Cvent app\u00a0You also need to select the region for your Cvent instance", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-1", "text": "Before you connect to Cvent in Domo, you need to whitelist the appropriate IP addresses for your region. You can access the list of IP addresses, by region, in Whitelisting IP Addresses for Connectors & Federated Adapters.\nConnecting to Your Cvent Account\nThis section enumerates the options in the Credentials and Details panes in the Cvent REST Connector page. The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Cvent account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAccount NameEnter the name for your Cvent Account.RegionSelect the region for your Cvent instance.Client IDEnter the client ID from your Cvent app.Client SecretEnter the client secret from your Cvent app.\nFor information about obtaining these credentials, see \"Prerequisites,\" above.\nOnce you have entered valid Cvent credentials, you can use the same account any time you go to create a new Cvent REST DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a single menu from which you select a Cvent report.", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-2", "text": "MenuDescriptionReportSelect the Cvent REST report you want to run. The following reports are available:AppointmentsRetrieves a list of Appointments.Appointment AttendeesRetrieves information of attendees of either events or appointment events.Appointment Meeting InterestsRetrieves information of appointment attendees' preferences in meeting with another organization or individual.AttendeesRetrieves attendees for events associated with your account.Attendance DurationRetrieves attendance duration.Available Appointment TimesRetrieves available appointment times associated with eventsAttendee ActivitiesRetrieves information about your customers activities for an event.Attendee InsightsRetrieves attendee insights for events associated with your account.Booth StaffRetrieves booth staff associated with the exhibitor for an event.Budget ItemsRetrieves a paginated list of budget items for event associated to the account of the access token.Budget TotalsRetrieves a paginated list of budget totals for event associated to the account of the access token.ContactsRetrieves contacts associated with your account.Contact GroupsRetrieves contact group\u00a0associated with your account.EventsRetrieves information about your events.Event SurveysRetrieves surveys associated with an event.Event Survey QuestionsRetrieves questions associated with", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-3", "text": "with an event.Event Survey QuestionsRetrieves questions associated with event survey.Event Survey ResponsesRetrieves responses associated with event survey.E-Literature RequestsRetrieves e-literature requests for an event.Event FeaturesRetrieves event features associated with an event.ExhibitorRetrieves exhibitors for an event.Exhibitor AdminsRetrieves admins associated with exhibitors for an event.List App\u00a0EventsRetrieves\u00a0a paginated list of Appointment Events.List Budget ItemsRetrieves a paginated list of budget items for events associated with the account of the access token.List Budget TotalsRetrieves a paginated list of budget totals for events associated with the account of the access token.List Ext Activities MetadataRetrieves a paginated list of external attendee activities metadata.List All Event ResponsesRetrieves a paginated list of event survey responses for all events and surveys.List Exhibitor QuestionsRetrieves a list of exhibitor questions for a given event id.List Exhibitor AnswersRetrieves a list of answers to exhibitor questions for a given exhibitor id.List Exhibitor's CategoriesRetrieves a paginated list of exhibitor-categories assigned to the provided exhibitor", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-4", "text": "list of exhibitor-categories assigned to the provided exhibitor Id.LeadsRetrieves leads gathered for an event.Meeting RequestsRetrieves information about the Meeting Requests.Meeting Request FormsRetrieves information about the Meeting Request Forms.OrdersRetrieves orders associated with an event.Order ItemsRetrieves order items associated with an event.Qualifying AnswersRetrieves answers provided by leads to qualifying questions.Qualifying QuestionsRetrieves qualifying questions which are asked to identify the lead.ScoreRetrieves engagement scores provided by attendees for events associated with your account.SessionsRetrieves sessions associated with events.Session AttendanceRetrieves session attendance associated with your events.Session EnrollmentsRetrieve enrollments for sessions associated with your events.Session SpeakersRetrieve information about speakers assigned to the provided session ID.SpeakersRetrieves speakers associated with your events.Sponsorship LevelsRetrieves sponsorship levels of exhibitors for your event.Standard SurveysRetrieves the details for the standard survey associated with your account.Standard Survey QuestionsRetrieves questions asked in standard survey.Standard Survey RespondentsRetrieves standard survey respondents.Standard Survey ResponsesRetrieves responses to the standard surveys.StatsRetrieves a quick overview of", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-5", "text": "the standard surveys.StatsRetrieves a quick overview of attendee insight scores.TransactionsRetrieves Transactions associated with an event.Transaction ItemsRetrieves transaction items associated with an event.WebcastsRetrieves information about webcasts associated with your event.Event Survey Selection CriteriaSelect whether you want to retrieve data for all event surveys or selected event surveys.Exclude ContactsSelect this checkbox if you want to exclude the contact information of the attendee.\t\t\tFor example, if an attendee record includes contact information, the Connector will exclude that contact information in your Attendees report.Event Selection CriteriaSelect whether you want to retrieve data for all events or selected events.EventsSelect the events.Event SubreportsSelect the sub reports you want to expand in the data.Survey Selection CriteriaSelect whether you want to retrieve data for all surveys or selected surveys.Standard SurveysSelect the standard surveys.Event SurveysSelect the event surveys.Skip Failing RecordsSelect this checkbox if you want to skip the failing records.", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-6", "text": "For example, select this checkbox in case a selected survey or event has been deleted.Meeting Requests from Selection CriteriaSelect whether you want to retrieve data for all meeting requests form ids or selected meeting requests form ids.Date SelectionSelect the date format for your data.Single DateSelect whether the report data is for a specific date or for a relative number of days back from today.Specific DateSelect the specific date using the date selector.Relative DateEnter the number of days back that you would like to get data for in the\u00a0Days Back\u00a0field. Specify either today or 0, yesterday or 1, or today-7 or 7 to get data for 7 days into the past.Date RangeSelect the specific or relative date range.Start Date - SpecificSelect\u00a0the first date in your date range using the date selector.End Date - SpecificSelect the last date in your date range\u00a0using the date selector.Start Date - RelativeEnter the number of days back that you would like to get data from (start day). Combine with\u00a0End Date\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Start Date\u00a0and\u00a05\u00a0for\u00a0End Date, the report would contain data for\u00a010 days ago up until\u00a05 days ago.End Date - RelativeEnter the number of days back that you would like to get data to (end day). Combine with\u00a0Start Date\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Start Date\u00a0and\u00a05\u00a0for\u00a0End Date, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Time PeriodSpecify the time period that you would like to receive data for.Starting Day of the WeekSelect the day you would like your week to start with.\nOther Panes", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-7", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nTroubleshooting\nCheck your credentials to make sure you have the proper access rights.\nFAQs\nWhat endpoint is the base URL for this connector?\nThe base URL for the Cvent REST connector is https://api-.cvent.com/ea/.\nWhich endpoint does each report call in this connector?", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-8", "text": "Which endpoint does each report call in this connector?\nReport NameEndpoint URLAppointments/appointmentAppointment Attendees/appointment-attendeesAppointment Meeting Interests/appointment-meeting-interestsAttendees/attendeesAttendee Activities/attendees/activitiesAttendee Insights/attendee-insightsBooth Staff/events/{exibitorId}/exhibitors/{eventId}/booth-staffContacts/contactsEvents/eventsEvent Surveys/events/{EventId}/surveysEvent Survey Questions/events/{eventId}/surveys/{surveyId}/questionsEvent Survey Responses/events/{eventId}/surveys/{surveyId}/responsesExibitor/events/{eventId}/exhibitorsExibitor Admins/events/{exibitorId}/exhibitors/{eventId}/adminsLeads/leadsMeeting Requests/meeting-request-forms/{MeetingRequestFormId}/requestsMeeting Request Forms/meeting-request-formsQualifying Answers/events/{eventId}/exhibitors/{exbitorId}/leads/{leadId}/answersQualifying Questions/events/{eventId}/exhibitors/{exhibitorId}/questionsScore/attendee-insights/{attendeeInsightId}/scoresSession/sessionsSession Attendance/sessions/attendanceSession Enrollment/sessions/enrollmentSession Speakers/sessions/{eventIds}/speakersSpeaker/speakersSponsership Levels/events/{eventId}/sponsorship-levelsStandard Surveys/standard-surveysStandard Survey Questions/standard-surveys/{StandardSurveyId}s/questionsStandard Survey Respondents/standard-surveys/{StandardSurveyId}s/respondentsStandard Survey Responses/standard-surveys/responsesStats/attendee-insights/{AttendeeInsightId}/statsWebcasts/webcasts\nWhat kind of credentials do I need to power up this connector?\nYou need your Cvent account name, and the client ID and client secret associated with your Cvent app. You also need to select the region for your Cvent instance.\nCan I use the same account to create multiple datasets?\nYes", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "5f2b3eac0a1e-9", "text": "Can I use the same account to create multiple datasets?\nYes\nHow often can the data be updated?\nAs often as needed.\nDo I need to whitelist any IP addresses?\nBefore you connect to Cvent in Domo, you should whitelist the following IP addresses to your Cvent account:\n54.208.95.23754.208.94.19454.208.87.12254.208.95.16750.207.240.16250.207.241.6234.198.214.100", "source": "../../raw_kb/article/cvent_rest_connector/index.html", "title": "Cvent REST Connector"}, {"objectID": "c5e752ccb76e-0", "text": "TitleDANSDBF Data ConnectorArticle BodyIntro\nDANS DBF Library is a Java library for reading and writing xBase database files. xBase is the name commonly used for dBase and its dialects. The central file in these databases is the DBF file or DataBase File, hence the name of this library.\u00a0 To learn more about DANSDBF library, refer to\u00a0DANS DBF Library.\nThis topic discusses the fields and menus that are specific to the DANSDBF connector user interface. For general information about adding DataSets, setting update schedules, and editing DataSet information, see\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is appropriate for retrieving reading and writing xBase database files.Primary MetricsReturns information for specific types such as Character, Numbers, Logicals, Dates, and Memo.Ease of Use (on a 1-to-10 scale with 1 being easiest)6\nPrerequisites\nTo connect to your\u00a0DANSDBF account and create a DataSet, you must have the following:\nThe username and password.SFTP host name for your DBF File.\nConnecting to Your\u00a0DANSDBF Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the DANSDBF Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains\u00a0three fields where you enter the user name, password, and the SFTP host name of your DBF file.", "source": "../../raw_kb/article/dansdbf_data_connector/index.html", "title": "DANSDBF Data Connector"}, {"objectID": "c5e752ccb76e-1", "text": "After you have entered valid credentials, you can use the same account in Domo any time you create a DANS DBF DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nFieldDescriptionUsernameEnter the username.PasswordEnter the password.HostEnter the SFTP host name for your DBF file.\nDetails Pane\nThis pane contains one field where you enter the\u00a0path to your DBF file.\u00a0For example, if the file is hosted on an SFTP server at /home/test, provide the path as /home/test/*.dbf.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nWhen should I use this connector?\nYou can use the DANS DBF connector to read and write xBase database files.\nHow can I open a DBF file?\nYou can open a DBF file by creating a Table object configuring it with an existing DBF file.\nWhat parameters do you need to check before opening a DBF file?\nLength of record must be > 1 and < max length. (max length = 4000 B in dBASE III and IV, can be 32KB in other systems).The number of records must be >= 0.The .DBF file must have at least one field.The number of fields must be <= the maximum allowable number of fields.File size reported by the operating system must match the logical file size. Logical file size = (Length of header + (Number of records * Length of each record)).\nWhat kind of credentials do I need to access this connector?\nYou need to obtain the username and the password and enter the SFTP host name for your DBF File.", "source": "../../raw_kb/article/dansdbf_data_connector/index.html", "title": "DANSDBF Data Connector"}, {"objectID": "2e9963c23f99-0", "text": "TitleDashboard Design Best PracticesArticle BodyTo download this PDF to your computer, click here.", "source": "../../raw_kb/article/dashboard_design_best_practices/index.html", "title": "Dashboard Design Best Practices"}, {"objectID": "d7b154f7f7c7-0", "text": "TitleDashboard MaturityArticle BodyUse this guide to help your company\u00a0develop a forward-thinking approach to dashboard creation. To download this PDF to your computer, click here.", "source": "../../raw_kb/article/dashboard_maturity/index.html", "title": "Dashboard Maturity"}, {"objectID": "a28170c161d2-0", "text": "TitleDashboard Optimization Best PracticesArticle BodyDashboard Design\nPages & Subpages\nPages are the default method for sharing and publishing content within Domo. Pages are designed to give you the freedom to structure and arrange your data stories. Within each page is the ability to create subpages of related content or reports.\nDetermine which pages and subpages you\u2019ll be creating first by asking who this dashboard is for and which business questions you\u2019re going to solve for within those pages and subpage. Simple navigation is the goal; use a nomenclature/hierarchy of Pages and Subpages that's familiar to the different functions within your company.\nCollections & Cards\nCollections are best used to organize your cards around your business questions. Collections give you the ability to group your Primary Outcome card with the driver cards that answer your questions. Also, you can expand and/or collapse a collection of cards for a more fluid and clean navigation within a page.\nLayout Tips for Collections and Cards\nLimiting your number of Collections to roughly 3-5 Collections per page is ideal for navigation and information consumption.Collection Titles are ideally your most pressing Business Questions that correspond to the function or page in Dashboard.Use the Description in your Collection to define the problems you want to solve and the goals you\u2019re trying to achieve.Use larger cards sizes for your Primary Outcome Cards or Hero Cards.Select 3 to 6 Driver Cards per Collection that are sized smaller than the Hero Card. This will draw more attention and focus to your Primary Outcome(s) while still making it easy to see cards related to the Primary Outcome.\nPage Locking", "source": "../../raw_kb/article/dashboard_optimization_best_practices/index.html", "title": "Dashboard Optimization Best Practices"}, {"objectID": "a28170c161d2-1", "text": "Page Locking\nEvery Domo user has the ability to take the cards and collections created on a page and arrange them to a custom view that makes most sense to them. However, as a Page Owner there are times when you want to lock the page layout so it can\u2019t be rearranged and the view is standardized for all users. In this case Page Owners can lock a page to maintain layout consistency for everyone access the page.", "source": "../../raw_kb/article/dashboard_optimization_best_practices/index.html", "title": "Dashboard Optimization Best Practices"}, {"objectID": "a28170c161d2-2", "text": "Page Filtering\nPage Filtering is best used to simplify and focus dashboards on data that matters most for the end user. For example, instead of a Manager having to build out a separate card for each Region or Country in your Business. You can quickly get to data and answer your questions by using Page Filters and filtering by country. Filters are not permanent and allow you to slice your data on the fly. You can quickly get to the information as well as save time by not having to build out a separate card for each dimension of the data you\u2019re looking for.\n\nRelated Cards\nWhile you can see the cards that are most directly related to a card in your collection the related card feature allows you to link to other cards found on other pages or subpages. This is especially useful if your related cards from other pages can provide more context and meaning to the card you\u2019re viewing.\n\nTracking to Goals/Pacing/Forecasting\nEvery Primary Outcome should have a goal you are tracking against and pacing to hit. We recommend cards that give context around the data being visualized. Visualizing your pace as it relates to the goal gives you quick insight on whether you\u2019re on or off target. If you\u2019re not on track and pacing with your goals, create cards that give you context to what you need to be doing in order to improve (what actions to take) to get back on track/pace.", "source": "../../raw_kb/article/dashboard_optimization_best_practices/index.html", "title": "Dashboard Optimization Best Practices"}, {"objectID": "a28170c161d2-3", "text": "Comparing Historical Data/Trends\nComparing current data to historical data is another effective way to track progress. Historical data provides the context that many executives and managers need to determine how their strategies and activities are making an impact on the growth of the company. This example card allows you to predict where you are and where you will be in comparison to previous months.\nContent: Thinking across Multiple Datasets\nTake a step back and think through the optimal metric required to drive the Primary Outcome you are looking for. What is the real outcome you are trying to impact? Most likely, it will take more than one DataSet to answer it. Don\u2019t limit your dashboard simply by not thinking across DataSets.\nFor example, you could track your campaign effectiveness three ways:\nGood: \u201cLeads by Campaign\u201dBetter: \u201cRevenue by Campaign\u201dBest: \u201cROI or Profitability by Campaign.\u201d\nThe best tracking method typically requires multiple DataSets. Also, remember that your current access to data should not limit the creativity of which metrics you should be using or plan to use to run your business successfully. Identify the optimal metrics, even if you can\u2019t build them now, to keep as a roadmap for where you want to go in the future.", "source": "../../raw_kb/article/dashboard_optimization_best_practices/index.html", "title": "Dashboard Optimization Best Practices"}, {"objectID": "a28170c161d2-4", "text": "Forecasting, Predictive, & Multi-Linear Regression\nDashboards can be especially insightful when you apply Predictive and Multi-Linear Regression Modeling.\nAdditional Elements in Domo to Optimize\nAlerts\nAlerts are especially useful for the decision makers who aren\u2019t interested in logging into the Domo platform every day in order to know what actions they should be taking with their business. Alerts allow business user to set up rules and conditions around their dashboards so as those are met they can get pinged directly when their attention is needed. Alerts tell you where to look for determining actions to take, and lead you to using Buzz as a collaboration tool to enact those decisions.\nBuzz\nBuzz is great tool to implement Best Practices within your instance. As you initially design your dashboards and cards you\u2019ll need quick collaboration and feedback on what you\u2019ve built. The goal is to build the most useful and relevant dashboards that answer your users\u2019 business questions.\nBuzz allows you to quickly iterate and collaborate on those core principles, accelerating the adoption of your Dashboards and Cards. Buzz can take the place of your weekly meetings, as you talk about the data and what decisions you\u2019re going to make, the notifications of a card alert can be acted upon by having a meeting right then over Buzz to determine what you need to do.\nProjects and Tasks\nOut of the business meetings you have in Buzz, with actions you need to happen based on your data, the projects and tasks can be assigned from Buzz conversations directly. By leveraging the value of Domo to establish a routine process of setting alerts, having Buzz collaboration conversations about a notification, and assigning projects and tasks based on the conversations, it allows you to move forward quickly on all of your data.", "source": "../../raw_kb/article/dashboard_optimization_best_practices/index.html", "title": "Dashboard Optimization Best Practices"}, {"objectID": "f5a9ac939d21-0", "text": "TitleDatabase.com ConnectorArticle BodyIntro\nSalesforce Database.com is an enterprise database built for the cloud that\u00a0enables developers to focus on building apps\u00a0instead of managing and maintaining databases and hardware. To learn more about the Database.com API, visit their page (https://www.salesforce.com/platform/database/).\nYou connect to your Database.com account in the Data Center. This topic discusses the fields and menus that are specific to the Database.com connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Database.com account and create a DataSet, you must have credentials for either a Salesforce Database.com or Sandbox account.\nConnecting to Your Database.com Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the Database.com Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThe Domo Database.com connector uses OAuth to connect, so there is no need to enter credentials within Domo. Select the environment you want to pull data from (either Database.com\u00a0or Sandbox) then click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if\u00a0you have\u00a0existing Database.com accounts in Domo) to open the Salesforce\u00a0OAuth screen where you can enter your Salesforce credentials. Once you have entered valid credentials, you can use the same account any time you go to create a new Database.com DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.", "source": "../../raw_kb/article/databasecom_connector/index.html", "title": "Database.com Connector"}, {"objectID": "f5a9ac939d21-1", "text": "Note:\u00a0If you are already logged into Salesforce when you connect in Domo, you are authenticated automatically when you click Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of Salesforce.\n\n\n\nDetails Pane\nThis pane contains menus for configuring how you want to pull data from Database.com.\nMenuDescriptionHow do you want to select your Database.com data?Select how you want to pull data from Database.com. If you select\u00a0Browse Objects\u00a0and Fields, you specify your data by selecting objects with associated fields and relationships. If you select\u00a0Query, you enter an SOQL\u00a0query to retrieve data.\n\u00a0Database.com ObjectsSelect the object (table) containing the data you want to pull into Domo.ColumnsSelect the names of the columns with data you want to pull into Domo.RelationshipsSelect joinable tables on the selected object.QueryEnter the SOQL\u00a0query you want to use to pull data into Domo.\nOther Panes\u00a0\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/databasecom_connector/index.html", "title": "Database.com Connector"}, {"objectID": "315171b5ba74-0", "text": "TitleDatabricks ConnectorArticle BodyIntro\n\u00a0\n\n\n \n\nNote: The Databricks Connector is currently still in development and is not yet available for use.", "source": "../../raw_kb/article/databricks_connector/index.html", "title": "Databricks Connector"}, {"objectID": "315171b5ba74-1", "text": "Databricks is a cloud-based collaborative data science, data engineering, and data analytics platform that combines the best of data warehouses and data lakes into a lakehouse architecture.\nWith Databricks you can access all your data, analytics, and AI on one lake house platform. The simple, open, and collaborative environment helps reduce the infrastructure complexity, keeps control of your data, and makes it easy for your teams to partner across the entire data and workflow. For more information about the Databricks API, visit their website. (https://docs.databricks.com/dev-tools/api/index.html)\nThe Databricks connector is a \"Database\" connector, meaning it retrieves data from a database using a query. In the Data Center, you can access the connector page for this and other Database connectors by clicking Database in the toolbar at the top of the window.\nYou connect to your Databricks database in the Data Center. This topic discusses the fields and menus that are specific to the Databricks connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to a Databricks database and create a DataSet, you must have the following:\nThe username and password you use to log into your Databricks hostThe host name for the databaseThe port number for the databaseThe database name or schema nameThe HTTP Path\nConnecting to Your Databricks Database\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details panes in the Databricks Connector page. The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/databricks_connector/index.html", "title": "Databricks Connector"}, {"objectID": "315171b5ba74-2", "text": "Note:\u00a0When using the copy/paste function for your credentials, ensure there is no whitespace at the beginning or end of the string. By accidentally pasting whitespace, it will cause the connector to error when trying to connect.", "source": "../../raw_kb/article/databricks_connector/index.html", "title": "Databricks Connector"}, {"objectID": "315171b5ba74-3", "text": "Credentials Pane\nThis pane contains fields for entering credentials to connect to your database. The following table describes what is needed for each field: \u00a0\nFieldDescriptionHostEnter the host name for the Databricks database.\nExample: db.company.comPortEnter the port number for the Databricks database.Database NameEnter the name of the Databricks database.UsernameEnter your Databricks username.PasswordEnter your Databricks password.HTTP PathEnter the HTTP path.\nOnce you have entered valid Databricks credentials, you can use the same account any time you go to create a new Databricks DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nIn this pane you create an SQL query to pull data from your database, with or without a parameter.\nMenuDescriptionHow would you like to import data into Domo?", "source": "../../raw_kb/article/databricks_connector/index.html", "title": "Databricks Connector"}, {"objectID": "315171b5ba74-4", "text": "MenuDescriptionHow would you like to import data into Domo?\n\u00a0Select whether you want to import your data by using the standard\u00a0update method (replace/append), by using the partition, or upsert mode.Query TypeSelect the desired query type.Query TypeDescriptionCustom QueryEnter the query to execute.Query BuilderSelect a table and fields to autogenerate your query.Standard updateReplaceReplaces the current dataset with a new datasetAppendAdd the current dataset with a new datasetUse PartitionPartition Criteria Selection\u00a0Select whether you want to partition your data using the date keys or meta queryPartition Column SelectionSelect the partition column name. Only date fields will be shown in this discoveryPartition Support FormatSelect the partition formatPast DaysEnter the number of past days that you want to get data for. Value can be X, where X is a positive integer. For example 30.\u00a0Past Days MonthsEnter the number of past months that you want to get data for. Value can be X, where X is a positive integer. For example 30.Past Days YearsEnter the number of past years that you want to get data for. Value can be X, where X is a positive integer. For example 30.Future MonthsEnter the number of future months that you want to get data for. Value can be X, where X is a positive integer. For example 30.\"Future YearsEnter the number of future years that you want to get data for. Value can be X, where X is a positive integer. For example 30.Partition QueryEnter the partition meta query to determine the distinct partition tags(or keys). The column containing the <b>Date</b> data is your partition column. Example: SELECT DISTINCT(COLUMN_NAME) FROM TABLENAMEUpsertUpsert Keys text fieldPlease enter upsert key column name or a comma separated list of upsert key column namesQueryEnter the Structured Query Language (SQL) query to use in selecting the data you want.", "source": "../../raw_kb/article/databricks_connector/index.html", "title": "Databricks Connector"}, {"objectID": "315171b5ba74-5", "text": "Example:\u00a0select * from Employee\nYou can use the\u00a0Query Helper\u00a0parameter to help you write a usable SQL query. To use the\u00a0Query Helper, do the following:Select your database table\u00a0and table columns in the appropriate menus.Copy the SQL statement that appears in the\u00a0Query Helper\u00a0field.Paste the copied SQL statement into the\u00a0Query\u00a0field.Database Tables\u00a0Select the database table you want to import into Domo.\u00a0Table Columns\u00a0Select the table columns you want to import into Domo.Query Helper\u00a0Copy and paste the SQL statement in this field into the\u00a0Query\u00a0field. For more information, see\u00a0Query, above.Fetch SizeEnter the fetch size for memory performance. The default value will be used if no fetch size specified. If an \"out of memory\" error occurs, retry decreasing the fetch size.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this connector?\nYou need the username, password, host name, port number, and database name of your Databricks database. You also need to provide the HTTP path.\nWhere can I find the values that I need to enter for my credentials?\nYou can find the hostname, database, port number, and HTTP path by going to your cluster in Databricks and viewing the JDBC/ODBC tab in the Advanced section of the cluster details.\nHow frequently will my data update?\nAs often as needed.\nAre there any API limits that I need to be aware of?\nLimits depend on your server configuration.\nWhat do I need to be aware of while writing a query?\nMake sure that all the words, table names and field names are correctly spelled. Refer to the Query Helper field for query help.\nWhat's the Fetch Size?", "source": "../../raw_kb/article/databricks_connector/index.html", "title": "Databricks Connector"}, {"objectID": "315171b5ba74-6", "text": "What's the Fetch Size?\nThe fetch size is for memory performance. The default value will be used if no fetch size is specified. If an \"out of memory\" error occurs, retry decreasing the fetch size.\nSelecting the Update Mode\nOnce you decide how you want to import your data into Domo(by using the standard\u00a0update method (replace/append), by using partition, or upsert mode), you need to select the relevant update mode in the Scheduling section in\u00a0the Connector.\nIf you select 'Partition' in the Details section, then you need to select the 'Append' update mode in the Scheduling section.", "source": "../../raw_kb/article/databricks_connector/index.html", "title": "Databricks Connector"}, {"objectID": "315171b5ba74-7", "text": "If you select 'Upsert' in the Details section, then you need to select 'Merge' update mode in the Scheduling section.", "source": "../../raw_kb/article/databricks_connector/index.html", "title": "Databricks Connector"}, {"objectID": "943c9d645524-0", "text": "TitleDatabricks using Personal Access Token AuthenticationArticle BodyIntro\n\u00a0\n\n\n\nNote:\u00a0The Databricks using Personal Access Token Authentication are currently still in development and are not yet available for use.", "source": "../../raw_kb/article/databricks_using_personal_access_token_authentication/index.html", "title": "Databricks using Personal Access Token Authentication"}, {"objectID": "943c9d645524-1", "text": "Databricks is a cloud-based collaborative data science, data engineering, and data analytics platform that combines the best of data warehouses and data lakes into a lakehouse architecture.\nWith Databricks you can access all your data, analytics, and AI on one lake house platform. The simple, open, and collaborative environment helps reduce the infrastructure complexity, keeps control of your data, and makes it easy for your teams to partner across the entire data and workflow. For more information about the Databricks API, visit their website. (https://docs.databricks.com/dev-tools/api/index.html)\nThe Databricks connector is a \"Database\" connector, meaning it retrieves data from a database using a query. In the Data Center, you can access the connector page for this and other Database connectors by clicking\u00a0Database\u00a0in the toolbar at the top of the window.\nYou connect to your Databricks database in the Data Center. This topic discusses the fields and menus that are specific to the Databricks connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to a Databricks database and create a DataSet, you must have the following:\nThe username and password you use to log into your Databricks hostThe hostname for the databaseThe port number for the databaseThe database name or schema nameThe HTTP PathAccess Token\nConnecting to Your Databricks Database\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes on the Databricks Connector page. The components of the other panes on this page, Scheduling,\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/databricks_using_personal_access_token_authentication/index.html", "title": "Databricks using Personal Access Token Authentication"}, {"objectID": "943c9d645524-2", "text": "Note: When using the copy/paste function for your credentials, ensure there is no whitespace at the beginning or end of the string. By accidentally pasting whitespace, it will cause the connector to an error when trying to connect.", "source": "../../raw_kb/article/databricks_using_personal_access_token_authentication/index.html", "title": "Databricks using Personal Access Token Authentication"}, {"objectID": "943c9d645524-3", "text": "Credentials Pane\nThis pane contains fields for entering credentials to connect to your database. The following table describes what is needed for each field: \u00a0\nFieldDescriptionHostEnter the hostname for the Databricks database.\nExample:\u00a0db.company.comPortEnter the port number for the Databricks database.Database NameEnter the name of the Databricks database.UsernameEnter your Databricks username.PasswordEnter your Databricks password.HTTP PathEnter the HTTP path.\nOnce you have entered valid Databricks credentials, you can use the same account any time you go to create a new Databricks DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nIn this pane you create an SQL query to pull data from your database, with or without a parameter.\nMenuDescriptionQuery TypeSelect the desired query type.Query TypeDescriptionCustom QueryEnter the query to execute.Query BuilderSelect a table and fields to autogenerate your query.QueryEnter the Structured Query Language (SQL) query to use in selecting the data you want.\nExample:\u00a0select * from Employee", "source": "../../raw_kb/article/databricks_using_personal_access_token_authentication/index.html", "title": "Databricks using Personal Access Token Authentication"}, {"objectID": "943c9d645524-4", "text": "Example:\u00a0select * from Employee\nYou can use the\u00a0Query Helper\u00a0parameter to help you write a usable SQL query. To use the\u00a0Query Helper, do the following:Select your database table\u00a0and table columns in the appropriate menus.Copy the SQL statement that appears in the\u00a0Query Helper\u00a0field.Paste the copied SQL statement into the\u00a0Query\u00a0field.Database Tables\u00a0Select the database table you want to import into Domo.\u00a0Table Columns\u00a0Select the table columns you want to import into Domo.Query Helper\u00a0Copy and paste the SQL statement in this field into the\u00a0Query\u00a0field. For more information, see\u00a0Query, above.Fetch SizeEnter the fetch size for memory performance. The default value will be used if no fetch size is specified. If an\u00a0\"out of memory\"\u00a0error occurs, retry decreasing the fetch size.Partition Support FormatSelect the Year, Day and Month format of the data you would like to retrieve.Fetch SizeEnter the fetch size for memory performance. The default value will be used if no fetch size specified. If an \"out of memory\" error occurs, retry decreasing the fetch size.How you would like to import data into Domo?You can import the data from the following: Standard Update (replace/append)User partitionUse upsert\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this connector?\nYou need the username, password, host name, port number, and database name of your Databricks database. You also need to provide the HTTP path and Access token.\nWhere can I find the values that I need to enter for my credentials?", "source": "../../raw_kb/article/databricks_using_personal_access_token_authentication/index.html", "title": "Databricks using Personal Access Token Authentication"}, {"objectID": "943c9d645524-5", "text": "Where can I find the values that I need to enter for my credentials?\nYou can find the hostname, database, port number, and HTTP path by going to your cluster in Databricks and viewing the JDBC/ODBC tab in the Advanced section of the cluster details.\nHow frequently will my data update?\nAs often as needed.\nAre there any API limits that I need to be aware of?\nLimits depend on your server configuration.\nWhat do I need to be aware of while writing a query?\nMake sure that all the words, table names, and field names are correctly spelled. Refer to the Query Helper field for query help.\nWhat's the Fetch Size?\nThe fetch size is for memory performance. The default value will be used if no fetch size is specified. If an \"out of memory\" error occurs, retry decreasing the fetch size.", "source": "../../raw_kb/article/databricks_using_personal_access_token_authentication/index.html", "title": "Databricks using Personal Access Token Authentication"}, {"objectID": "236c79809d70-0", "text": "TitleDatadog ConnectorArticle BodyIntro\nDatadog is a monitoring service for cloud-scale applications, bringing together data from servers, databases, tools, and services to present a unified view of an entire stack.\u00a0 To learn more about the Datadog API, visit their page (https://docs.datadoghq.com/api/?lang=python#overview).\nYou connect to your Datadog account in the Data Center. This topic discusses the fields and menus that are specific to the Datadog connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Datadog account and create a DataSet, you must have the following:\nA Datadog API keyA Datadog application key\nTo obtain and manage credentials, navigate here and log in using your Datadog\u00a0username/email and password:\u00a0https://app.datadoghq.com/account/lo...2Fsettings#api\nConnecting to Your Datadog Account\nThis section enumerates the options in the Credentials and Details panes in the Datadog Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Datadog account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAPI KeyEnter your Datadog API key.Application KeyEnter your Datadog application key.\nOnce you have entered valid Datadog credentials, you can use the same account any time you go to create a new Datadog DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.", "source": "../../raw_kb/article/datadog_connector/index.html", "title": "Datadog Connector"}, {"objectID": "236c79809d70-1", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Datadog report you want to run.\u00a0The following reports are available:Emeddable GraphsReturns a list of previously created embeddable graphs.MetricsReturns a list of actively reporting metrics from a given time until now.MonitorsReturns a list of all monitors.Monitor DowntimeReturns details about all monitor downtimes.Query Time SeriesReturns metrics from any time period.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/datadog_connector/index.html", "title": "Datadog Connector"}, {"objectID": "236c79809d70-2", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nHow frequently will my data update?\nAs often as needed.\nAre there any API limits I should be aware of?\nWhile Datadog may enforce call rate limits, you should not need to worry about it.", "source": "../../raw_kb/article/datadog_connector/index.html", "title": "Datadog Connector"}, {"objectID": "e53f9236314c-0", "text": "Title\n\nDataFlow and DataFusion Troubleshooting and FAQs\n\nArticle Body\n\nDataFlows\nRow size is wider than MySQL supports\nThis most commonly happens when you have one or more text columns that have too many characters in them. For example, description columns can have hundreds of words, which take up most of the space on your MYSQL table causing this error.\nTo solve this issue, remove columns that you do not need, especially large string columns, or remove column widths.\nFor more detailed information on limits and how data size is calculated, see the MySQL documentation.\nCould not convert/parse X data type to Y data type\nThis happens when characters exist in a value that cannot be converted to a data type. For example, you would receive this error if you were trying to convert the string value 'z84084' to a number. Every character in the value must be able to be converted to the new datatype.\u00a0\nThis error typically happens when an input DataSet is running using the Append method. It occurs\u00a0because Domo doesn't do data validation when appending data.\u00a0So\u00a0if the data provider adds extra columns to the same report or sends data with a data type that does not match the original schema, this DataSet will likely break when put into a DataFlow. To fix this, clean up your data before attempting to convert it from one data type to another,\u00a0or change the run method to Replace.\n10,000 duplicate row error in Magic ETL\nMagic ETL does not allow for a join to occur when the column on the left\u00a0contains over 10,000 of the same values. For example, if the column on the left of your join is order number and the order number '12345' existed on more than 10,000 different rows, then an error will occur because these are considered duplicate values.", "source": "../../raw_kb/article/dataflow_and_datafusion_troubleshooting_and_faqs/index.html", "title": "DataFlow and DataFusion Troubleshooting and FAQs"}, {"objectID": "e53f9236314c-1", "text": "To solve this issue, just change the positions of the tables in the join. Instead of 'Sample Data' on the left side, move it to the right. Keep in mind you'll want to make sure you change your join accordingly. In the example above, the join should change from a left to a right join to ensure 'HH - Pages' is still being joined onto 'Sample Data'.\n\nWhy is there a variance in my DataFlow run times?\nThe way Domo runs DataFlows is very complex but efficient. Due to the complexity of Domo's infrastructure, DataFlows have variance in run times; this is expected and normal. As a principle, the larger the input of the DataFlow, the larger the variance can be.\nThe fastest run time of a DataFlow should not be considered the expected run time of a DataFlow. Even the average run-time for that version cannot always be expected every time the DataFlow runs. It is an average after all.\nDomo is constantly making changes to this part of the infrastructure to improve consistency. To decrease overall run-time look to how you can optimize the DataFlow query by following the principles described in Optimizing an SQL DataFlow. Dive even deeper into DataFlow optimization with the following document:\nhttps://dev.mysql.com/doc/refman/5.5/en/select-optimization.html\nWhy is my DataFlow in a draft state?\nWhen DataFlows are in a draft state, this typically means that it has been caught in a trigger loop. This means that the inputs that you use to trigger the current DataFlow is\u00a0causing a never-ending loop of input triggers upstream from the DataFlow.\u00a0\nFor example, DataSet A triggers DataFlow 1. DataFlow 1 creates DataSet B. DataSet B triggers DataFlow 2. DataFlow 2 outputs DataSet A. This results in the following endlessly triggered DataFlow.", "source": "../../raw_kb/article/dataflow_and_datafusion_troubleshooting_and_faqs/index.html", "title": "DataFlow and DataFusion Troubleshooting and FAQs"}, {"objectID": "e53f9236314c-2", "text": "To solve this issue, you will need to break the loop. Remove the input(s) from triggering the DataFlow that is causing the loop and save\u00a0the settings. If the draft flag is gone, you have broken the trigger loop.\nEdit the inputs that trigger a DataFlow by,\nFinding and selecting the DataFlow in the Data Center.\u00a0Select\u00a0Settings.Check/uncheck the box next to any input that you want or don't want to trigger this DataFlow to run.Click\u00a0Apply.\u00a0\nWhy did all of my individual transforms run successfully in the\u00a0preview, but the DataFlow failed its overall run?\nWhen running and previewing the data in a transform sometimes not all rows from the input DataSet are being processed. You may notice that by default, an input only has its first 10,000 rows loaded. If a DataFlow ran into an error but each individual transform ran successfully in the preview window, the value that caused the DataFlow to throw the error probably exists outside of the first 10,000 rows that were processed during the preview.", "source": "../../raw_kb/article/dataflow_and_datafusion_troubleshooting_and_faqs/index.html", "title": "DataFlow and DataFusion Troubleshooting and FAQs"}, {"objectID": "e53f9236314c-3", "text": "Increase the number of rows being processed in the preview by selecting more than 10k rows in the DataFlow edit view.\u00a0\nWhy is the Redshift preview data not in the order I specified in my ORDER BY clause?\nWhen Redshift processes a query that contains an ORDER BY clause, it does honor the specified order while processing the logic of that query. However, when it has finished running the query, it reorders the data to optimize the actual storage of the data. (Redshift stores data tables distributed across many nodes, and splits the data up according to its own storage optimization methods.)\u00a0When Domo pulls up a preview of the data, it is loaded out of the table after being reordered and stored by Redshift. When working with Redshift, it's important to include an ORDER BY clause in every individual query that relies on that specific ordering of the data,\u00a0not\u00a0to rely on the order persisting from a prior transform.\nBlend (DataFusion)\nCannot load required dependency\nThis most likely means that the schema of one of the inputs was changed or deleted, causing an error in the DataFusion. Datafusions are not designed to compensate automatically for schema changes which is part of what allows them to join large DataSets so quickly.\u00a0\nTo solve this issue, simply edit the DataFusion and change the join to a different join type and save. For example, if you had it set to a left join, change it to an inner join and save. Then, edit the DataFusion again, changing it back to the original left join. This resets the schema, fixing the issue.\u00a0\nSlow DataFusion run-time\nSlow DataFusions are typically caused by 3 things.\nJoins on non-numeric columnsJoining on too many columnsNested DataFusions. i.e. Using a DataFusion as an input to another DataFusion.", "source": "../../raw_kb/article/dataflow_and_datafusion_troubleshooting_and_faqs/index.html", "title": "DataFlow and DataFusion Troubleshooting and FAQs"}, {"objectID": "e53f9236314c-4", "text": "To solve this issue, we recommend changing the join to include only one numeric column of type Long for each join. If the DataFusion is nested 3 deep or more, move those inputs into the same DataFusion, if possible.", "source": "../../raw_kb/article/dataflow_and_datafusion_troubleshooting_and_faqs/index.html", "title": "DataFlow and DataFusion Troubleshooting and FAQs"}, {"objectID": "ea5e1504709a-0", "text": "TitleDataFusion Performance RecommendationsArticle BodyIntro\nIf you are experiencing performance issues with a Fusion DataSet, there may be a number of contributing factors. Please consider the following recommendations to improve your query performance.\nSize of Inputs\u00a0\nGenerally speaking, the larger your input DataSets are, the less performant your Fusion will be. If your largest input DataSet is over 5 billion rows, there is a good chance that many of the queries will time out and not return results. Domo Support does not currently have an effective method of improving Fusions this large. However, Domo Engineering is currently developing solutions that will facilitate Fusions built on billion+ row DataSets.\nRecommendations\nReduce the size of the largest input DataSet.Do not build Fusions on DataSets larger than 5 billion rows.\nNumber of Inputs\u00a0\nThe more input DataSets there are, the less performant your Fusion will be. If the Card you are building doesn\u2019t require any fields from one or more of the input DataSets, consider creating a new Fusion that does not include those input DataSets. This will make the query that populates the Card run faster.\nRecommendations\nCreate the smallest possible Fusion for the Card being built. This may result in multiple similar Fusions, but it will improve performance.\nJoin Columns\nDataSets joined by string fields are the least performant. Joining by doubles is better, and joining by integers is best.\nRecommendations\n\u00a0\nOnly join on string columns as a last resort. Always join on numeric fields when you can.If you have a string field that needs to be used for the Fusion, see if there is a corresponding numeric identifier that you can add to the tables and join on those instead.\nBeast Mode", "source": "../../raw_kb/article/datafusion_performance_recommendations/index.html", "title": "DataFusion Performance Recommendations"}, {"objectID": "ea5e1504709a-1", "text": "Beast Mode\nComplex Beast\u00a0Mode calculations are calculated against every row of the Fusion DataSet, and as such can contribute to poor query performance. Beast Mode calculations\u00a0calculated against string fields are the most query-intensive. Complex Beast Mode calculations, such as large CASE statements, can significantly impact your performance.\nRecommendations\nBuild your Beast Mode calculations\u00a0on numeric or date fields instead of string fields.Consider calculating the Beast Mode field before importing your data. This will prevent wasting\u00a0time running\u00a0the calculation at query time.Avoid large case statements calculated on string fields.\nSupport\nIf you have followed the recommendations to improve your Fusion and you are still not experiencing adequate performance, please reach out to Domo Support for assistance.", "source": "../../raw_kb/article/datafusion_performance_recommendations/index.html", "title": "DataFusion Performance Recommendations"}, {"objectID": "ff37368efe30-0", "text": "TitleDataGov ConnectorArticle BodyIntro\nData.gov is the home of the U.S. government's open data. You can\u00a0use\u00a0Domo's Data.gov connector to\u00a0compile reports containing freely available government data.\u00a0To learn more about the\u00a0Data.gov API, visit their\u00a0page (https://www.data.gov/developers/apis/).\nThe Data.gov connector is a \"Cloud App\" connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the connector page for this and other Cloud App connectors by clicking Cloud App in the toolbar at the top of the window.\nYou connect to your\u00a0Data.gov account in the Data Center. This topic discusses the fields and menus that are specific to the\u00a0Data.gov connector user interface. General information for adding data sources, setting update schedules, and editing data source information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\nBecause Data.gov data is freely available, you do not need to create an account in Domo to access it. Hence, you do not need to\u00a0enter credentials in the Data.gov connector page. However, there are many report types\u00a0that require you to enter a\u00a0Data.gov API key. You can\u00a0obtain a free API key by\u00a0going to\u00a0https://api.data.gov/signup/\u00a0and entering your first name, last name, and email address. The API key then appears on the screen. You can copy and paste this key as necessary in the Domo Data.gov connector interface.\nCreating a Data.gov report\nThis section enumerates the options in the\u00a0Details\u00a0pane in the\u00a0Data.gov Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your Data Source, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-1", "text": "The\u00a0Details\u00a0pane contains\u00a0a primary menu,\u00a0Report, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-2", "text": "MenuDescriptionReportSelect a\u00a0Data.gov report. The following reports are available:Agency Priority GoalReturns a list of agency priority goals.Amber WavesReturns a list of articles related to farming and food production.AgencyReturns detailed information about government agencies.AgenciesReturns a list of names of government agencies.ARMSReturns valid survey codes for ARMS (Agricultural Resource Management Survey).App & TechnologiesReturns a list of accessible apps and assistive technologies.AuctionsReturns data about GSA auctions.AuthorsReturns information about authors of government publications.Average Wait Time Until Hearing HeldReturns the average time (in months) from the hearing request date until a hearing is held for claims pending in the Office of Disability Adjudication and Review's hearing offices. Allows users to estimate the amount of time they may have to wait for a hearing to be held.Broadband ProvidersReturns information about broadband providers.Building SystemsReturns information about building systems such as lighting, HVAC, etc.Business ArticlesReturns information about articles available for users of BusinessUSA.Business DataReturns information about different data assets users can access on BusinessUSA.Business EventsReturns information about events", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-3", "text": "users can access on BusinessUSA.Business EventsReturns information about events users can view on BusinessUSA.Business License PermitsReturns permits, licenses, and registrations needed to run a business.Business ProgramReturns information about programs users can access through BusinessUSA.Business ServicesReturns information about services users can access through BusinessUSA.Business Success StoriesReturns information about success stories featured on BusinessUSA.Business ToolsReturns information about tools featured on BusinessUSA.ChartsReturns information about government charts, including chart names and URLs.Content PagesReturns information about content pages on Data.gov.Convenience Contact  Returns\u00a0convenience contact information\u00a0for service providers, equipment manufacturers, schools and universities, and national and international organizations.ContentReturns content for fact sheets and User Voice questions.Disability TypesReturns a list of disabilities, including the disability type name, ID, and description.Drug Adverse EventsReturns information about undesirable effects of drugs reported to the FDA, including serious side effects, product use errors, product quality problems, and therapeutic failures.Drug LabelingReturns information about drug product labeling, such as\u00a0ID, indications, recommended dosage, etc.\u00a0Drug EnforcementReturns information about drug product recalls.Device Adverse", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-4", "text": "EnforcementReturns information about drug product recalls.Device Adverse EventsReturns information about undesirable effects of medical devices reported to the FDA.Device EnforcementReturns\u00a0information about medical device recalls.Data ProductsReturns information about data products on Data.gov.EventsReturns a list of past and upcoming disability-related events.Foods EnforcementReturns information about food recalls, including reason, distribution pattern, product quantity, etc.Food DollarRetrieves a collection of valid food dollar data table names and numbers.Federal ContactsReturns a list of federal agencies that provide accessibility-related services or information.Hearing Office Workload DataReturns information about pending, receipts, dispositions, and average processing time for each hearing office in the\u00a0Office of Disability Adjudication and Review (ODAR).\u00a0Hearing Office Average Processing Time Ranking ReportReturns ranking\u00a0information for the Office of Disability Adjudication and Review (ODAR) hearing offices by the\u00a0average number of days until the final disposition of the hearing request.IndicatorsReturns performance indicators for government agencies.Loans Grants SearchReturns information about financial assistance programs for small businesses.MaterialsReturns\u00a0a list of names and descriptions", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-5", "text": "small businesses.MaterialsReturns\u00a0a list of names and descriptions of building materials.OpportunitiesReturns information about grant opportunities for a given\u00a0search query.PCC\u00a0AwardReturns information about PCC\u00a0(Publicity Club of Chicago) Awards for a given state, zip code, or\u00a0coordinates.Products Mobile DevicesReturns information about mobile devices, such as brand, maker, model number, etc.Products Manufacturer Mobile DevicesReturns a list of manufacturers of mobile devices.Products Details Mobile DevicesReturns a list of accessibility features (with name and description) that are supported by a particular mobile device.Products Features Mobile DevicesReturns a list of accessibility features (with name and description) that may be supported by various mobile devices.Products Regions Mobile DevicesReturns a list of regions where mobile devices may be available.Products Search Mobile DevicesReturns a list of mobile devices based on a given search query (manufacturer, brand, or model).PublicationsReturns information about the last 100 publications posted on Data.gov, ordered by descending release date.PetitionsReturns a list of petitions, with title, body, signature threshold, number of signatures needed, etc.ProductsReturns a list of products, with name, purchase", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-6", "text": "etc.ProductsReturns a list of products, with name, purchase URL, etc.Real Property - Total\u00a0Domestic Office and Warehouse Square FeetReturns total domestic office and warehouse square footage for government agencies.Real Property - Freeze the Footprint ProjectsReturns \"Freeze the Footprint\" data for government agencies.RecallsReturns\u00a0information about product recalls.Strategic GoalReturns strategic goals for government agencies.Strategic ObjectiveReturns strategic objectives for government agencies.SAM\u00a0RegistrationsReturns\u00a0public data about SAM registrations.\u00a0ServicesReturns a list of services, with name, description, required products, etc.StatesReturns a list of states by which convenience contacts can be grouped.TopicsReturns a list of topics on Data.gov.TagsReturns a list of search tags on Data.gov.USA JobsReturns\u00a0information about employment opportunities found on commercial job boards, mobile applications, and social media sites.Video Programming DistributorReturns contact information\u00a0for various video programming distributors.WorkspacesReturns a list of workspaces, including name and overview information.White House Policy SnapshotsReturns information about work the president has done since taking office and his/her plans for continuing progress.\u00a0Agency AcronymEnter the acronym for", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-7", "text": "for continuing progress.\u00a0Agency AcronymEnter the acronym for the agency for which you want to retrieve data. Example: DHSAgency IDEnter the ID for the agency for which you want to retrieve data. Example: 550API KeyEnter\u00a0your\u00a0Data.gov API key. For information about obtaining a Data.gov API key, see \"Prerequisites\" above.BrandEnter the brand name for which you want to retrieve data. Example: SonyBusiness\u00a0TypeSelect the business type for which you want to retrieve data.CategorySelect\u00a0the category for which you want to retrieve data.Content TypeSelect the content type for which you want to retrieve data.CountryEnter a country to retrieve information specific to that country. Example: GermanyData SourceSelect the data source for which you want to retrieve data.DateSelect the date for your report. The report will return records for this date only.Disability IDEnter the disability ID for which you want to retrieve data. Example:\u00a03End DateSelect the end date for your report. The report will return records created on or before this date. Pair with Start Date to include a range of dates in your report.Entity", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-8", "text": "Date to include a range of dates in your report.Entity TypeSelect the entity type for which you want to retrieve data.Group by StateSpecify whether the list items should be broken down by state.IDEnter the ID for the\u00a0entity for which you want to retrieve data. Example:\u00a08Job TitleEnter a job title to return job announcements with the specified title. Example: Technical WriterLatitudeEnter the latitude of the location for which you want to retrieve data. Example:\u00a034.876011LongitudeEnter the longitude of the location for which you want to retrieve data. Example:\u00a0-83.400856Max SalaryEnter a maximum salary to return all jobs that have that salary or lower. Example: 72,000. Pair with Min Salary to return data for the given range.Min SalaryEnter a minimum salary to return all jobs that have that salary or higher. Example: 15,500. Pair with Max Salary to return data for the given range.NameEnter the name of the entity for which you want to retrieve data. Example:\u00a0HVACOrganizationSelect the organization for which you want to retrieve data.Petitions Created AfterSelect a date for your", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-9", "text": "retrieve data.Petitions Created AfterSelect a date for your report. The report will return petitions created on or after this date. If you select a date here, you should not select a date for Petitions Created Before.Petitions Created BeforeSelect a date for your report. The report will return petitions created on or before this date. If you select a date here, you should not select a date for Petitions Created After.Product IDEnter the ID of the product for which you want to retrieve data. Example: 716QueryEnter\u00a0a\u00a0keyword or phrase to use to filter the data in your report. \u00a0Example: bioinformaticsRadiusEnter the radius length in whole numbers\u00a0from the specified\u00a0latitude-longitude coordinates.\u00a0Example:\u00a02Recall QueryEnter a query term to filter the data in your report. This filter handles word variants. For example, if you entered the term choke, the report would return items with the query term choking hazard.RegionEnter the world region for which you want to retrieve data. You can choose from any of the following:", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "ff37368efe30-10", "text": "AfricaAsia PacificEuropeLatin AmericaMiddle EastNorth AmericaRegistration IDEnter the registration ID for which you want to retrieve data. Example: 1459697830000Request BySpecify\u00a0how the report data is to be broken down.Search TermEnter a search term (manufacturer, brand, or model) to filter the data in your report.Show ClosedSelect whether closed items should be shown in your report.Show IncompleteSelect whether incomplete items should be shown in your report.Start DateSelect the start date for your report. The report will return records created on or after this date. Pair with End Date to include a range of dates in your report.State AbbreviationEnter the two-letter abbreviation of the U.S. state for which you want to retrieve data.TypeSelect a type to retrieve information specific to that type.VPD TypeSelect a VPD type to retrieve information specific to that VPD type.Zip CodeEnter the 5-digit zip code for the location for which you want to retrieve data. Example:\u00a084765\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/datagov_connector/index.html", "title": "DataGov Connector"}, {"objectID": "b5e9a1ef9f9e-0", "text": "TitleDataset Copy Advanced ConnectorArticle BodyIntro\u00a0\nYou can use Domo's DataSet Copy Advanced connector to copy data from one Domo instance to another. You do this in the Data Center.\nThe DataSet Copy DataSet connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking File in the toolbar at the top of the window.\nThere are two versions of this connector, a regular and an advanced version. The only difference between these is that the advanced version asks you for the username and password for the Domo instance you are copying from. Because of this, we do not provide separate documentation for the advanced version.\nThis topic discusses the fields and menus that are specific to the DataSet Copy\u00a0connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\u00a0\nFor creating the credentials you can refer following steps:\nTo obtain your Domo Client ID and Client Secret:\nLog into the Domo developer account.In the top right corner under My Account click New Client.Enter the application name and description.Provide the application scope by selecting the checkboxes for Data and User.Click Create. Once you have created a client, you can manage the client by clicking on Manage Client.Your Value will appear in the Manage Client section.\nThe DataSource ID of the data you want to copy. You can find the DataSource ID by opening the Details view for the DataSet in the Data Center. The ID is the number in the URL following \"/datasources/\".", "source": "../../raw_kb/article/dataset_copy_advanced_connector/index.html", "title": "Dataset Copy Advanced Connector"}, {"objectID": "b5e9a1ef9f9e-1", "text": "Configuring the Connector\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the\u00a0DataSet Copy\u00a0Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\u00a0\nThis pane contains fields for entering credentials to create a DataSet Copy account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionDomo Client IDEnter the Client ID for the Domo instance you are copying from.Domo Client SecretEnter the Client Secret for the Domo instance you are copying from.\nFor more information about obtaining the above credentials, see \"Prerequisites,\" above.\nOnce you have entered valid\u00a0credentials, you can use the same account any time you go to create a new DataSet Copy\u00a0DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nIn this pane you enter details about the DataSet you want to copy.\nFieldDescriptionReportSelect the report you want to run.DataSource\u00a0IDThe DataSource ID of the data you want to copy. You can find the DataSource ID by opening the Details view for the DataSet in the Data Center.\u00a0Only Updated DataChoose 'Yes' to retrieve data only if the dataset has run after the latest run of this copy dataset. This helps prevent stale data.DataSet's SchemaChoose 'Yes' to use the schema from the inputted dataset for every run.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this connector?", "source": "../../raw_kb/article/dataset_copy_advanced_connector/index.html", "title": "Dataset Copy Advanced Connector"}, {"objectID": "b5e9a1ef9f9e-2", "text": "FAQs\nWhat kind of credentials do I need to power up this connector?\nYou need the username, password, and access token of an account that has access to the original dataset.\nI am not able to connect with my access token. Why?\nAccess tokens are associated with specific user accounts and have the same access as the user. If the user's permissions change, the access token reflects that.\nWhy do I need the access token?\nAccess tokens are used to import data in or export data from Domo.\nHow can I get the access token?\nYou need to have an \"Admin\" security role in generating an access token; otherwise, you need to request an access token from your Domo Admin. Visit Managing Access Tokens for more details.\nWhat else do I need to do to power up my connector?\nYou need to provide the Domo instance containing the dataset and the dataset id of the data you want to copy.\nHow can I find the Domo instance and dataset id?\nIf your Domo URL is \"https://abc123.domo.com\"\", the instance would be \"abc123\".\nYou can find the dataset id in the Details view for the dataset in the Data Center. The id is the number in the URL following \"/datasources/\".\nHow often can the data be updated?\nDatasets should be set to update once every 15 minutes.\nAre there any API limits that I need to be aware of?", "source": "../../raw_kb/article/dataset_copy_advanced_connector/index.html", "title": "Dataset Copy Advanced Connector"}, {"objectID": "1313ed6fa219-0", "text": "Title\n\nDataSet Copy Connector\n\nArticle Body", "source": "../../raw_kb/article/dataset_copy_connector/index.html", "title": "DataSet Copy Connector"}, {"objectID": "1313ed6fa219-1", "text": "Intro\u00a0\nYou can use Domo's DataSet Copy connector to copy data from one Domo instance to another.\u00a0You do this\u00a0in the Data Center.\nThe DataSet Copy DataSet connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking File in the toolbar at the top of the window.\nThere are two versions of this connector, a regular and an advanced version. The only difference between these is that the advanced version asks you for the username and password for the Domo instance you are copying from. Because of this, we do not provide separate documentation for the advanced version.\nThis topic discusses the fields and menus that are specific to the DataSet Copy\u00a0connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\u00a0\nTo copy data from one instance to another, you must have the following:\nThe access\u00a0token for the DataSet you are copying data from. This token belongs to the owner of the DataSet you are copying data from. If you have an \"Admin\" security role, you can generate access tokens; otherwise, you need to request an access token from an administrator.\u00a0Make sure the token is not expired. For information about generating access tokens, see\u00a0Managing Access Tokens.The Domo instance containing the DataSet you want to copy. For example, if your Domo URL was https://abc123.domo.com, the instance would be\u00a0abc123.The DataSource\u00a0ID of the data you want to copy. You can find the DataSource ID by opening the Details view for the DataSet in the Data Center. The ID is the number in the URL following\u00a0\"/datasources/\".\u00a0\u00a0  \u00a0Credentials (username and password) for the Domo instance you are copying from (Advanced version only).", "source": "../../raw_kb/article/dataset_copy_connector/index.html", "title": "DataSet Copy Connector"}, {"objectID": "1313ed6fa219-2", "text": "Configuring the Connector\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the\u00a0DataSet Copy\u00a0Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\u00a0\nThis pane contains fields for entering credentials to create a DataSet Copy account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsername (Advanced version only)Enter the username for the Domo instance you are copying from.Password (Advanced version only)Enter the password for the Domo instance you are copying from.Access\u00a0TokenEnter the developer token for the DataSet you are copying data from.\nFor more information about obtaining the above credentials, see \"Prerequisites,\" above.\nOnce you have entered valid\u00a0credentials, you can use the same account any time you go to create a new DataSet Copy\u00a0DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nIn this pane you enter details about the DataSet you want to copy.\nFieldDescriptionDataSource\u00a0IDEnter the Domo DataSource ID of the DataSet to pull data from. The DataSource ID is the GUID in the URL of the DataSet, immediately after /datasources/.Domo InstanceEnter the Domo instance name.", "source": "../../raw_kb/article/dataset_copy_connector/index.html", "title": "DataSet Copy Connector"}, {"objectID": "1313ed6fa219-3", "text": "For example, in https://abc123.domo.com, abc123 is the instance.Exclude Meta ColumnsSelect this checkbox if you do not want Domo to add meta columns to the copied dataset (Batch ID, Batch Last Run, and Domo Instance).Fetch Incremental DataSelect this checkbox to fetch incremental data for the input dataset. The connector will fetch the incremental data from the last successful runs only when the Update mode for your input dataset as well as the DataSet Copy dataset is set to Append.Only Updated DataChoose 'Yes' to retrieve data only if the dataset has ran after the latest run of this copy dataset. This helps prevent stale data.Preserve Empty StringSelect this checkbox if you do not want the empty strings to be converted into null values.Use Origin DataSet's SchemaChoose 'Yes' to use the schema from the inputted dataset for every run.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this connector?\nYou need the username, password and the access token of an account that has access to the original dataset.\nI am not able to connect with my access token. Why?\nAccess tokens are associated with the specific user accounts and have the same access as the user. If the user's permissions change, the access token reflects that.\nWhy do I need the access token?\nAccess tokens are used to import data in or export data from Domo.\nHow can I get the access token?\nYou need to have an \"Admin\" security role to generate an access token; otherwise, you need to request an access token from your Domo Admin. Visit Managing Access Tokens for more details.\nWhat else do I need to do to power up my connector?", "source": "../../raw_kb/article/dataset_copy_connector/index.html", "title": "DataSet Copy Connector"}, {"objectID": "1313ed6fa219-4", "text": "What else do I need to do to power up my connector?\nYou need to provide the Domo instance containing the dataset and the dataset id of the data you want to copy.\nHow can I find the Domo instance and dataset id?\nIf your Domo URL is \"https://abc123.domo.com\"\", the instance would be \"abc123\".\nYou can find the dataset id in the Details view for the dataset in the Data Center. The id is the number in the URL following \"/datasources/\".\nHow often can the data be updated?\nDatasets should be set to update once every 15 minutes.\nAre there any API limits that I need to be aware of?\nNo", "source": "../../raw_kb/article/dataset_copy_connector/index.html", "title": "DataSet Copy Connector"}, {"objectID": "2eed7e9b1695-0", "text": "TitleDataSet Copy Connector Access Token OnlyArticle BodyIntro\u00a0\nYou can use Domo's DataSet Copy connector to copy data from one Domo instance to another.\u00a0You do this\u00a0in the Data Center.\nThe DataSet Copy DataSet connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking File in the toolbar at the top of the window.\nThere are two versions of this connector, a regular and an advanced version. The only difference between these is that the advanced version asks you for the username and password for the Domo instance you are copying from. Because of this, we do not provide separate documentation for the advanced version.\nThis topic discusses the fields and menus that are specific to the DataSet Copy\u00a0connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\u00a0\nTo copy data from one instance to another, you must have the following:\nThe access\u00a0token for the DataSet you are copying data from. This token belongs to the owner of the DataSet you are copying data from. If you have an \"Admin\" security role, you can generate access tokens; otherwise, you need to request an access token from an administrator.\u00a0Make sure the token is not expired. For information about generating access tokens, see\u00a0Managing Access Tokens.The Domo instance containing the DataSet you want to copy. For example, if your Domo URL was https://abc123.domo.com, the instance would be\u00a0abc123.The DataSource\u00a0ID of the data you want to copy. You can find the DataSource ID by opening the Details view for the DataSet in the Data Center. The ID is the number in the URL following\u00a0\"/datasources/\".\u00a0\u00a0  \u00a0\nConfiguring the Connector", "source": "../../raw_kb/article/dataset_copy_connector_access_token_only/index.html", "title": "DataSet Copy Connector Access Token Only"}, {"objectID": "2eed7e9b1695-1", "text": "Configuring the Connector\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the\u00a0DataSet Copy\u00a0Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\u00a0\nThis pane contains fields for entering credentials to create a DataSet Copy account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAccess\u00a0TokenEnter the developer token for the DataSet you are copying data from.Domo InstanceEnter the Domo instance name. For example, in https://abc123.domo.com, abc123 is the instance.\nFor more information about obtaining the above credentials, see \"Prerequisites,\" above.\nOnce you have entered valid\u00a0credentials, you can use the same account any time you go to create a new DataSet Copy\u00a0DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nIn this pane you enter details about the DataSet you want to copy.\nFieldDescriptionDataSource\u00a0IDEnter the Domo DataSource ID of the DataSet to pull data from. The DataSource ID is the GUID in the URL of the DataSet, immediately after /datasources/.Exclude Meta ColumnsSelect this checkbox if you do not want Domo to add meta columns to the copied dataset (Batch ID, Batch Last Run, and Domo Instance).Only Updated DataChoose 'Yes' to retrieve data only if the dataset has ran after the latest run of this copy dataset. This helps prevent stale data.Preserve Empty StringSelect this checkbox if you do not want the empty strings to be converted into null values.Use Origin DataSet's SchemaChoose 'Yes' to use the schema from the inputted dataset for every run.\nOther Panes", "source": "../../raw_kb/article/dataset_copy_connector_access_token_only/index.html", "title": "DataSet Copy Connector Access Token Only"}, {"objectID": "2eed7e9b1695-2", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this connector?\nYou need the access token and Domo instance of an account that has access to the original dataset. For example, if your Domo URL is \"https://abc123.domo.com\"\", the instance would be \"abc123\".\nWhere can I find my access token?\nIf you have an \"Admin\" security role, you can generate access tokens; otherwise, you need to request an access token from the administrator. Make sure the token is not expired. For information about generating access tokens, see Managing Access Tokens.\nWhy do I need the access token?\nAccess tokens are used to import data in or export data from Domo.\nI am not able to connect with my access token. Why?\nAccess tokens are associated with the specific user accounts and have the same access as the user. If the user's permissions change, the access token reflects that.\nHow often can the data be updated?\nDatasets should be set to update once every 15 minutes.\nAre there any API limits that I need to be aware of?\nNo", "source": "../../raw_kb/article/dataset_copy_connector_access_token_only/index.html", "title": "DataSet Copy Connector Access Token Only"}, {"objectID": "666a6f384648-0", "text": "TitleDataSet Copy Unload ConnectorArticle BodyIntro\nWith the DataSet Copy Unload connector, you can export data from Domo to your DataSet located at a specific instance. This connector allows you to connect with your Domo Developer account credentials (client id and client secret).\nYou export your data in the Data Center. This topic discusses the fields and menus that are specific to the DataSet Copy Unload connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\n\n\n\u00a0\n\nNote: The owner of a writeback dataset must also be an owner or co-owner of the input dataset.", "source": "../../raw_kb/article/dataset_copy_unload_connector/index.html", "title": "DataSet Copy Unload Connector"}, {"objectID": "666a6f384648-1", "text": "Prerequisites\nTo configure this connector, you will need your\u00a0Domo Client ID and Client Secret.\u00a0\nTo create your Domo client ID and client secret:\nLog into the\u00a0Domo developer account.In the top right corner under\u00a0My Account\u00a0click\u00a0New Client.Enter the application name and description.Provide the application scope by selecting the checkboxes for\u00a0Data\u00a0and\u00a0User.Click\u00a0Create.Once you have created a client, you can manage the client by clicking on\u00a0Manage Client.\nYour\u00a0Client Secret\u00a0will appear in the\u00a0Manage Client\u00a0section.\nConfiguring the Connection\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the DataSet Copy Unload\u00a0Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Domo\u00a0developer\u00a0account. The following table describes what is needed for each field:\u00a0 \u00a0\nFieldDescriptionDomo Client IDEnter your Domo client ID.\u00a0Domo Client SecretEnter your Domo client secret.\nFor more details about obtaining these credentials, see \"Prerequisites.\"\nOnce you have entered valid credentials, you can use the same account any time you go to create a new DataSet Copy Unload connection. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a number of fields for specifying your data and indicating where it's going.\nMenuDescriptionInput Domo DataSet IDEnter your Domo dataset ID (GUID) located in the dataset URL.", "source": "../../raw_kb/article/dataset_copy_unload_connector/index.html", "title": "DataSet Copy Unload Connector"}, {"objectID": "666a6f384648-2", "text": "Example: https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settingsInput Source Domo DataSet InstanceEnter your input Domo dataset instance name located in the dataset URL.\nExample: https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settingsUse High Bandwidth UploadSelect this checkbox to use high bandwidth upload transfer.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this Connector?\nYou need your Domo developer account client id and client secret.\nWhere can I find my Domo client id and client secret?\nLog into the\u00a0Domo developer account.In the top right corner under\u00a0My Account\u00a0click\u00a0New Client.Enter the application name and description.Provide the application scope by selecting the checkboxes for\u00a0Data\u00a0and\u00a0User.Click\u00a0Create.Once you have created a client, you can manage the client by clicking on\u00a0Manage Client.\nYour\u00a0Client Secret\u00a0will appear in the\u00a0Manage Client\u00a0section.\nHow do I find the Input Dataset ID?\nYour Domo input dataset id is in the URL of the dataset you are exporting data from.\nExample: https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settings\nHow do I find the Input Source Dataset instance?\nYour Domo input dataset instance name is located in the URL of the dataset you are exporting data from.Example: https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settings", "source": "../../raw_kb/article/dataset_copy_unload_connector/index.html", "title": "DataSet Copy Unload Connector"}, {"objectID": "d4663e1c5c64-0", "text": "Title\n\nDataSet Update Methods\n\nArticle Body", "source": "../../raw_kb/article/dataset_update_methods/index.html", "title": "DataSet Update Methods"}, {"objectID": "d4663e1c5c64-1", "text": "Intro\nWhen configuring a DataSet, you have the option to update your data based on a replace or append method. Some but not all connectors also provide upsert\u00a0or partition functionality, which is discussed further in this article.\u00a0\nBy default, all DataFlows update on a replace method, however, through some technical SQL transformations, you can create a recursive DataFlow that appends on to itself. Using the replace method, every time a DataSet runs, the table is replaced with the new data. However, using the append method, all of the new data pulled in is appended to the bottom of the existing DataSet.\nFor many DataSets where the data is not large and the load is small, replacing data is preferable because there is less propensity for error.\nAppending data is useful when:\nLoad and processing time is longYou want to snapshot dataThe API restricts how much data can be returned in one query\nFor more information on how to configure your DataSet with a replace or append update\u00a0method, see DataSet Scheduling.\nWhen to replace your data\nIf you consider processing time, and you have the time and resources to pull a large amount of data on a regular basis, then a replace data structure will be an easy way to maintain your data and have it updated for the time period you specify. This data is a final state DataSet, and you're not interested in the process it went through to get to the final state. For example, you want to see actual deals made, bookings, and other transactions that are finalized. The data will show you when those transactions occur and you can visualize that transactional data.\nDespite the processing load that it creates to do the transactional database where the data is being replaced, it's often easier to do a full replace, pull your specific time period's history, and replace it so you have the constant historical DataSet.\u00a0\nUse Cases", "source": "../../raw_kb/article/dataset_update_methods/index.html", "title": "DataSet Update Methods"}, {"objectID": "d4663e1c5c64-2", "text": "Use Cases\nYou are pulling two years of historical sales data and replacing your DataSet every month so that you always have the historical two months' data to reference and create visualizations from. With this type of transactional data, which doesn't change state, you could do a full replace every month and have the constant two-year historical DataSet. So, if you were pulling your January 2018 data, at the beginning of February, you would have a replaced DataSet that would show you all the January 2018 data back through January of 2016. The next month you have it scheduled to replace, the DataSet would give you the February 2018 through February 2016 DataSet. You have a rolling two-year time period but aren't keeping that previous January 2016 data. It is replaced by the next months' data. This type of transactional replace DataSet allows you to see how much business you've done in week 1 of the quarter, in week 2 of the quarter, and so on, as you can see it in the full DataSet.\u00a0\nWhen to append your data\nYou primarily use the append data process when you are looking to snapshot your data, you want to see the progression over time, and to be able to show progression in visualizations. Another primary reason to append data is if you want to avoid the processing load of a full replace of data.\nWhat to be aware of and avoid when choosing to append data:\nPossible missed data if an update fails for some reasonPossible duplicated data or data that could be double counted if not properly interpreted.", "source": "../../raw_kb/article/dataset_update_methods/index.html", "title": "DataSet Update Methods"}, {"objectID": "d4663e1c5c64-3", "text": "In addition, if you're appending data based upon a time selector, or whatever time period you've scheduled, you want to make sure that none of your data is lost as you go from week to week or month to month. You want to also make sure you aren't getting any duplicate records. In a scenario where you are pulling weekly data that is set to append\u00a0weekly from Saturday to Sunday, somehow\u00a0with different time zones your end of Friday data bleeds into your beginning of day Saturday data. This could possibly bring\u00a0in duplicate records and then your data wouldn't be correct. There is a potential for duplicate records, and you want to make sure data is clean and accurate.\nTo set up an append DataFlow that would look specifically for those duplicate records and remove them from the final DataSet, see Recursive ETL. If you were doing a weekly replacement of your data, you could then have it set for intentional overlap when pulling the data. Then, by doing a recursive DataFlow, you can pull those duplicates and produce a master DataSet after it's gone through this process.\nYou can use the append data process when looking for progression in your visualizations. You set up your data so that you take the snapshot period of the data and append it to your schedule, whether it's weekly, monthly, or any time frame, and then the append DataSet has the snapshot data for you to build your visualizations tracking the specific progress of specific identifiers over time.\u00a0\nUse Cases", "source": "../../raw_kb/article/dataset_update_methods/index.html", "title": "DataSet Update Methods"}, {"objectID": "d4663e1c5c64-4", "text": "Use Cases\nIf you have data that changes state with an identifier, for example, Opportunity ID, this is when you want to use a snapshot and append your data. An example use case might be that you are looking at SalesForce or opportunity migration data, and you want to know how the opportunity stage progresses through different time periods. You would take a snapshot, for example, of all your SalesForce opportunities on Monday of every week so you can see the migration of the opportunity over time. If you use the snapshot, then you could track opportunity\u00a0X, and see two months ago that it was in stage 1, then it took 2 weeks to get to stage 2, and then 3 weeks to enter stage 4, and so on. Using this method, you can understand and follow how the sales cycle went as that opportunity progressed.If you have marketing customer profile data, and you want to see where customers are in the marketing funnel\u00a0and all relevant stages that the customer progresses through in your visualizations. You would want to use the snapshot data and append process in order to see all the stages of progression the customers go through over time.\u00a0\nUpsert and partition\nSome Connectors allow you to update DataSets with restated data (or \"upsert\") to ensure you have the most up-to-date information. This functionality is currently available for the following Connectors:\nShopifyHubSpot\u00a0UpsertAmazon S3", "source": "../../raw_kb/article/dataset_update_methods/index.html", "title": "DataSet Update Methods"}, {"objectID": "d4663e1c5c64-5", "text": "Note: Regarding the Upsert update method, only the numeric and text columns can be part of the \"Merge Key.\" The \"Merge Key\" can be selected from the \"Merge Key Location\" dropdown, which is populated with only the columns that qualify based on the data type.\n\n\n\nOther Connectors let you select a rolling window of data to keep when updating, making it easier to focus on the relevant data. Connectors with partitioning include the following:\nPostgreSQLMySQL DatabaseMariaDB DatabaseOracle DatabaseAmazon RedshiftAmazon AuroraAmazon AthenaIBM DB2IBM InformixSnowflakeVerticaSybase/SAP ASENetSuite SuiteAnalytics", "source": "../../raw_kb/article/dataset_update_methods/index.html", "title": "DataSet Update Methods"}, {"objectID": "1e3c6668fc4a-0", "text": "TitleDataSet Validation RequirementsArticle BodyThe following PDF points out the features of DataSets you should be familiar with to get them validated. To download this PDF to your computer, click here.", "source": "../../raw_kb/article/dataset_validation_requirements/index.html", "title": "DataSet Validation Requirements"}, {"objectID": "324a221ef58b-0", "text": "Title\n\nDataSet via Email Connector\n\nArticle Body\n\nIntro\n\nThe Dataset via Email connector enables you to get your data into Domo quickly and easily by capturing and processing .xls, .xlsx, and .csv files that have been included as attachments to an email message. Additionally, the DataSet via Email Connector can parse through a Gzip or Zip file to return a .csv or .xlsx file. When you configure this connector, it generates a unique email address that corresponds to your Dataset via Email DataSet in Domo. Once the email address is created, you can send the spreadsheet as an attachment to that email address. The connector will then process the email and update your data accordingly.\u00a0\nPlease note that this connector currently pulls in data from only one attachment. If your email message includes more than one attachment, data is pulled from the first listed attachment.\u00a0\n\nNote that each time you configure this connector, you do so for an attachment with a fixed set of characteristics. For instance, if you create a configuration for CSV files with specific parameters, you cannot then send XLS spreadsheets to the email address that is generated unless you go back into the connector interface and switch the file type to XLS. If you want to change the attachment file type, it is highly recommended that you create a new connection with its own email address.\nYou configure settings for your DataSet via Email connection in the Data Center. This topic discusses the fields and menus that are specific to the DataSet via Email connector user interface. General information for adding DataSets and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\n\n\u00a0\n\nNote:\u00a0\u00a0The maximum size of emails you can pull into Domo is 25 MB.\n\n\n\n\n\n\n\u00a0\n\nVideo - DataSet Via Email Connector", "source": "../../raw_kb/article/dataset_via_email_connector/index.html", "title": "DataSet via Email Connector"}, {"objectID": "324a221ef58b-1", "text": "Video - DataSet Via Email Connector\n\n\n\n\n\u00a0\nPrerequisites\nNone. No credentials are required.\nConfiguring Your DataSet via Email Connection\nThis section enumerates the options in the Details pane in the DataSet via Email connector page. The components\u00a0 of the other pane in this page, Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nDetails Pane\nThis pane contains various options for configuring your DataSet via Email connection.\nMenuDescriptionEmailAutomatically generates the email address you will send your attachments to. Click Copy Email to copy this email address to your clipboard.File LocationSelect a file location option:Email Attachment. Indicates that the DataSet\u00a0file is attached to the email or in the body of the email you are connecting to.Download directly from link in email. Indicates that the email contains a direct download link to the DataSet file.\n\n\n\u00a0\n\nNote: If there are multiple links in the message, the connector will use the first link that appears in the message.\n\u00a0\n\n\nDownload from link to page. Indicates that the email contains a URL leading to a page with a download link to the DataSet file.\u00a0\n\n\n\u00a0\n\nNote: To find the download page, look for the first link in which the URL or link label contains the file type. If a link with the file type cannot be not found, the connector will look for a link that contains \"report\" or \"download\". \nAfter retrieving the download page, the connector will look for the actual data link the same way (matching against the file type). Note that the URLs on the download page cannot be generated by JavaScript.", "source": "../../raw_kb/article/dataset_via_email_connector/index.html", "title": "DataSet via Email Connector"}, {"objectID": "324a221ef58b-2", "text": "File TypeSelect the file type for your attachments:Email body as one row. Use if the data you want to import is found in the body of the email (rather than as an attachment) and should be imported into Domo\u00a0as a single row of data in a DataSet.\u00a0\u00a0\u00a0Email body as HTML. Use if the data you want to import is found in the body of the email (rather than as an attachment) and should be imported into Domo as HTML. You\u00a0must\u00a0set\u00a0File Attachment\u00a0to\u00a0Email Attachment\u00a0when using this option.\u00a0CSV. Use if your DataSet\u00a0is contained within an attached CSV file.XLS. Use if your DataSet is contained within an attached XLS file.XLSX. Use if your DataSet\u00a0is contained with an attached XLSX file.\n\n\u00a0\n\n\nNotes: \u00a0\nThe file size limit of your chosen email provider dictates the file size limit for this connector.The connector is not able to determine your extension file type if the email does not use TLS 1.2 or above.\n\n\n\nFrom Email AddressEnter the email address you will be sending attachments from. Emails sent from other email addresses will be rejected. Enter NONE to accept emails from any email address.\n\n\n\u00a0\n\nNote:\u00a0Make sure that the email in the configurations matches the email you are sending the file to!", "source": "../../raw_kb/article/dataset_via_email_connector/index.html", "title": "DataSet via Email Connector"}, {"objectID": "324a221ef58b-3", "text": "Email Subject ExpressionEnter a regular expression that will be used to validate the subject of the email you send. Emails with a subject that do not fully match the expression will be rejected. Enter NONE to accept any email subject. \t\t\tHere is an example of a full match:\nSubject = 1900 Audit Report for Domo\u00a0\t\t\tRegex = .*Audit_Report.*Email Body ExpressionEnter a regular expression that will be used to validate the body of the email you send. Emails with body content that do not fully match the expression will be rejected. Enter NONE to accept anything included in the body.\nHere is an example of a full match:\nBody = Here is the Audit Report file to upload to Domo.\u00a0\t\t\tRegex = .*Audit\\sReport.*Attachment Name ExpressionEnter a regular expression that will be used to validate the filename of the attachment. Attachments that do not fully match the expression will be rejected. Enter NONE to accept any filename.\nHere is an example of a full match:", "source": "../../raw_kb/article/dataset_via_email_connector/index.html", "title": "DataSet via Email Connector"}, {"objectID": "324a221ef58b-4", "text": "Attachment = 1.audit_report_1.1.1900.1.00AM.xlsx\t\t\tRegex = .*audit_report.*Show Advanced OptionsCheck the box to show advanced configuration options.Header Row (Optional)Enter the header row number. If you do not enter a number, the first row is considered the header row.Skip Footer Rows (Optional)If there are one or more rows at the bottom of the file you do not want to import, enter the number of rows to skip.Starting Data Row (Optional)Enter the starting data row number. If you do not enter a number, the first row after the header row is considered the starting row.DelimiterSelect the delimiter used to parse your CSV file. If you select\u00a0Detect the delimiter, the appropriate delimiter is determined automatically.\u00a0Escape CharacterSelect whether or not to use an escape character.\u00a0Quote CharacterSelect whether to use double quotes or single quotes as a quote character, or no quote character at all.Detect EncodingCheck this box if you want Domo to detect your encoding type automatically; leave it unchecked if you want to enforce UTF-8 encoding.\u00a0Excel Sheet NameEnter the sheet", "source": "../../raw_kb/article/dataset_via_email_connector/index.html", "title": "DataSet via Email Connector"}, {"objectID": "324a221ef58b-5", "text": "enforce UTF-8 encoding.\u00a0Excel Sheet NameEnter the sheet name of the Excel file you want to create a DataSet from.Excel Sheet Tab NumberEnter the tab number of the Excel sheet you want to create a DataSet from.Raw HeadersIf you select Yes, the connector will use the headers in the sent file. If you select No, the connector will modify any special characters found in the header text.Date FormatSelect the date format used in the attachment. If you want dates to be represented as text, select Show Dates as Strings.Data Contains Header RowSet this to true if the data contains a row containing column headers. Set this to false if the data does not contain a header row.Number As Strings ColumnsSelecting yes will add a column for all columns that are numbers and format them as strings to preserve their original format including any leading zeros.Ignore Decoding ErrorsSelect 'yes' to ignore decoding errors.\t\t\tOnly data that is processed before the decoding error will be retrieved.\t\t\tThis is recommended if there are extraneous characters at the end of the email file that are", "source": "../../raw_kb/article/dataset_via_email_connector/index.html", "title": "DataSet via Email Connector"}, {"objectID": "324a221ef58b-6", "text": "characters at the end of the email file that are corrupt.Allow Leading Digit in Column NameSelect if you want to allow column names to begin with a digit. Otherwise, every column name that starts with a digit will have the letter 'C' prefixed, i.e. 'C123'.Skip Empty RowsSelect this checkbox to avoid adding empty rows to the dataset.", "source": "../../raw_kb/article/dataset_via_email_connector/index.html", "title": "DataSet via Email Connector"}, {"objectID": "324a221ef58b-7", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling,\nretry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nTroubleshooting\nData is parsed incorrectly, such as having more rows or columns than expected.\nEnsure any literal quote character in a field's data is escaped to avoid it being parsed as one of the quote characters that enclose the field. A quote character can be escaped by the following ways:\nUsing backslash ( \\ ): \"dimensions\", \"two-by-four: 2\\\" x 4\\\"\"Using the quote character twice: \"dimensions\", \"two-by-four: 2\"\" x 4\"\"\"", "source": "../../raw_kb/article/dataset_via_email_connector/index.html", "title": "DataSet via Email Connector"}, {"objectID": "457a3a3847ab-0", "text": "Title\n\nDataSet Views\n\nArticle Body\n\nIntro\nViews Explorer is a business-friendly data exploration tool designed to make it easy to explore, wrangle, and combine your data in an easy to understand spreadsheet interface. This simple, yet powerful, DataSet Views experience allows you to perform data cleansing operations such as filtering, grouping, aggregating, and even combining data through a simple visual interface. Anyone with an Admin user or a Custom Role that has either the \"Edit DataSet\" or \"Manage DataSet\" grants enabled can use DataSet Views. For more information on Custom Roles see Managing Custom Roles.\nVideo - DataSet Views Explorer\n\nHow to find Views Explorer\nTo get started, choose the Open With\u00a0dropdown and select Views Explorer. This will take you to a new window where you will be able to explore your data.\n\nHow to explore data using Views Explorer\nViews Explorer has many ways to manipulate and cleanse data, they are:\nRemove ColumnsAdd ColumnsRename ColumnsMove ColumnsAdd Calculated ColumnPerform JoinsPerform UnionsFilter DataGroup DataSort DataLimit DataUndo/Revert ChangesSave the DataSet View\nRemove Columns\nRemoving columns is a common step in cleansing data, and Views Explorer makes it easy to remove columns from your DataSet. Columns can be removed by selecting the triple dot menu on each column and selecting Delete. To remove multiple columns, you can also choose the Select All\u00a0option from the columns on the left and the delete icon.\n \u00a0 \u00a0 \u00a0 \nAdd Columns\nIn certain situations, it makes sense to add a duplicate of a column to your DataSet. To add columns to the DataSet, choose the Add Column button at the bottom of the columns on the left.\n\nRename Columns\nRename a column by selecting the triple dot menu on a column and choosing Rename. This can also be done in by using the columns on the left.", "source": "../../raw_kb/article/dataset_views/index.html", "title": "DataSet Views"}, {"objectID": "457a3a3847ab-1", "text": "Move Columns\nReordering columns can be done in two places. To move a column select the Move\u00a0option in the triple dot menu and selecting one of the four options: Left, Right, First,\u00a0or Last. You may also drag and reorder columns by using the columns on the left.\n \u00a0 \nAdd Calculated Column\nTo add a new calculated column choose the plus sign and select Add Calculated Column.\n \u00a0 \nWhen you have finished writing the calculation, select Save and Close. This will automatically add the new calculated column to your DataSet in Views Explorer. For the new calculated column to save to your DataSet, you will need to save your DataSet as a new DataSet View.\n\nPerform Joins\nTo join DataSets using Views Explorer, toggle to the DataSet side of the schema rail. From there, select the Add Join button.\n\nNext, select a DataSet to join and choose the Join Keys from each DataSet. Once you have selected the keys, choose the Join Type you would like to use (we support Left, Right, and Inner Joins.) You will then have to select the columns you would like to include in your DataSet.\n\nFrom the Select Columns\u00a0modal choose which columns you would like to include in your DataSet and select Save. The columns you selected will now appear in your DataSet.\n\nPerform Unions\nTo union DataSets using Views Explorer, toggle to the DataSet side of the schema rail. From there, select the Add Union button.\n\nNext, select the DataSet(s) to union and select Save. The data from the selected DataSets will now appear in your DataSet.\n\n\n\n\n\n\nNote: In order for unions to work properly, both DataSets must contain the same columns. If your DataSets contain different columns, it is best to join them instead.", "source": "../../raw_kb/article/dataset_views/index.html", "title": "DataSet Views"}, {"objectID": "457a3a3847ab-2", "text": "Filter Data\nColumn filtering can be done with the columns on the left, using the Filter button at the top of Views Explorer, by choosing the triple dot menu on each column and selecting Filter, or by selecting Filter on each column.\n \u00a0 \nGroup Data\nGrouping data helps you to see only columns you are interested in and aggregations to summarize the data from the categories you have selected.\u00a0To Group data, start by selecting the Group\u00a0button.\n\nIn the Group By modal, drag columns you are interested in into the Categories section.\n\nIn the Group By modal, drag columns you are interested in aggregating into the Aggregations section.\n\nWhen you are finished, simply select the Finish button.\n\nSort Data\nSorting data allows you to sort your data alphabetically, by date, Low to High, or High to Low. To sort data, choose the Sort button at the top or select the triple dot menu on each column.\n \u00a0 \nLimit Data\nLimit means the DataSet will only return the number of rows you have chosen, sorted by the sorting rules you select. To limit the number of rows in your data, choose Limit and add the number of rows you would like to include in your DataSet.\n \u00a0 \nUndo/Revert Changes\nSometimes you want to undo your last step or just start over. Use the undo and revert buttons to go back a few steps, or start over at the beginning of your data exploration.", "source": "../../raw_kb/article/dataset_views/index.html", "title": "DataSet Views"}, {"objectID": "457a3a3847ab-3", "text": "Save the DataSet View\nWhen you have finished your exploration, you may want to save it as a new DataSet View. To save your Views exploration as a new DataSet View choose the Save As\u00a0Button. Choose a new name for your View and then Select Save.\n \u00a0 \nYour DataSet View will appear in Domo as a new DataSet that can be shared with others in Domo. PDP policies from the parent DataSet will still apply\u00a0and the DataSet View will update when the parent DataSet updates.\nFAQ's\nHow do DataSet Views treat NULL values?\nNull values are not considered to be equal or not equal to any other value. This means that comparing a column with a null value to another column using either = or != (<>) will always return false for that row.\nCan I use a View inside of a MySQL DataFlow?\nNo, at this time Views are not supported inside of MySQL DataFlows.", "source": "../../raw_kb/article/dataset_views/index.html", "title": "DataSet Views"}, {"objectID": "1ac9be3ad8e6-0", "text": "TitleDataWorld ConnectorArticle BodyIntro\nData.World is the modern catalog for data and analysis. It activates the hidden data workforce within your enterprise, multiplies your data value, and creates a faster data-driven culture. The modern, intuitive user experience brings together employees of all roles, backgrounds, and skills to\u00a0collaborate, and\u00a0keeps the data connected to everything people need to find, understand, and use. As a result, your data, analysis, and expertise become more\u00a0discoverable,\u00a0trustworthy, and reusable. Use Domo's DataWorld Connector to pull your data to get clear, accurate, and fast answers to your business question. To learn more about the Data.World API, visit https://apidocs.data.world/.\nYou connect to your Data.World account in the Data Center. This topic discusses the fields and menus that are specific to the Data.World Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Data.World account and create a DataSet, you must have the following:\nYour API Token associated\u00a0with the\u00a0Data.World account. You can find it\u00a0under\u00a0User Setting >> Advanced Tab.Your Data.World User ID.\u00a0You can find your user ID under your profile icon below your user name, in the upper right corner of the screen.\n\n\n \n\n\nNote:\u00a0The \"@\" sign is not a part of your user ID. Please do not include \"@\" while entering your user id.", "source": "../../raw_kb/article/dataworld_connector/index.html", "title": "DataWorld Connector"}, {"objectID": "1ac9be3ad8e6-1", "text": "Connecting to Your Data.World Account\nThis section enumerates the options in the Credentials and Details panes in the Data.World Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most Connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Data.World account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAPI KeyYour API Token associated\u00a0with the\u00a0Data.World account. You can find it\u00a0under\u00a0User Setting >> Advanced Tab.User IDYour Data.World User ID.\u00a0You can find your user ID under your profile icon below your user name, in the upper right corner of the screen.\nNote:\u00a0The \"@\" sign is not a part of your user ID. Please do not include \"@\" while entering your user id.Branded/single-tenant instance URLPlease enter branded/single-tenant instance URL. (For example:\u00a0[customername].data.world) This URL is necessary if you have a branded/single-tenant instance\nOnce you have entered valid Data.World credentials, you can use the same account any time you go to create a new Data.World DataSet. You can manage Connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a number of fields and menus for entering your query and specifying the location of the data you want to retrieve.\nMenuDescriptionQuery LanguageSpecify whether you want to use SQL or SPARQL as the query language for your query.Select the DataSet TypeSpecify whether you want to fetch the data from your own DataSets or public DataSets.Enter the User IDEnter the User ID you want to fetch the data for.", "source": "../../raw_kb/article/dataworld_connector/index.html", "title": "DataWorld Connector"}, {"objectID": "1ac9be3ad8e6-2", "text": "Note:\u00a0The User ID is only visible if you select\u00a0the Public Dataset option in the Dataset Type.\u00a0Note that the User ID\u00a0entered here is the creator of the Public DataSet you want to fetch.\n\nExample: To discover a public DataSet created by a user XYZ, you need to enter XYZ\u00a0as the user ID.\n\n\nDataSetsSelect the dataset you want to fetch the data from.Query TypeSpecify whether you want to write a customized query or an auto-generated query using the query builder.Custom QueryEnter the SQL query to execute based on the selected dataset. If your table contains space or hyphens then replace them with an underscore.\nExample: If the table name is Sample-Data Info\u00a0then it will become Sample_Data_InfoQuery BuilderSelect a table and fields to auto-generate your query.QueryEnter your database query to execute.Database TablesSelect the database table you want to fetch the data from.Table ColumnsSelect the table columns you want to fetch the data from.Data Model TypeSpecify whether you want to fetch the data from your DataSets or Projects File.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/dataworld_connector/index.html", "title": "DataWorld Connector"}, {"objectID": "1105612b8345-0", "text": "Titledata.world Writeback ConnectorArticle BodyIntro\ndata.world is the modern catalog for data and analysis. It activates the hidden data workforce within your enterprise, multiplies your data value, and creates a faster data-driven culture. The modern, intuitive user experience brings together employees of all roles, backgrounds, and skills to collaborate while keeping the data connected to everything people need to find, understand, and use. As a result, your data, analysis, and expertise become more discoverable, trustworthy, and reusable.\nWith the data.world Writeback connector export your data from a Domo dataset to your specified file securely and enhance your data performance.. To learn more about the data.world\u00a0API, visit their page https://apidocs.data.world/.\nYou configure your Domo-data.world connection in the Data Center. This topic discusses the fields and menus that are specific to the data.world\u00a0Writeback Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\n\n\n\n\n\nNote: The owner of a writeback dataset must also be an owner or co-owner of the input dataset.", "source": "../../raw_kb/article/dataworld_writeback_connector/index.html", "title": "data.world Writeback Connector"}, {"objectID": "1105612b8345-1", "text": "Prerequisites\nTo configure this Connector, you must have your data.world account API token. You can find your API token under\u00a0User Setting >> Advanced Tab.\nConfiguring the Connection\nThis section enumerates the options in the Credentials and Details panes in the data.world\u00a0Writeback\u00a0Connector page. The components of the other panes in this page, Scheduling and Name & Describe Your DataSet, are universal across most Connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your data.world account where you want your data to be copied to. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAPI TokenEnter the API Token. You can find your API token under\u00a0User Setting >> Advanced Tab.\nOnce you have entered valid credentials, you can use the same account any time you go to set up a new data.world\u00a0Writeback\u00a0DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a number of fields for specifying your data and indicating where it's going.", "source": "../../raw_kb/article/dataworld_writeback_connector/index.html", "title": "data.world Writeback Connector"}, {"objectID": "1105612b8345-2", "text": "This pane contains a number of fields for specifying your data and indicating where it's going.\nMenuDescriptionInput DataSet IDEnter your Domo dataset ID(GUID) located in the dataset URL. Example: https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settingsSelect data.world DataSet MethodSpecify whether you want to select your data.world dataset or enter the dataset ID manually.data.world DataSetSelect the location to copy your data.world dataset from.\u00a0data.world DataSetEnter the data.world dataset id.Select File Name MethodSpecify whether you want to select your output file or enter the filename manually.File NameEnter the output file name without the file extension. If the provided file name collides with an existing file, the connector will overwrite that file.FilesSelect the file name you want to write to. Files will have the `.csv` extension.Append Date or Number to FilenameOperationDescriptionNoneDoesn't append anything to the file name. This means that the file will always be overwritten on every run instead of creating new files.Append dateAppends date of current run to the file name (e.g., FileName_MM-DD-YYYY.csv). This means that if there are multiple runs on the same day, the previous run will be overwritten. Runs on new days will create new files.Append numberAppends run number (starting from 1) to file name (e.g., FileName_1.csv). This means that a new file will be created on every run.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this connector?\nYou need the API token associated with your data.world account.\nWhere can I find my API token?\nYou can find your API token under User Setting >> Advanced Tab.", "source": "../../raw_kb/article/dataworld_writeback_connector/index.html", "title": "data.world Writeback Connector"}, {"objectID": "1105612b8345-3", "text": "You can find your API token under User Setting >> Advanced Tab.\nAre there any API limits that I need to be aware of?\nNo\nHow often can the data be updated?\nAs often as needed.\nHow do I find the Input Dataset ID?\nYour Domo input dataset id is in the URL of the dataset you are exporting data from. For example: https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settings\nWhat is the File Name?\nThe File Name is the name of the output file where you would like to save your data. Provide the output file name without the file extension. If the provided file name collides with an existing file, the Connector will overwrite that file.", "source": "../../raw_kb/article/dataworld_writeback_connector/index.html", "title": "data.world Writeback Connector"}, {"objectID": "ed128ce882ed-0", "text": "TitleData Assembler Usage GuidelinesArticle BodyIntro\nThere are six key criteria for any Data Assembler job. If the job does not meet these criteria, the stability of the job and the validity of the data may be compromised.\nUsage Guidelines\nAverage Partition Size\nCalculate this by dividing the total row count by the number of partitions.Choose a partition key that makes the\u00a0average partition size as large as possible.An ideal average partition size is over 100k rows, but we recommend a minimum of 50k rows per partition.\nInput Ratio\nThis is the ratio of the input DataSets to the output DataSets. It represents the amount of data that is updated during execution and should be as small as possible.We recommend targeting 1% or less, but this percentage should not be greater than 10%.When using the Upsert option, limit yourself to one input.\nUpdate Frequency\nThe Data Assembler is intended to be used to update data once a\u00a0day.We strongly recommend that you do not exceed four executions a day.You should not use the Data Assembler to update data more frequently than once an hour.\nNumber of Partitions Updating on Each Execution\nYou should not use the Data Assembler if you need to update a large number of partitions on each execution.Jobs that execute daily should not exceed 100 partitions.Jobs that execute more frequently than daily should not exceed 10 partitions.\nSchema Changes\nThe schemas of the input and output DataSets should not change. The Data Assembler can handle certain schema changes, but we do not recommend using the tool to make these changes because the data may become corrupted.\nOverall Data Size\nFor data sizes less than 10 million rows, we generally recommend using the standard DataFlow options due to the additional flexibility provided by these Domo features.\nOther Tips", "source": "../../raw_kb/article/data_assembler_usage_guidelines/index.html", "title": "Data Assembler Usage Guidelines"}, {"objectID": "ed128ce882ed-1", "text": "Other Tips\nWhen you create a new job, start with a manual schedule. When the job is confirmed as ready for production, change the schedule as needed.Do not use triggers unless necessary. We recommend that you use the scheduling options first.Review jobs periodically. Deactivate or delete jobs that are no longer needed.When processing large historical fill jobs (over 100 million rows), consider breaking the history into smaller DataSets.\u00a0If an error occurs, you can restart the job on the portion of the data that failed.", "source": "../../raw_kb/article/data_assembler_usage_guidelines/index.html", "title": "Data Assembler Usage Guidelines"}, {"objectID": "897e4cf62d71-0", "text": "Title\n\nData Center Layout\n\nArticle Body\n\nIntro\nYou can manage DataSets and user accounts for connectors in the Data Center. If you have sufficient access rights, you can access this page by clicking Data\u00a0in the main toolbar at the top of the screen.\nThe Data Center is divided into four tabs. You can switch between the tabs by clicking the icons in the vertical navigation pane on the left side of the screen.The following tabs are available:\nTabIconDescriptionData WarehouseSee a three-dimensional visual representation of all DataSets in your Domo broken down by connector.DataSetsView and manage DataSets added to Domo. For any DataSet, you can access a Details\u00a0page that provides additional options.DataFlowsCreate and manage DataFlows, DataSets created by combining and transforming two\u00a0or more input DataSets.AccountsManage accounts used for connecting DataSets to Domo.Beast Mode ManagerView and manage Beast Modes created on your Cards.Data ScienceView the Data Science Home Page to find the tools you need.MoreView additional options that are enabled in your instance.\nYou can click the \u00a0icon to show the names of the above tabs.\u00a0\nVideo - Data Center Overview", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-1", "text": "Data Warehouse tab\nThe Data Warehouse in Domo provides a three-dimensional visual representation of all DataSets in your Domo, broken down by connector, along with data currently flowing into and between them. DataSets for each connector type are represented as stacks on a rotating palette.\u00a0You can configure the order and height of the connector stacks to indicate different metrics. For example, you could sort the connector stacks by number of rows but have the height of the individual stacks represent the number of DataSets. For more information about The Data Warehouse, see Using The Data Warehouse to Manage Data.\nIn the Data Warehouse\u00a0tab you have access to a toolbar that provides shortcuts for opening the DomoR installation page.\nDataSets\u00a0and DataFlows\u00a0tabs\nThe\u00a0DataSets\u00a0and\u00a0DataFlows\u00a0tabs list\u00a0DataSets and DataFlows, respectively, in your Domo instance. In these tabs you can", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-2", "text": "view DataSets\u00a0and DataFlows that have been added to DomoFor a DataSet, you can view information about the DataSet, including the connector, the name, the owner, the number of rows and columns in the DataSet, the number of cards being powered by the DataSet, the total number of times these cards have been viewed, and the amount of time since the DataSet was last updated. You can also preview or delete a DataSet.For a DataFlow, you can view information about the DataFlow, including the name, owner, number of input and output DataSets, number of runs vs. success rate, and the amount of time since the DataFlow was last run.Both tabs include options for searching with or without filters, applying quick filters, saving favorite filters, and sorting the DataSets/DataFlows in the list.add DataSets and DataFlowsFrom either tab you can...add data into Domo from third-party systems, then transform the data to power up multiple cards without having to locate and re-upload that DataSet for each card.\u00a0add Magic transforms of any kind\u2014DataFusions.get information about and download Workbench.get information about installing the DomoR plugin.visually identify DataSets needing attentionDataSets with errors do not run as scheduled until errors are resolved.\u00a0 Domo lets you know when a DataSet cannot run successfully by displaying the error on the DataSet in the Data Center and in the Accounts page, and by sending the owner of the DataSet an alert that describes the error and links to the DataSet.\nThe following screenshot points out the most important parts of the DataFlows\u00a0and DataSets\u00a0tabs. (This\u00a0screenshot is specifically from the\u00a0DataFlows\u00a0tab, but\u00a0the only difference between the two tabs is different information in the rows.)", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-3", "text": "You can learn about these components in the following table:\nNameDescriptionSearchLets you search for the desired DataSet/DataFlow in the list. This search draws from a variety of metadata, including name, connector, owner, tags, status, and more. You can refine your search by doing any of the following:Clicking a recommended filter. These may appear when you enter a search term or part of search term in the field.Adding a custom filter. You do this by clicking\u00a0Add Filter, selecting the desired filter from the list, then entering the filter criteria as required.\u00a0For example...", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-4", "text": "If you selected\u00a0Owned\u00a0By, you would be prompted to enter the owner of the DataSet or DataFlow you wanted to view.\u00a0If you selected\u00a0DataFlow\u00a0Type, you would be prompted to select the desired DataFlow type from a list.\u00a0\u00a0If you selected Last Updated, you would be prompted to specify whether the DataFlow or DataSet was last updated on, before, or after the given date, then select the date itself.\u00a0Changing the sort used in the list. You do this by clicking in the menu\u00a0in the top right corner of your search results (shown as \"Relevance\" in the above screenshot) then\u00a0choosing the desired sort type.For DataSets, you can sort by relevance, name, number of cards, number of rows, last run date,\u00a0or status (broken DataSets\u00a0are shown first).\u00a0For DataFlows, you can sort by relevance, creation date, last modified date, name, status (broken DataFlows are shown first), success rate, or last run date.Add FilterLets you filter the DataSets or DataFlows in the list. This option is described in more detail in the entry for \"Search,\" above.Favorite Filter iconLets you save the current filter configuration. When you click this icon, you are prompted to enter a unique name for the filter. When you save the new filter, the star icon turns gray and the filter appears under Favorite Filters\u00a0in the panel on the left side of the screen. You can then click this filter anytime to apply it.\u00a0\nYou can remove a saved filter by clicking the gray star icon and then choosing\u00a0Remove.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-5", "text": "You can remove a saved filter by clicking the gray star icon and then choosing\u00a0Remove.\nYou can also do a \"Save As\" on a saved filter. This is useful when you make changes to an existing filter and want to keep both filters. To do this, you click on the gray star icon, enter the new filter name in the name field, click Update, then select\u00a0Save As New Filter.\nThese options are also available via the\u00a0Favorite Filters\u00a0area in the panel on the left side of the screen. If you mouse over a filter here, a gear icon displays. Clicking this icon reveals the following options:Rename. Lets you rename a saved filter.Duplicate. Lets you do a \"Save As\" on the filter. When you save the filter in this way, a second version of the filter appears in the list, with the word \"copy\" appended to it. You can rename it if you want using the\u00a0Rename\u00a0option described above.Delete. Removes the filter.Sort\u00a0Lets you sort the DataSets or DataFlows\u00a0in the list.\u00a0This option is described in more detail in the entry for \"Search,\" above.Quick Filters/Favorite FiltersLets you quickly apply any prebuilt\u00a0filters (Quick Filters) or saved filters (Favorite Filters).", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-6", "text": "Available Quick Filters include the following:All DataSets/All DataFlows. Removes all filters from the list. This is a good way to \"reset\" the list after you have applied many filters.Recently run. Filters to show only those DataSets/DataFlows that have been run in the last several days.Owned by you. Filters to show only those DataSets/DataFlows\u00a0you are the owner of.Needs attention. Filters to show only those DataSets/DataFlows in a broken state (indicated by a red exclamation point on the DataSet/DataFlow in the list).Favorite Filters are those filter configurations you have saved.\u00a0These are\u00a0described in more detail in the entry for \"Favorite Filter icon,\" above.DataSets/DataFlowsDisplay information about each DataSet or DataFlow.\u00a0For DataSets, this information includes\u00a0the name, connector, owner profile picture, number of rows and columns, number of cards powered by the DataSet, any tags, and the last update time. For DataFlows, the information includes the name, owner, number of input and output DataSets, number of runs versus the success rate, any tags, and the last run time.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-7", "text": "For each DataSet or DataFlow, you can do the following:Click the photo to open a user avatar card with information, including a list of groups to which the user belongs, card view statistics, and a Follow button so you can follow this user in Buzz.Click the DataSet or DataFlow name to open the details view for the DataSet or DataFlow, which provides additional options. For example, for a DataSet, you can see a preview of the data, cards being powered by the DataSet, a run history, and so on.\u00a0Mouse over the row to reveal a wrench icon \u00a0that provides access to various options for interacting with this DataSet\u00a0or DataFlow. These options are listed in the entry for \"Options,\" beow.Add search tags to this DataSet or DataFlow so you can more easily find it later. This is described in more detail in the entry for \"Add Tag,\" below.Take bulk actions on DataSets or DataFlows. You can select an initial\u00a0DataSet\u00a0or DataFlow\u00a0to take action on by mousing over the row then clicking the checkbox that appears", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-8", "text": "over the row then clicking the checkbox that appears in the top left corner of that row. This pops up a blue bar along the top of the screen with new options. From here you can do all of the following:Select more DataSets/DataFlows by mousing over them and checking their checkboxes.Select all DataSets/DataFlows\u00a0currently appearing in the list by clicking\u00a0Select All\u00a0in the blue bar.Tag selected DataSets/DataFlows by clicking the \u00a0icon on the right side of the blue bar. For more information about tagging, see the entry for \"Add Tag\" in this table.Apply additional actions by clicking the \u00a0icon on the right side of the blue bar then selecting the desired option:Run Now. This executes all of the selected DataSets or DataFlows.\u00a0Change Owner.\u00a0This allows you to select a new owner for the selected DataSets or DataFlows. For more information, see\u00a0Changing the Owner of a DataFlow.\u00a0Delete. This allows you to delete all of the selected DataSets\u00a0or DataFlows. To delete DataSets you must have an \"Admin\"", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-9", "text": "To delete DataSets you must have an \"Admin\" security profile or be the DataSet owner. For more information, see Disabling or Deleting a DataFlow.\u00a0\u00a0For a DataFlow, you can also mouse over the \"Inputs/Outputs\" data to show all of the input and output DataSets\u00a0for this DataFlow.\u00a0Add TagLets you add search tags to this DataSet or DataFlow so you can easily find it later by searching for those tags. When you click\u00a0Add tag, a dialog appears with a list of existing\u00a0tags. (If no tags have yet been added for your company, this list appears blank.)", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-10", "text": "You can then do either of the following:Add existing\u00a0tags to the DataSet or DataFlow\u00a0by clicking them in the list.\u00a0Any tags that have already been added appear blue. You can filter the tags in the list by entering the desired tag name in the\u00a0Search and add tags\u00a0field.\u00a0Add a\u00a0new\u00a0tag\u00a0to the DataSet or DataFlow by entering the tag name in the\u00a0Search and add tags\u00a0field. If the tag does not yet exist, it appears alone in the list area with the \"Create new tag\" designation. You can then click it to add it to the DataSet or DataFlow.\u00a0You can remove a tag from a DataSet or DataFlow by clicking the \"x\" next to the tag while in this dialog.\nYou can also access the tagging functionality by doing any of the following:Selecting View Tags\u00a0in the options menu for a DataSet or DataFlow in the list view.Selecting\u00a0Tags\u00a0in the options menu for a DataSet\u00a0or DataFlow\u00a0details view.Clicking\u00a0Add tag\u00a0under the DataSet\u00a0name in the details view (DataSets only).You can add tags to multiple DataSets or DataFlows at once by selecting the DataSets or DataFlows (by checking their checkboxes), clicking \u00a0to open the tagging dialog, then adding tags as desired. For more information about this dialog, see the entry for \"Add Tag\" in this table.\u00a0\u00a0\nWhile in the list view for a DataSet or DataFlow, if you click a tag, the list filters to show only DataSets and DataFlows with that tag.\u00a0Options icon", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-11", "text": "Displays a menu of options for the DataSet or DataFlow. Appears when you mouse over the row for a DataSet or DataFlow.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-12", "text": "For a DataSet, available options are as follows:View Details. Opens the Details view for this DataSet. In this view you can see an overview of the DataSet, previews of the cards powered by the DataSet, and a run history.\u00a0View Tags. Opens the tag dialog for this DataSet. For more information, see \"Add Tag\" in this table.Preview. Opens a preview of\u00a0the data in this DataSet.Delete. Deletes this DataSet. You can delete DataSets\u00a0owned by other users only if you have an \"Admin\" security role or a custom role with the \"Manage DataSets\" privilege enabled .\u00a0For more information about security roles, see Understanding security roles. For information about custom roles, see\u00a0Managing Roles.For a DataFlow, available options are as follows:Details. Opens the Details view for this DataFlow. In this view you can set scheduling options, view input and output DataSets, see the run history, and access a list of previous versions of the DataFlow.Edit. Opens the editor for the DataFlow. For more information about creating or editing a DataFlow, see\u00a0Magic Transforms.View", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-13", "text": "creating or editing a DataFlow, see\u00a0Magic Transforms.View Tags. Opens the tag dialog for this DataFlow. For more information, see \"Add Tag\" in this table.Run. Runs this DataFlow, which updates the input and output DataSets. For more information, see\u00a0Running a DataFlow.Disable. Disables the DataFlow, preventing\u00a0DataSets powered by the DataFlow from running. For more information, see\u00a0Disabling or Deleting a DataFlow.Notifications. Lets you choose whether you want to receive email notifications when the DataFlow fails to update. For more information, see\u00a0Configuring Notifications for Failed DataFlows.Create Copy. Does a \"Save As\" on this DataFlow.\u00a0\u00a0Copied DataFlows can be renamed, edited, saved, and run independently of their original DataFlow. For more information, see\u00a0Copying a DataFlow.Delete. Deletes this DataFlow.\u00a0You can delete DataFlows\u00a0owned by other users only if you have an \"Admin\" security role or a custom role with the \"Manage DataFlows\" privilege enabled .\u00a0For more information about security roles, see Understanding security roles. For information about custom roles,", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-14", "text": "see Understanding security roles. For information about custom roles, see\u00a0Disabling or Deleting a DataFlow.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-15", "text": "Important:\u00a0If you delete a DataSet that powers any cards, the cards display a \"Data not loading\" message.\u00a0\n\n\n\n\u00a0\n\n\n\n\n\nTips:\nYou can view DataSets for a specific third-party application or data provider from the Launcher page. For more information, see\u00a0Viewing Cards for Data Providers.In the Profile page, you can view all of the DataSets owned by a user. For more information, see\u00a0Viewing a User's Owned DataSets.\n\n\n\n\nView Details\nWhen you click the name of a DataSet or click \u00a0> View Details, a page appears showing details for the DataSet. This page is divided into a number of tabs. Some of these tabs appear for every DataSet you view, while others only appear for specific user roles and connector types.\u00a0These tabs are\u00a0Overview, Cards, History, Settings, Data Lineage, and Personalized Data Permissions.\u00a0\nFor more information, see\u00a0Connecting to Data with Connectors.\nVideo - Domo Interface - DataSet Details", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-16", "text": "The\u00a0Overview\u00a0tab includes four tiles that show you the number of cards powered from this DataSet, the number of users it is being shared with, the number of DataSets\u00a0built from the connector, and the number of DataFlows\u00a0created from the DataSet. All of these tiles also provide \"jump-off points\" to other actions in Domo\u00a0using this DataSet.\u00a0\u00a0The\u00a0Cards\u00a0tab contains previews of cards powered by the DataSet.The\u00a0History tab shows the run history for the DataSet and only appears if the DataSet\u00a0has run history data.The\u00a0Settings\u00a0tab lets you change the connector configuration options for the DataSet.\u00a0It only appears if you are the DataSet owner or have an \"Admin\" security role or a custom role with the \"Manage DataSets\" privilege enabled.The\u00a0Data Lineage\u00a0tab shows you the lineage for this DataSet.\u00a0The\u00a0Personalized Data Permissions\u00a0tab allows you to set PDP policies on the DataSet\u00a0and does not appear unless you are the DataSet owner or have an \"Admin\" security role or a custom role with the \"Manage DataSets\" privilege enabled.\nFor more information, see\u00a0Connecting to Data with Connectors.\u00a0\nGeneral options\nThe following screenshot shows the details view for a sample DataSet\u00a0called \"College Enrollment.\" All of the options called out here are available in all tabs.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-17", "text": "You can learn about these components in the following table:", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-18", "text": "NameDescriptionConnector\u00a0The connector for this DataSet.\u00a0For more information about connectors, see\u00a0Configuring Each Connector.DataSet description\u00a0A description of this DataSet. You can add or change a description by selecting\u00a0Edit Name & Description in the Options\u00a0menu then entering the desired\u00a0description.\u00a0DataSet name\u00a0The name of this DataSet. You can add or change a name for a DataSet\u00a0by selecting\u00a0Edit Name & Description in the Options\u00a0menu then entering the desired\u00a0name.\u00a0Number of columns and rows\u00a0A size indicator for the DataSet. \u00a0Add tag optionLets you add search tags to this DataSet. If any tags have already been added, they appear here. Adding tags is described in more detail in the previous table.\u00a0Most recent update timeIndicates when this DataSet was last refreshed.\u00a0For more information, see\u00a0Setting the Expected Update Frequency for a DataSet.OwnerShows who owns this DataSet. You can click a user's photo to see an avatar card with his or her basic information and a Go To Profile button that provides access to the Profile page. For more information about the Profile page, see\u00a0Profile", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-19", "text": "For more information about the Profile page, see\u00a0Profile Page Layout.DataSet OverviewShows how this DataSet is being used.TabsLet you switch between the tabs in the details page for the DataSet.   about each tab is provided later in this section.Edit WebformEdit Webform\u00a0(Webforms only). Lets you edit the data for this Webform. For more information, see\u00a0Uploading a webform.Open with menuProvides access to a number of options for the DataSet. Some options may not appear, depending on the Connector and your security role.Analyzer. Opens this DataSet\u00a0in Analyzer so you can create a KPI card around it. For more information, see\u00a0Analyzer Overview.Views Explorer. Opens the DataSet Views editor. For more information, see DataSet Views.Magic ETL. Opens the Magic ETL editor. For more information, see Creating a Magic ETL DataFlow.SQL DataFlow. Opens the SQL DataFlow editor. For more information, see Creating a SQL DataFlow.Options iconProvides access to a number of options for the DataSet. Some options may not appear, depending on the Connector and your security role.\u00a0Edit Name\u00a0& Description.\u00a0Lets you", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-20", "text": "your security role.\u00a0Edit Name\u00a0& Description.\u00a0Lets you open a dialog in which you can change the name and\u00a0description of this DataSet.Tags.\u00a0Lets you add search tags to this DataSet. Adding tags is described in more detail in the previous table.Notifications.\u00a0Lets you subscribe to or unsubscribe from email notifications for the DataSet.Request certification. Lets you request the DataSet be certified.Chart color rules. Lets you apply color rules to the columns in this DataSet. For more information, see\u00a0Setting Color Rules for a Chart.Share DataSet. Lets you share the DataSet with users.Export data. Lets you export this DataSet to an Excel or CSV file. For more information, see\u00a0Exporting DataSets.Run Now.\u00a0Updates the DataSet, causing any connected cards to be updated as well.Delete.\u00a0Deletes the DataSet. You can delete a DataSet only if you are the owner or have an \"Admin\" security profile or a custom role with the \"Manage DataSets\" privilege enabled. For more information about security profiles, see Managing Roles.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-21", "text": "Overview tab\nIn the Overview tab you can see how the DataSet is being used, the number of Cards powered from this DataSet, the number of users it is being shared with, the number of DataFlows created from the DataSet, and the number of Alerts on the DataSet. All of these tiles also provide \"jump-off points\" to other actions in Domo using this DataSet. The following screenshot shows an example of these tiles in the Overview\u00a0tab for a typical DataSet.\n\u00a0\u00a0\n\"Jump-off points\" for these tiles are as follows:\nView Full Impact\u00a0opens the Lineage tab for the DataSet.Create A Visualization\u00a0opens the Analyzer for this DataSet so you can build a chart or table from it. For more information about Analyzer, see\u00a0Analyzer Overview.Share This DataSet opens the menu to share the DataSet with users.Tidy Up This DataSet\u00a0opens the interface for building an ETL DataFlow using this DataSet.Create A New Alert opens the interface for creating an Alert on the DataSet.\nData tab\nIn the\u00a0Data\u00a0tab, you can do all of the following:\nView all of the rows in the DataSet, and sort and filter to show the data interests you.See statistics for individual columns or for the entire DataSet.Change the order of columns in the DataSet.Rename columns in the DataSet.Add or edit column descriptions.Add search tags for columns.\nThe\u00a0Data tab\u00a0is made up of three subtabs \u2014Table,\u00a0Schema, and\u00a0Stats. You can switch between the subtabs\u00a0by clicking the desired subtab name in the upper right corner of the\u00a0Data\u00a0tab.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-22", "text": "In all three subtabs, you can search\u00a0for and filter to a specific column or columns in the DataSet\u00a0by entering the desired column name in the\u00a0Search\u00a0Columns\u00a0field.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-23", "text": "You can enlarge the column pane to fill the whole screen by clicking  or restore it to its original size by clicking . You can Show column statistics/Hide column statistics by clicking . This allows you to show or hide the \"filter charts\" that appear at the top of the column pane in the Table\u00a0subtab.\u00a0Appears in the\u00a0Table\u00a0subtab only.\nTable subtab\nIn the Table\u00a0subtab, you can see all of the columns in your DataSet. Options are available for searching, sorting, and filtering so you can find the data you need. The following screenshot shows the components of the\u00a0Table\u00a0subtab:\u00a0\n\u00a0\nYou can learn about these components in the following table:\nNameDescriptionSort button\nLets you sort the values in a given field in ascending or descending alphanumerical order. Sorts are \"nested\" based on the order in which you apply them. The numeral beneath the sort icon tells you the priority of this column in your sort.\u00a0\nFor example, if a user first applied an ascending sort to the \"Transaction ID\" column, all of the rows in the DataSet would be ordered in numerical order. If he then applied an ascending sort to the \"SKU\" column, the groupings of rows for each SKU would be ordered numerically. If he finally applied a third sort, descending, to the \"Zip Code\" column, those values would be sorted in descending order for each transaction-SKU grouping.\u00a0\nThe following screenshot shows an example of this behavior:\u00a0\u00a0Info button", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-24", "text": "The following screenshot shows an example of this behavior:\u00a0\u00a0Info button\nShows the description for this column, if you have added one in the\u00a0Schema\u00a0subtab.Data type iconIndicates the data type for this column.  indicates a numeric (value) column,  indicates a date or date-time column, and \u00a0indicates a string column. For more information about data types, see\u00a0Understanding Chart Data.Filter chartLets you filter the data in the DataSet\u00a0based on the values in a given column. Different chart types appear for columns depending on the data type for that column.\nFor text (string) columns, a horizontal bar appears, divided into segments for each unique name in the column. You can filter the data in the DataSet\u00a0to a particular name by clicking on that section\u00a0in the bar.\u00a0(Names appear as hovers when you mouse over a section.) In the following example, the user could filter the DataSet to show only the data for \"Delivery Truck\" by clicking on the corresponding bar.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-25", "text": "For date/date-time and value columns, a vertical bar chart with a horizontal slider appears. You can filter the data in the DataSet to show dates or values above or below a certain threshold by sliding the bar accordingly. In the following example, the user could filter the DataSet to show only values of .05\u00a0or higher by sliding the bar to \".05.\"\u00a0\n\n\nSchema subtab\nIn the Schema\u00a0subtab, you can reorder the columns in your DataSet,\u00a0add descriptions and tags, etc. The following screenshot shows the components of the Schema\u00a0subtab:\u00a0\n\u00a0\nYou can learn about these components in the following table:\nNameDescriptionSearch fieldLets you search for and filter to a specific column or columns in the DataSet.Reorder iconLets you change the order of columns in this DataSet by clicking and dragging a column into the desired position. Note that this option is only available for the \"Index\" column, and only when sorting has been applied to that column. So, for example, you could not sort by column name and then click and drag to reorder columns.\u00a0Sort buttonLets you apply an ascending or descending sort to the column. Note that this sort\u00a0only\u00a0changes the order of column names in the\u00a0Schema\u00a0subtab\u00a0so you can more easily find what you're looking for. It does\u00a0not\u00a0change the actual order of columns in the DataSet. If you want to change the column order, use the Reorder icon.\u00a0Column descriptionLets you enter a description for a column or edit an existing description.Column tagsLets you enter search tags for this column. For more information about tags, see the table under\u00a0DataSets\u00a0and DataFlows\u00a0Tabs.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-26", "text": "Stats subtab\nIn the\u00a0Stats\u00a0subtab, you can see Domo-generated statistics for the columns in your DataSet. For numeric and date/date-time columns, you can correlate the columns against other numeric columns in the DataSet, and for text columns, you can see the highest aggregated values for the sum or\u00a0count of any selected column.\u00a0The following screenshot shows the components of the Schema\u00a0subtab:\u00a0\n\u00a0\nYou can learn about these components in the following table:\nNameDescriptionSearch fieldLets you search for and filter to a specific column or columns in the DataSet.Column stats rowShows statistics for a given column.\u00a0Statistics for date/numeric columnShows automatically-generated statistics for a numeric or date/date-time\u00a0column. Available statistics include minimum, maximum, average, median, upper and lower quartile, and standard deviation.Statistics for string columnShows automatically-generated statistics for a string column. Available statistics include shortest string, longest string, and average length.Correlations chartShows correlations for numeric and date/date-time columns. For the given numeric or date/date-time column, you can select any other numeric column and see how the two columns correlate.\u00a0Aggregations chartShows the five highest aggregated values of the given column from the selected\u00a0column. If the selected column is numeric, the values are summed; otherwise the values are counted. For example, in the above screenshot, the user has chosen to view the five highest aggregated sales values for the \"Customer Name\" column. He does this by selecting \"Sales\" in the Aggregations\u00a0dropdown for the \"Customer Name\" column.\n\nCards tab\nThe following screenshot\u00a0shows the components of the Cards tab:\n\nYou can learn about these components in the following table:\nNameDescriptionConnected CardsAll cards powered by this DataSet. You can click on a card to see the Details view for that Card.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-27", "text": "Tip:\u00a0You can view Cards powered by a specific third-party application or data provider from the Launcher page. For more information, see Viewing Cards for Data Providers.\n\n\nSwitch cards to a different DataSet optionAllows you to move all of the Cards powered by this DataSet to another DataSet. This option appears if you are the owner of the DataSet, depending on the DataSet type. For more information, see Connecting Cards to a Different DataSet.Add Card optionLets you power up a Card using the DataSet. Select the DataSet from the list to use it in powering a card. By default, the Card is added to the Overview page in Domo. This option may not appear, depending on the DataSet type. For more information about this option, see Powering a Visualization Card with Data.\u00a0\n\nHistory tab\nIn the History tab, you can see a listing of all of the times this DataSet has been updated. This tab shows the start and end times for all updates, the amount of time the update took (duration), whether the update was scheduled or manual, and the result of the update (successful or failed). You can also see the amount of time that has transpired since the last successful run, the average update duration, and the overall success rate. Finally, for any history item, several actions are available by mousing over the row and clicking the wrench icon.\nThe following screenshot shows an example of a History tab for a DataSet called \"Compensation Costs\":", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-28", "text": "Action items available by clicking the wrench icon are as follows:\nPreview updated rows. Displays the rows in the DataSet that were changed when this update occurred.\u00a0Download. Downloads the data in this version of the DataSet as a flat text file that can be opened in an Internet browser or word processor.Delete this update. Deletes this version of the DataSet from your history.Revert to this point. Reverts the DataSet to this version.\nActions are available only for successful updates.\nSettings tab\nIn the Settings\u00a0tab for a DataSet, you have access to the\u00a0options used to set up this DataSet, similar to the view that appears when you initially configure a connector DataSet. Click a row to open up the\u00a0settings in that row for viewing and editing. Settings categories may differ between connectors. For more information about connector configuration options, see\u00a0Adding a DataSet Using a Data Connector.\n\nData Lineage tab\nIn the Data Lineage tab for a DataSet, you can see the DataSets that have been combined and/or transformed through DataFlows or DataFusion to yield this DataSet. The Data Lineage interface in Data Center is the same as that used for a DataSet in Analyzer. For more information, see\u00a0Viewing the Lineage of a DataSet in Analyzer.\nPersonalized Data Permissions (PDP) tab\nPersonalized Data Permissions (PDP)\u00a0allow you to create a customized experience for each Domo user through the definition of Entitlement Policies. Using\u00a0Entitlement Policies, you can\u00a0filter data in a DataSet for specified users and/or groups. For more information, see\u00a0Personalized Data Permissions (PDP).\nThe following screenshot points out the most important parts of the PDP tab:", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-29", "text": "You can learn about these components in the following table:\nNameDescriptionSearch fieldLets you search for existing policies that have been added to this DataSet.Policy rowShows information about a policy, including its name, the rows of data it provides access to, and the groups and users the policy has been applied to.\u00a0Enable PDP\u00a0toggleActivates/deactivates this PDP policy.Impact buttonOpens a list of cards and alerts that will be affected by PDP policies on this DataSet.\u00a0Add Policy buttonLets you add a new PDP policy to this DataSet. For more information about adding a PDP\u00a0policy, see Creating and Deleting PDP Policies.\n\nView Details (DataFlows)\nWhen you click the name of a DataFlow or click \u00a0> View Details, a page appears showing details for the DataFlow. This page is divided into tabs\u2014\u200b\u200bSettings, DataSets, History, and Versions. For more information about these tabs, see\u00a0Viewing DataFlow Details.\nEditor view (MySQL and Redshift DataFlows)\nThe following screenshot shows you the main components of the\u00a0Create/Edit DataFlow\u00a0view for a MySQL or Redshift DataFlow.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-30", "text": "You can refer to the following table to learn more about each component:\nNameDescriptionDataFlow Type & InfoWhen collapsed, this panel displays the type of DataFlow and the schema.\nWhen expanded (by clicking ), this panel displays information about the RDS instance for this DataFlow, including the host name, schema, user name, password, and port number. You can copy and paste this information into any SQL tool, including Workbench.DataFlow Name & DescriptionAllows you to specify a name and description for this DataFlow.Input DataSetsAllows you to select one or more existing DataSets for use in creating DataFlows. You can choose as many input DataSets as you want. For any given transform or output, you can use one, some, or all of the DataSets you select here. This is essentially a storage area for DataSets you may potentially use. You cannot manipulate data in the DataSets at this stage. You also cannot select the same input DataSet twice in the same DataFlow.\n\n\n \n\n\nNote:\u00a0Input\u00a0DataSets in a DataFlow must already exist in Domo; you cannot upload new DataSets in the Create DataFlow view. For information about uploading new DataSets to Domo, see Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-31", "text": "You can click an input DataSet you have added to see a preview of the DataSet.TransformsAllows you to manipulate data in your input DataSets using SQL statements before you make your final manipulations in the Output DataSets stage. The Transforms stage is optional. All input DataSets you have selected are available for use in transforms.Output DataSetsAllows you to specify how your input DataSets are to be combined to produce one or more output DataSets. This interface is almost the same as that of Transforms. However, while adding transforms is optional, the Output DataSets stage is required; otherwise you cannot save your DataFlow. All input DataSets and transforms you have selected/created for this DataFlow are available to be used in your Output DataSet SQL.Save buttonsAllow you to save this DataFlow. Two separate buttons are available: Save and Save and Run. Save saves changes you have made to this DataFlow and adds it to the DataFlows listing in the Data Center but does not generate a DataSet (or, if you are editing an existing DataFlow, it does not update the output DataSets). Save and Run saves changes to the DataFlow and adds it to the DataFlows listing in the Data Center and runs the script for the DataFlow to output a DataSet (or update the output DataSets if it has already been created).\nFor more information about DataFlows, see\u00a0SQL DataFlows.\nEditor view (Magic ETL DataFlows)\nFor times when you need to transform your DataSets before you can make compelling visualizations, you can use Magic ETL DataFlows to transform multiple DataSets into a new DataSet that you can use to power up cards.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-32", "text": "Tip:\u00a0ETL means Extract, Transform, and Load, which refers to a process in database warehousing for extracting data, transforming it into proper format or structure for querying and analyzing purposes, and loading it into the target data warehouse. In Magic ETL DataFlows, your DataSets are extracted and loaded automatically, and transformed based on actions in the Magic ETL DataFlow.\n\n\n\nMagic ETL DataFlows let you visually define and sequence operations to transform your Domo DataSets\u2014without learning SQL or leaving Domo. For information about the Magic ETL DataFlow tiles, see all of the following:\u00a0\nCreating a Magic ETL v2 DataFlowMagic ETL v2 Tiles: DataSetsMagic ETL v2 Tiles: TextMagic ETL v2 Tiles: Dates and NumbersMagic ETL v2 Tiles: UtilityMagic ETL v2 Tiles: FilterMagic ETL v2 Tiles: Combine DataMagic ETL v2 Tiles: AggregateMagic ETL v2 Tiles: PivotMagic ETL v2 Tiles: ScriptingMagic ETL v2 Tiles: Data ScienceMagic ETL v2 Tiles: Performance\nETL DataFlow Example", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-33", "text": "The UI for ETL DataFlows consists of the following elements:\nElementDescriptionAction paneContains the transform tile you can drag and drop into the canvas to use in transforming data.\nFor more information, see the following topics:\u00a0Creating a Magic ETL v2 DataFlowMagic ETL v2 Tiles: DataSetsMagic ETL v2 Tiles: TextMagic ETL v2 Tiles: Dates and NumbersMagic ETL v2 Tiles: UtilityMagic ETL v2 Tiles: FilterMagic ETL v2 Tiles: Combine DataMagic ETL v2 Tiles: AggregateMagic ETL v2 Tiles: PivotMagic ETL v2 Tiles: ScriptingMagic ETL v2 Tiles: Data ScienceMagic ETL v2 Tiles: PerformanceCanvasContains your transformation flow\u2014the sequence of tiles to perform on input DataSets to save as an output DataSet. You connect tiles to sequence them and select tiles to configure them.Lets you preview the transform tile.Lets you schedule\u00a0the Magic ETL DataFlow to run whenever the specified input DataSets change.Lets you show or hide the Magic ETL DataFlow checklist, which shows a list of things to do before you can save a Magic ETL DataFlow.\n\nETL DataFlow checklist\nThe checklist shows a list of things to do before you can save a Magic ETL DataFlow. You can show the checklist by clicking  at the top of the canvas. The following example illustrates essential tasks to perform before saving a Magic ETL DataFlow.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-34", "text": "Essential tasks include the following:\nCreate a Magic ETL Dataflow.From the tiles pane on the left, drag Input DataSet tiles to the canvas and select the DataSets to transform.Drag transform tiles to the canvas.For more information, see the following topics:Creating a New Magic ETL DataFlowNew Magic ETL Tiles: DataSetsNew Magic ETL Tiles: TextNew Magic ETL Tiles: Dates and NumbersNew Magic ETL Tiles: UtilityNew Magic ETL Tiles: FilterNew Magic ETL Tiles: Combine DataNew Magic ETL Tiles: AggregateNew Magic ETL Tiles: PivotNew Magic ETL Tiles: ScriptingNew Magic ETL Tiles: Data ScienceNew Magic ETL Tiles: PerformanceDrag an Output DataSet tile to the canvas, then name the DataSet to be output from your transformed data.Draw connections between the transform tiles to build a DataFlow that defines and sequences operations to transform the input DataSets (clean, aggregate, join, etc.).(Optional) Configure settings for when the DataFlow runs.By default, the DataFlow runs in the cloud whenever the specified input DataSets change.Name and save your Magic ETL Dataflow.\nFor more information about DataFlows, see\u00a0Viewing Dataflow Details.\nAccounts tab\nIn the Accounts tab, you can manage third-party system accounts associated with DataSets in Domo. You can view accounts associated with DataSets; add, edit, or delete accounts; and associate DataSets with another account.\n\n\n \n\n\nTip:\u00a0By creating accounts to data source systems, you can specify and maintain your credentials in one central place, then create DataSets in Domo using the account\u2014without having to specify your credentials to the data source system.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-35", "text": "In the Accounts tab, you can\nadd user accounts for different connectors (similar to a DSN)view a list of user accounts you have addeddisconnect accounts you have addedshare accounts with other userssee DataSets you have access totransfer DataSets from\u00a0one account to another accountchange account associationsrename accountsvisually identify DataSets needing attentionedit credentials for accounts that require manual authentication (as opposed to those that use OAuth)\nThe following screenshot calls out the most important parts of the\u00a0Accounts\u00a0tab:\n\nYou can learn about these components in the following table:\nNameDescriptionSearch boxLets you filter the accounts in the list.Add Account buttonLets you add a new user account to this list.Add Federated buttonLets you add a Federated account to this list.Assign Someone buttonLets you assign someone to create an account for a specific Connector.Connector accountsProvide information and options for all accounts you have used to connect to data sources, including the account name, the usernames or handles you use to connect, and options such as share, reconnect, etc.\nFor each account, you can ado the following:Click an account to open a panel in which you can edit the account information. For all accounts, you can edit the name. For non-OAuth accounts (those in which you must authenticate manually), you can also edit the account credentials.Mouse over the DataSets item to see a list of DataSets powered by this account. These are categorized according to whether they are owned by you or another user. You can click a DataSet to open the Details view for that DataSet. You can also transfer ownership of the account to another user or change the account credentials for multiple DataSets.Mouse over the account to display the Options icon. Options icon", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-36", "text": "Lets you access various options for working with this account, including the following:Edit account. Lets you edit the name and credentials for this account.Share account. Lets you share user accounts with others so they can access the DataSets through the connector.Reconnect. Lets you reconnect any connectors that use oAuth for authentication. If the connector requires manual authentication, this icon does not appear.Delete account. Lets you remove an account, in which case any associated DataSets depending on the account to power up cards may stop functioning.\nFor more information about managing user accounts, see\u00a0Managing User Accounts for Connectors.\nBeast Mode Manager tab\nIn the Beast Mode Manager tab in the Data Center, you can view statistics on Beast Mode usage as well as perform various actions, such as editing formulas, locking and unlocking calculations, deleting unwanted calculations, and so on.\nThe following screenshot calls out the most important parts of the main screen (Overview) of the\u00a0Beast Mode Manager\u00a0tab:", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-37", "text": "You can learn about these components in the following table:\nNameDescriptionList of calculationsShows all of the calculations in your Domo instance you have access to see.\u00a0You can filter the list by entering search keywords in the\u00a0Search Beast Modes\u00a0field; by filtering on certain criteria such as name, owner, or status (valid or invalid); or by clicking on a card in the Dashboard.\nIf you are the calculation owner or have a grant with the \"Manage All Cards and Pages\" role enabled, you can also select calculations in this list. When you select calculations, a wrench menu appears at the top of the list. This menu contains actions you can apply to calculations in bulk, such as locking/unlocking calculations, changing calculation owners, and duplicating calculations to other DataSets.\u00a0\u00a0\u00a0\u00a0Filter buttonLets you filter the calculations in the list by any of a variety of criteria, including\u00a0 Beast Mode type (Card of DataSet), locked or unlocked status, owner, etc.DashboardShows cards for various Beast Mode-related statistics, such as the total number of Beast Mode calculations, the number saved to a DataSet, the number not being used in a Card, etc. You can click on a card to filter the calculations list accordingly; for example, clicking \"Locked\" filters the list to show all calculations that have been locked.\u00a0\u00a0Add Beast Mode buttonLets you add a calculation to a selected DataSet.\u00a0\nIf you click on a calculation in the list on the left, a details view for that calculation appears. Here you can see information specific to the calculation and access additional options. The following screenshot points out the most important parts of this view:", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-38", "text": "You can learn about these components in the following table:", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-39", "text": "NameDescriptionCalculation data typeShows the data type for\u00a0this calculation, such as String, Number, Date, etc. For more information about data types, see\u00a0Understanding Chart Data.Calculation ownerShows the owner of this calculation. By default, Beast Mode calculations on a DataSet are owned by the DataSet owner and calculations on a Card are owned by the Card owner. You can change the owner of a Beast Mode calculation by selecting\u00a0Change owner\u00a0in the options menu.Tab navigationLets you switch between the three views of the details view for this calculation.\u00a0Summary, shown in the screenshot, provides an overview of the calculation.\u00a0Cards\u00a0displays all Cards this calculation is used in.\u00a0Data\u00a0shows the calculated column together with the column(s) it is derived from.\u00a0Parent DataSetShows the parent DataSet for the calculation or, if the calculation is found on a Card rather than a DataSet, it shows the parent DataSet for the Card. You can click on the DataSet to open its details view.\u00a0Options menuProvides access to actions you can take on this calculation, including the", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-40", "text": "actions you can take on this calculation, including the following:Create Beast Mode. Lets you add another Beast Mode calculation to the parent DataSet\u00a0for this calculation.Edit formula. Opens the Beast Mode editor for the calculation so you can make changes to the formula. You can also open the editor by mousing over the formula in the\u00a0Overview\u00a0tab and clicking the pencil icon.Lock. Locks this calculation so that the formula can only be edited by the owner or an Admin-level user.\u00a0Unlock. Unlocks the calculation if it has been locked. This can only be done by the owner of an Admin-level user.\u00a0Change owner. Lets you select a new owner for the calculation. You must be the owner or an Admin-level user to do this.Buzz owner. Lets you send a Buzz message to the calculation owner. For more information about Buzz, see\u00a0Chatting in Buzz.DashboardShows you statistics for this calculation, including the number of Cards using the calculation, the total number of views of the Card, and the last update date.\u00a0LocationShows the Card or", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "897e4cf62d71-41", "text": "and the last update date.\u00a0LocationShows the Card or DataSet\u00a0this calculation is built on. You can jump to the details view for the Card or DataSet by clicking on the link.\u00a0FormulaShows the formula for this Beast Mode calculation. You can edit the calculation by mousing over it and clicking the pencil icon. For more information about editing Beast Mode, see\u00a0Transforming Data Using Beast Mode.", "source": "../../raw_kb/article/data_center_layout/index.html", "title": "Data Center Layout"}, {"objectID": "5fe6ce94eda4-0", "text": "TitleData Cleaning Operations Using SQL and Magic ETL DataFlowsArticle BodyIntro\nThis topic lists a number of data clean-up operations available through SQL and Magic ETL DataFlows and shows how data will look both before and after using an operation.\nChanging Your Data Type\nExample: Changing the data type of an ID number from a numeric to a text field\nIn MySQL...In Magic ETL...BeforeAfterCAST(`id` AS CHAR) AS `id_cast_datatype`Use the \"Set Column Type\" tile11 (looks the same but behaves a text dimension)\nConcatenating Columns to Create a Compound Field\nExample: Concatenating \"First Name\" and \"Last Name\" columns to create a \"Full Name\" column\nIn MySQL...In Magic ETL...BeforeAfterCONCAT(`first_name`, ' ', `last_name`) AS `full_name`Use the \"Combine Columns\" tile\u2018John\u2019 | \u2018Smith\u2019 \u00a0(first and last name in two separate columns)\u2018John Smith\u2019 (a single column containing the complete name)\nExtracting a Portion of a Text String\nExample: Extracting the first part of an email address to use as a user ID\nIn MySQL...In Magic ETL...BeforeAfterSUBSTRING_INDEX(`email`,'@', 1) AS `user_id`Use the \"Replace Text\" tile on that column to specify which portion of the string should be replaced with an empty string: \u00a0@.+userid@email.comuserid\nReformatting a Date\nExample: Formatting a non-standard date string into a date type format\nIn MySQL...In Magic ETL...BeforeAfterSTR_TO_DATE(`send_date`, '%d.%m.%Y') AS date_formattedUse the \"Set Column Type\" tile.23.01.2017 (string data type)01/23/2017 (date data type)\nDeriving Date Attributes from a Date Column", "source": "../../raw_kb/article/data_cleaning_operations_using_sql_and_magic_etl_dataflows/index.html", "title": "Data Cleaning Operations Using SQL and Magic ETL DataFlows"}, {"objectID": "5fe6ce94eda4-1", "text": "Deriving Date Attributes from a Date Column\nExample: Extracting the day of the week from a date column\nIn MySQL...In Magic ETL...BeforeAfterDAYNAME(`receive_date`) AS receive_date_nameUse the \"Date Operations\" tile.01/23/2017Monday\nSplitting a Column into Two Columns Based on a Character in the Column\nExample: Dividing a \"Status Code\" column into status code parts based on the / delimiter found within the column\nIn MySQL...In Magic ETL...BeforeAfterSUBSTRING_INDEX(`status_code`, '/', 1) AS status_code_p1            , SUBSTRING_INDEX(`status_code`, '/', -1) AS status_code_p2Use the \"Replace Text\" tile on that column to specify which portion of the string should be replaced with an empty string: \u00a0\\/.+ for the first part and .+\\/ for the second.SHI/DELVSHI | DELV\nTrimming Erroneous Spaces from a Column\nExample: Trimming the leading and trailing spaces from the \"Department\" column\nIn MySQL...In Magic ETL...BeforeAfterTRIM(`department`) AS department_trimmedUse a regular expression within the \"Replace Text\" tile to pinpoint the leading and training spaces and replace them with nothing.' department ''department'\nChanging the Case of an Entire Column\nExample: Changing the \"Category\" column to uppercase letters\nIn MySQL...In Magic ETL...BeforeAfterUPPER(`category`) AS category_change_caseUse the All upper case option within the \"Text Formatting\" tile.healthHEALTH\nCapitalizing the First Letter of a Column\nExample: Capitalizing the first letter of the first word in the \"Category\" column", "source": "../../raw_kb/article/data_cleaning_operations_using_sql_and_magic_etl_dataflows/index.html", "title": "Data Cleaning Operations Using SQL and Magic ETL DataFlows"}, {"objectID": "5fe6ce94eda4-2", "text": "Example: Capitalizing the first letter of the first word in the \"Category\" column\nIn MySQL...In Magic ETL...BeforeAfterCONCAT(UPPER(LEFT(`category`, 1)), SUBSTRING(`category`, 2, LENGTH(`category`))) AS category_cap_firstUse the Capitalize first letter option within the \"Text Formatting\" tile.healthHealth\nCategorizing Rows Based off the Value in a Specific Column\nExample: Assigning a region to each row based on a store number\nIn MySQL...In Magic ETL...BeforeAfter(CASE WHEN `store_number` IN ('100', '101', '104', '109') THEN 'region_1'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0WHEN `store_number` IN ('102', '105', '110') THEN 'region_2'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0WHEN `store_number` IN ('103', '106', '107', '108') THEN 'region_3'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0ELSE 'no region' END) AS regionUse the Filter Rows tile to separate into distinct groups based on store number, use Add Constants\u00a0to add a category to each group, then use Append Rows\u00a0to stitch the rows back together.store_number = 100store_number = 100 | region = \u2018region_1\u2019\nCategorizing Rows Based off the Value in a Specific Column\nExample: Assigning a category to the animal name based on the first letter of the name", "source": "../../raw_kb/article/data_cleaning_operations_using_sql_and_magic_etl_dataflows/index.html", "title": "Data Cleaning Operations Using SQL and Magic ETL DataFlows"}, {"objectID": "5fe6ce94eda4-3", "text": "Example: Assigning a category to the animal name based on the first letter of the name\nIn MySQL...In Magic ETL...BeforeAfter(CASE WHEN `animal_names` LIKE 'H%' THEN 'Hs'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0WHEN `animal_names` LIKE 'B%' THEN 'Bs'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0WHEN (`animal_names` LIKE 'A%' OR `animal_names` LIKE 'E%') THEN 'AEs'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0ELSE 'another letter' END) AS animal_names_categoriesUse a regular expression within the \"Replace Text\" tile to identify the patterns and assign a category. \u00a0For example ^H.+ finds names starting with H, ^B.+ finds names starting with B, ^A.+|^E.+ finds names starting with either A or E, and ^[^H,B,A,E].+ finds names that do not start with H, B, A, or E.animal_names =\u00a0\u2018Horse\u2019animal_names =\u00a0\u2018Horse\u2019 | animal_names_categories = \u2018Hs\u2019\nCategorizing Rows Based off a Numeric Value Threshold\nExample: Categorizing rows based on salary amount", "source": "../../raw_kb/article/data_cleaning_operations_using_sql_and_magic_etl_dataflows/index.html", "title": "Data Cleaning Operations Using SQL and Magic ETL DataFlows"}, {"objectID": "5fe6ce94eda4-4", "text": "Example: Categorizing rows based on salary amount\nIn MySQL...In Magic ETL...BeforeAfter(CASE WHEN `salary` < 50000 THEN '<$50,000'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0WHEN `salary` < 100000 THEN '$50,000 - $99,999'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0WHEN `salary` < 150000 THEN '$100,000 - $149,999'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0WHEN `salary` < 200000 THEN '$150,000 - $199,999'            \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0ELSE '>$200,000' END) AS salary_bucketUse the \"Filter Rows\" tile to separate into distinct groups based on store number, use \"Add Constants\" to add a category to each group, then use \"Append Rows\" to stitch the rows back together.salary = $45,000salary = $45,000 | salary_bucket = \u2018<$50,000\u2019", "source": "../../raw_kb/article/data_cleaning_operations_using_sql_and_magic_etl_dataflows/index.html", "title": "Data Cleaning Operations Using SQL and Magic ETL DataFlows"}, {"objectID": "e2700faaece9-0", "text": "TitleData Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline OptimizationArticle BodyRelational Database Basics\nA relational database stores data in tables (rows and columns). It makes use of \u201crelationships\u201d that exist between tables to minimize the disk space required to store that data. While Domo isn't strictly speaking a relational database management system (RDBMS), it does use relational databases in its infrastructure and many basic principles of relational databases can be used to describe and understand how Domo functions and how to work with data in Domo effectively.\nDimensions and Metrics\nA metric is often a numeric value, such as number of web ad clicks or dollar value of sales. A dimension describes the value. For example, a table of sales data might include a product SKU column. That table would store data that would allow answering business questions such as what are total sales amounts (metric) per SKU (dimension)?\n\nFigure 1 - Row, Columns, Dimensions, and Metrics.\nTypes of Tables\nOften, rather than presenting a single table with transactions and all associated dimensional information, data are presented in multiple tables, with one table containing the metrics and the other(s) containing more information about the dimensions. These various tables are related to each other by \u201ckeys\u201d (discussed below). The keys establish the relations in a \u201crelational database\u201d. There are at least two types of tables.\nTransaction\nA transaction table (also sometimes called a \"fact\" table in data warehousing terminology) contains one record for every transaction event, where a \"transaction\" is a record of some action or event such as a social media post interaction, a website click, or a record of a sale.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-1", "text": "Figure 2 - In a transaction or fact table, each record represents a transaction or event. In this table, each record represents a sale of some quantity of product.\nDimension\nA dimension table contains a single record for each dimension value and attributes further describing that dimension. For example, a dimension table might contain information about products, where each row represents a single SKU and the various columns in the table describe the SKU. Such attributes might include product name and unit price.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-2", "text": "Figure 3 - A dimension table contains one record for each value for a type of dimension. In this case, there is one record for each SKU with additional information - such as description and unit price - that describes the dimension.\nTypes of Data\nThe data type of a column will govern what data can be stored in the column and which operations are allowed on the column. Data type also affects how efficiently some operations can be executed on the column. For example, joins, filters, and case logic on numeric columns are faster than equivalent operations on text columns. There are many different data types, and the fine details of each vary from one database system to another. However, these data types can be considered in just a few groups:\nStrings\u00a0(also known as text or \u201ccharacter\u201d) columns can store letters, numbers, and other characters. While strings can store numeric data, with just a few exceptions (i.e. US zip codes, which can contain leading zeros), it is best to store numeric data in numeric-typed columns.Numbers\u00a0(integers; fixed- and floating-point decimals; others) can store only numeric data.Dates and times\nThere are certainly other types of data but these three groups are by far the most common.\nNULL: No Value", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-3", "text": "NULL: No Value\nEach cell in a table can contain a value or no value at all. When a cell contains no value, it is said to be null. It is important to understand what null means and how databases and Domo treat null. First, null is NOT zero and it is NOT an empty string. Zero and the empty string are both values, whereas null is no value at all. Null serves a very specific purpose: it is how the lack of any information/value is represented. Because null is no value at all, it is impossible for null to equal anything else, including other nulls. Therefore, when logic is to be applied to a column that could contain null (and it is safest to assume all fields could contain null), that logic must account for or \u201chandle\u201d nulls. This is especially true in logic where two columns are compared with one another, where a column is compared to a constant value, or when performing mathematical calculations. Examples include CASE logic, filters, and join conditions.\nGranularity and Uniqueness\nTo prevent misuse or misinterpretation, it is important to understand the granularity of the data in a table. Granularity describes the \"level\" at which data are presented. For example, a table of employees is granular at the employee level meaning each record represents a single employee. That table may contain many attributes of the employee, such as employee ID, name, and department. A table of transactions is granular at the transaction level. That is, each record represents a transaction and the values in the various columns describe the transaction.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-4", "text": "Closely related is the idea of \"uniqueness\". Uniqueness is an indication of the number of distinct values in a column relative to the total number of rows in the table. In the employee table, Employee ID is expected to be fully unique (each employee has an Employee ID and no two employees share the same Employee ID); the employee name might be highly unique but not fully unique because in a large organization, it is likely multiple employees share the same name; and department might be highly non-unique (because many people are assigned to each department).", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-5", "text": "Figure 4 - In the table of employees, which is granular at the Employee level (meaning there is a single row per Employee), the Employee ID column alone uniquely identifies each row.\nOften, a table will include a single ID column that uniquely identifies each row in the table. Such is the case with the table of employees: Each employee has an Employee ID and no two employees share the same Employee ID. But tables don\u2019t always contain explicit ID columns, nor is uniqueness always defined by a single column. In the sales transaction table for example, there is no \u201cTransaction ID\u201d field, and three columns must be considered together to establish uniqueness: PO Number, Customer, and Product SKU. (A single purchase order may include multiple SKUs, and since PO number comes from the customer, it is possible that two customers could submit the same PO number. Therefore, only the combination of PO Number, Customer, and Product SKU establishes uniqueness.)", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-6", "text": "Figure 5 - In the sales transactions table, which has no single column that uniquely identifies each row, a combination of PO Number, Customer, and Product SKU is required to uniquely identify each row.\nData Relationships\nOften, data from multiple tables will need to be combined to answer a particular business question. This is especially true when the source data comes from a data warehouse, which will often make use of a \u201cstar schema\u201d data architecture where a central transaction (fact) table references one or many other peripheral dimension tables that can be used to further describe each transaction. For example, the table of sales transactions lists the date of the sale, the quantity sold, the dollar amount of the sale, the SKU of the item sold, and the employee ID of the sales rep who performed the sale. Separately, the table of products contains SKUs with the associated product names and categories. To determine the dollar value of sales within each product category, the sales transaction table and product table will need to be referenced together making use of the key columns that define the relationship between the tables.\nCardinality\nCardinality is closely related to the concepts of granularity and uniqueness described above. Cardinality describes the nature of the relationships between records in two tables. In the example above, there is a many-to-one relationship between the sales transaction table and the product table: Each SKU may be represented in the sales transactions data zero, one, or many times; and the sales transaction table is related to the product table where each SKU is presented exactly once.\nPrimary and Foreign Keys", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-7", "text": "Primary and Foreign Keys\nA column (or set of columns) that uniquely identifies a row of data is called a primary key. Thus, a primary key is fully unique. Primary key columns can\u2019t contain NULL values. A primary key in one table can be referenced by a foreign key in another table. Put another way, the relationship between two tables is described by their primary and foreign keys. In the sales transactions example above, the Product SKU column is the primary key in the product and it is referenced by the Product SKU column in the sales transactions table, in which it is a foreign key.\nIn relational database management systems, primary-foreign key relationships can be codified such that the database will enforce \u201creferential integrity\u201d by requiring that primary keys be unique and by preventing deletes of records in dimension tables that are referenced by foreign keys in transaction tables. As stated, Domo is not a relational database management system, so primary and foreign key constraints aren\u2019t enforced. Still, to effectively use Domo, it is important to understand these concepts.\nEntity relationship diagrams\nData tables and relationships between tables are often represented using \u201centity relationship diagrams\u201d (ERD). The lines connecting two tables indicate the existence of a relationship; the terminals on the lines represent the cardinality of the relationship. In the sales data example, there are many-to-one relationships between the sales transaction table and the employee table; and between the sales transaction table and the product table. These relationships can also be interpreted in natural language: A single employee can complete a sale 0, 1, or many times; a particular SKU could be sold 0, 1, or many times.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-8", "text": "Figure 6 - An entity relationship diagram can be used to represent the nature of relationships between data tables.\nJoins\nA join makes use of the relationships between tables. It is a command that tells the database how to combine two tables side-by-side. The \u201cside-by-side\u201d nature of the join is hinted at by the names of various join types: \u201cleft outer\u201d and \u201cright outer\u201d, for example.\nThe various types of joins are differentiated from each other in terms of the way they handle matched and unmatched rows between the two tables being joined.\nInner join: returns only those records that meet the join condition.Left outer join: returns all records from the left table with records joined from the right table whenever the join condition is met.Right outer join: the mirror image of left outer join \u2013 returns all records from the right table with records joined from the left table whenever the join condition is met.Full outer join: returns all records from the left and right tables with records joined whenever the join condition is met.\nIn the diagram below each join type is presented as a Venn diagram where the two circles represent the rows in the two tables being joined. The left portion of the left circle represents records in the left table for which there are no right-table records that meet the join condition; the right portion of the right circle represents records in the right table for which there are no left-table records that meet the join condition; and the overlapping portion represents those left- and right-table records that correspond with each other based on the join condition. The shaded regions represent those records that are returned by the join.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-9", "text": "Figure 7 - Join types are presented as Venn diagrams. The circles represent the two tables participating in the join. The shaded regions represent those records that are returned by the join.\nGranularity and Uniqueness \u2013 Common Pitfalls\nIt is important to establish a good understanding of the granularity and uniqueness of various tables and the relationships between them. Otherwise, data may be joined or used in ways that are misleading or entirely incorrect. There are two common pitfalls that both result from misunderstanding or misuse of the underlying data.\nCartesian Joins\nIn mathematics, a \u201ccartesian product\u201d is all possible combinations of elements of two sets. The cartesian product of sets {A, B} and {C, D} would be {AC, AD, BC, BD}. The same concept appears when joining two data tables: without a good understanding of the granularity and uniqueness of the tables, and without specifying the appropriate join conditions, a cartesian join may result. In a cartesian join, records from one or both tables will be duplicated, resulting in inaccurate results. Row count increases and duplication of records \u2013 a column or columns that were previously unique identifiers are no longer after the join \u2013 are indicators of a cartesian join. There are specific use cases where a cartesian join is desired and expected, but those use cases are the small exception.\nTo prevent a cartesian join, the join should be defined such that the join conditions uniquely identify a row. Typically, this is accomplished by using the unique identifier column(s) of the joining table in the join condition. Of course, this requires knowing which column(s) constitute the unique identifier. In circumstances where uniqueness isn\u2019t guaranteed (e.g., in the absence of a system generated unique ID, and especially when dealing with manually-maintained data), best practice suggests enforcing uniqueness.\nTables Containing Mixed Attribution or Level of Granularity", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-10", "text": "Tables Containing Mixed Attribution or Level of Granularity\nTypically, a table contains records of distinct instances of some type or class of entity. For example, the sales transaction table contains one record for each sales transaction; the product table contains one record for each SKU; etc. And each instance of each entity is defined by its various dimensions or attributes. Thus, each data dimension can be thought of as an attribute of some thing.\nThe organization of data in a star schema usually makes it easy to identify which dimensions are attributes of which entities; or, which dimensions apply at which levels of granularity. For example, the unit price is an attribute of a product, identified by SKU in the product table; and salary might be an attribute of an employee, identified by employee ID in the employees table. However, when data from multiple tables are combined via a join, it can be difficult to understand which attributes apply to which class. In the employee table, it is probably acceptable to treat salary as a metric that can be summed (i.e., to calculate total salary overhead) or averaged (i.e., to calculate average salary per seniority band). However, when the employee data is joined to the sales transaction table, any attempt to consider salary as a metric would yield incorrect results. That\u2019s because salary is an attribute of employee and granular at the level of one record per employee; but the sales table is granular at the level of one record per sales transaction.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-11", "text": "Best practice suggests not mixing attributes from different classes or levels of granularity. Thus, instead of joining the salary data to the sales data and then trying to answer questions about salary from that combined data, salary should be kept separate from the sales data and questions about salary should rely upon the employee data directly. If this isn\u2019t possible, consider adopting a naming convention that makes clear the class or level of granularity of a column. In the example of sales and salary, \u201cEmployee Salary\u201d might be a more appropriate and helpful column name than merely \u201cSalary\u201d. Salary is universally understood to be an attribute of an employee so there is low risk of misinterpretation, but that isn\u2019t always the case. For example, perhaps the region of a sale can be attributed in two different ways: 1) based on the customer\u2019s headquarters location; 2) based on the geographical hierarchy of the sales organization. In that case, appropriate column names might be \u201cCustomer Region\u201d and \u201cSales Geography Region\u201d.\nInvestigating and Enforcing Uniqueness\nWhile Domo isn't a relational database management system, basic principles of relational databases are in play. For example, even though Domo doesn't allow defining a primary key on a DataSet, it is important to understand the granularity and uniqueness of data, especially when joining or aggregating. There are a few different ways to check for and/or ensure the uniqueness of data.\nAnalyzer and Views", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-12", "text": "Analyzer and Views\nIf a column (or group of columns) uniquely identifies the rows in a table, then there will be exactly one row represented by each of the values in the column (or each permutation of values across the columns). Uniqueness can be explored or validated by configuring a table card in Analyzer or using Views Explorer. To check for uniqueness, aggregate the data to the level of the presumed unique key and count the rows. If the column(s) is in fact a unique key in the table, the row counts will all be exactly 1; if not, the column(s) is not a unique key.\nIn the sales transactions example, neither PO Number, Customer, nor Product SKU is a unique identifier (and therefore not a primary key) in the table. This is clear because there are examples of PO Number, Customer, and Product SKU where there are multiple records (row count greater than 1). However, combined, the three columns form a unique identifier.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-13", "text": "Figure 8 - Neither PO Number, Customer, nor Product SKU is a unique identifier (because each has at least one instance where there is more than 1 record identified by a particular value. However, the combination of the three fields does form a unique identifier.\nOf course, if a field (or combination of fields) should be a unique identifier but isn\u2019t, then ideally the duplication issue would be corrected upstream in the source system. However, if that isn't possible for whatever reason, uniqueness can be enforced using Domo Views or ETL processes. That is done by eliminating duplicate rows. In this context, \u201cduplicate\u201d means two or more records that share the same values in the field or fields that should constitute the primary key. The important distinction is that \u201cduplicate\u201d records may not be exact duplicates across all columns.\nAggregation\nDuplicate records can be removed via aggregation. In this approach, the data is aggregated up to the level of the column(s) in which uniqueness is to be enforced. Additional attributes/dimensions can be included as well but that must be done cautiously: sometimes, adding dimensions to the aggregation will result in the non-uniqueness in the column(s) in which uniqueness should be enforced. Metric/value columns can be aggregated. Be careful when choosing the aggregation method (i.e. sum, average, min, etc.). After the aggregation, the data is unique at the level the aggregation was performed.\nRemoving Duplicates\nRather than aggregating, it is also possible to remove duplicate rows. Magic ETL provides a \u201cRemove Duplicates\u201d tile. To use that tile, specify the fields that should constitute the unique key. Domo removes duplicates by retaining only one record for each unique key value and filtering out all others. However, using this tile is often not recommended because it doesn't give control over which record is retained and which are filtered out.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-14", "text": "Rather than using the Remove Duplicates tile, the preferred method involves \u201cpartitioned\u201d row numbering and filtering. In this method, the rows are numbered, partitioned by the unique key field(s), and then filtered such that only the one record with row number 1 is retained and all others are filtered out. By specifying the sort order in this approach, more logic can be implemented to select which one record is retained and which records are filtered out. This method can be accomplished in Magic ETL using the \u201cRank & Window\u201d tile or in Redshift SQL using the row_number() over(partition by [] order by []) function. MySQL in Domo doesn't support an equivalent row number function; however, the same result can be achieved using variables, though the syntax is more complex.\nDomo Architecture\nTo understand how to optimize data processing in Domo, you must have a basic understanding of Domo's architecture. Domo stores data in two places: Vault, the Domo data warehouse; and Adrenaline, the high-performance Card-rendering engine. The Adrenaline engine is used for supporting Card-rendering by responding to the queries that are executed each time a Card is loaded. Data processes (extract/transform/load processes, or \u201cETL\u201d for short) run in separate data processing environments. Exceptions are Adrenaline Dataflows and Views, which are both executed on the Adrenaline engine. When a data process begins, data must be loaded from Vault into the ETL engine: Magic ETL, MySQL, or Redshift. Then, the data transformations specified in the ETL process are executed. Finally, the output data is written to Vault and then written to Adrenaline and indexed.\nData Pipeline Optimization", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-15", "text": "Data Pipeline Optimization\nOftentimes, especially with ETL processes consuming very large input data, the initial input and the final output step account for the majority of the total ETL processing time. Thus, any methods that effectively reduce the size of the input and/or output data (either rows or columns) will reduce total processing time. This is the over-arching guiding principal: reduce the total size of the data to be processed as soon (i.e. as far upstream) as possible. There are several general guidelines that if followed will produce robust, efficient, performant data pipelines. These guidelines apply to ETL in general, both inside and outside of Domo, but this document focuses on how these guidelines can be implemented in Domo. These principles apply at the macro level (several data processes in a complete pipeline) and at the micro level (various transformations within a single DataFlow).\nMinimize number of times a record is processed\nThe idea here is to process a record as few times as possible. If a record might be updated in the source data (i.e. the phone number of a customer, stored in a contacts dimension table; or the status of an order) then it is necessary to re-process any changed records if/when they change. However, if the data doesn't change over time (that is, new records may be added but existing records will never be modified) it isn't necessary to re-process historical records. There are several methods that follow this general guideline:\nPartitioning\nPartitioning will be discussed in detail later. The idea is to \"subset\" or \"partition\" the data and then only process those partitions that need to be processed. Partitioning can be accomplished using some Domo Connectors, the Domo command line interface (CLI), or by scripting against Domo APIs (directly or using Domo\u2019s Java or Python SDKs).\nFiltered DataFlow inputs using Views", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-16", "text": "Filtered DataFlow inputs using Views\nIn this method, a View is used to limit the number of records that must be run through an ETL process. For example, if a DataSet contains a lot of historical data but the end use only requires recent data (i.e. rolling 13 months or similar), a Dynamic Date Filter can be implemented in a View such that the View returns only the rolling 13 month data, and thus only that data is processed through the ETL.\nEnhanced data processing in the Magic ETL v2 engine\nThe Magic ETL v2 engine is smart enough to know which records are new or have been updated since the last time a DataFlow ran. Thus, it will process only the new or changed data where possible. Some functions within Magic ETL v2 (e.g. aggregations, windowed functions, etc.) require all input data to be processed.\nMinimize the number of rows processed\nThis can be done using filters and/or aggregations. The extent to which a DataSet can be filtered or aggregated is defined by the end use case: obviously, if the final output requires a particular scope of data or a particular level of granularity, then that scope and granularity must be persisted from the source to the final output DataSet. However, oftentimes raw data provides much more granularity than is necessary. For example, does the final output really need to see every web click of every link? Or is it sufficient to simply know the number of clicks per user or per some other dimension(s)?\nMinimize the number of columns processed", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-17", "text": "Minimize the number of columns processed\nBy default when a DataSet passes into an ETL process, all the columns are copied from Vault into the ETL engine, whether those columns are actually necessary in the process or not. MySQL DataFlows can be configured to select only specific columns to be copied into the ETL engine. This isn\u2019t possible with Magic ETL, but Views can be used to subset only the necessary columns and then that view output can be read into a Magic ETL v2 process as an input.\nAvoid single-threading through a single DataFlow or DataSet\nDomo users sometimes design their data environments with too much focus on creating a single pipeline or DataSet to support all (or too many) use cases. There are at least two reasons such designs may not be optimal: first, they increase the number of users and Cards reliant upon a DataSet, and in extreme cases that can lead to Card- and Page-rendering performance problems; second, such designs typically result in more complex processes and larger DataSets, both of which can result in excessive processing times. Rather than creating a single data pipeline and a single output DataSet \u201cto rule them all\u201d, consider creating processes and output DataSets that are designed for specific end use cases. This approach will generally allow a greater degree of row- and column-reduction. It also allows data processing to be conducted in parallel rather than in series. Such architectures are also optimized for Card-rendering performance, which is especially important with very large DataSets and/or large numbers of concurrent users.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-18", "text": "Of course, this must be balanced with other important ideas such as avoiding replication of logic or data. In general, any two use cases probably have some data, dimensionality, and logic in common; and other data, dimensionality, and logic that are distinct between them. Data processes should be created such that common concepts and logic are \u201ccollocated\u201d in processes that support both use cases; but as the two use cases diverge in terms of their definitions or data requirements, they should also be allowed to diverge in terms of their supporting data processes, too. This suggests architectures in which data standardization and clean-up, as well as some look-ups and other operations, take place in a semantic middle layer designed to support multiple end use cases, followed by a use case-specific layer in which processes diverge to support the requirements of the various end use cases. This concept can be applied at the macro-level (across multiple DataFlows and DataSets) or the micro level (within a single DataFlow).", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-19", "text": "One common example of this balanced approach can be seen in the need for aggregated historical data and detailed recent data. For example, perhaps the business requires a trailing 13-month view of sales volume as well as the customer, product, and rep details for transactions in the current month. Rather than a single DataSet containing 13 months\u2019 worth of data at the detail level, this business need can be met with two distinct DataSets: an aggregated history DataSet, which contains 13 months\u2019 worth of aggregated data by region; and a detailed current month DataSet, which contains detailed data for a single month. As illustrated in the diagram below, depending on the scope and dimensionality of the raw data, the two combined output DataSets could represent an 85 to 90% total row count reduction. (In terms of visualization, Domo\u2019s drill-down functionality allows drilling down from one DataSet into another. End users don\u2019t need to juggle or even be aware of the presence of two DataSets.)", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-20", "text": "Figure 9 - By allowing data processes to diverge when data, dimensionality, and logic requirements diverge, pipelines can be performance-tuned to support the specific end use cases.\nOther tips\nStrike a balance between joining data too far upstream or downstream\nThere is a balance to be struck between joining upstream, which may enable upstream aggregation but also increase up-stream column counts; and joining downstream, which may result in retaining more granularity (and therefore more rows) further down the data pipeline. Perform joins upstream whenever significant aggregation can be accomplished upstream as a result.\nJoin Using Numbers Rather Than Strings\nWhenever possible, perform joins using numeric columns rather than string columns. Joins on numbers, especially integers, are faster than joins on strings.\nDefine indexes\nDatabases use indexes just like a person might use the index in a textbook: to quickly find information. Indexes are essential for data processes to run efficiently, especially operations such as joins, aggregations, and filters. In Magic ETL and Redshift, the database engine effectively creates indexes automatically. However, in MySQL, indexes must be created explicitly within the DataFlow. Missing indexes can have a huge negative impact on data processing time.\nData Accumulation\nThere are many reasons it may be necessary to accumulate data in Domo. For example, perhaps a particular source system stores only a rolling 30-day window of data but the use case requires more historical data. Or perhaps a particular data pipeline is complex and requires significant processing time and therefore it is preferred to accumulate the processed data downstream rather than processing all the raw historical data each time new data arrives. There are a few different ways data can be accumulated in Domo.\nData Accumulation Methods\nAppend", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-21", "text": "Data Accumulation Methods\nAppend\nThe simplest method of accumulating data is to \"append\" new incoming data to existing historical data. This can be done in most Connectors, via Workbench, using the CLI utility, or by scripting against the Domo APIs (directly or via Domo\u2019s Java or Python SDKs). While appending is simple, it is also frequently not recommended. If a data ingestion step runs twice, for example, duplicate data may be stored in Domo. Or if a data ingestion step fails to run on schedule, it may result in gaps in the data. Avoid using Append unless there is a specific reason to use it. The other methods described below are almost always preferred.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-22", "text": "Figure 10 - Using append, new incoming records are simply added to the existing records.\nUpsert\nThe term \"Upsert\" comes from the combination of \"update\" and \"insert\". It is a method that allows inserting new records and updating any existing records as necessary. Upserting is a relatively computationally intensive process because it requires comparing all incoming records with all existing records. End-to-end, it is still more efficient than a recursive DataFlow. Some Connectors now support upsert, as does Workbench.\n\nFigure 11 - Using upsert, new incoming records are checked against existing records. If a record exists, it is updated from the new incoming data; if a record doesn't exist, it is inserted.\nOther resources:\nUsing Upsert in Workbench 5.1\nPartitioning\nTo accumulate data by partitioning, the data is subsetted using a partition key (dates are often ideal partition keys for transactional data). Then, any time new data is to be ingested, only the affected partitions need to be modified by deleting the existing data in the partition and inserting the new data for the partition. Partitioning is a performant method for data accumulation because it follows the general guideline of processing data no more often than necessary. Partitioning is supported by some connectors, Workbench, the CLI utility, and by Domo's APIs.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-23", "text": "Figure 12 - Using partitioning, data are subsetted according to the partition key. When new incoming data is processed for a particular partition, if that partition already exists in the accumulated data, the existing data is replaced by the new incoming data; if a partition doesn\u2019t exist, it is added.\nOther resources:\nPartition ConnectorsAbout Partition ConnectorsWorkbench 5.1 Partition Support\nRecursive DataFlow\nAppending, upserting, and partitioning are supported in various parts of the Domo product such as Connectors, Workbench, and the Domo APIs. Depending on how the recursive DataFlow is configured, it can yield results that are effectively the same as the results of appending, upserting, or partitioning. The benefit of using a recursive DataFlow is that the logic can be fine-tuned to the particular use case. However, recursive DataFlows are costly to run because, by definition, their output data grows continually over time and because they require re-processing all historical data each time new incoming data is processed. Still, recursive DataFlows can be an important part of a pipeline solution as long as the total data size and growth rate remain relatively small.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-24", "text": "Figure 13 - Recursive DataFlows can be used to produce DataSets that are functionally equivalent to those created by append, upsert, or partition operations. Recursive DataFlows are often not recommended because they require reprocessing existing records.\nOther resources:\nCreating a Recursive/Snapshot Magic ETL DataFlowCreating a Recursive/Snapshot DataFlow in Magic ETL v2Creating a Recursive/Snapshot SQL DataFlow\nAccumulating Raw Data at Point of Ingestion\nDepending on the ingestion method, accumulating data by appending, upserting, or partitioning may all be supported. In addition, recursive DataFlows can be created immediately downstream from raw source data to effectively accumulate at the point of ingestion. However, accumulating data upstream violates the general guideline that records should be processed as few times as possible (that\u2019s because any downstream DataFlows would likely be operating on the full accumulated DataSets). The guideline to process data as few times as possible suggests accumulating data downstream, and that will often yield a more performant data pipeline.\nAccumulating Data \u201cMid-stream\u201d\nWhile all accumulation methods may be applicable at the point of ingestion, Domo currently doesn\u2019t support mid-stream accumulation natively. Data accumulation can still be accomplished downstream, but it requires a bit more creativity. Recursive DataFlows are one possibility. Another option is to extract data from Domo (i.e. by scripting using the Domo CLI or by scripting against Domo\u2019s APIs) and then re-upload to Domo using partitioning or upsert. The re-upload step could be accomplished using CLI commands or Workbench).\nPicking the Appropriate Method", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-25", "text": "Picking the Appropriate Method\nAn append process is simple and performant but it isn\u2019t robust to process upsets. Any of the other methods \u2013 upsert, partition, or recursive DataFlow \u2013 are more robust, but are also more complex and may not be quite as performant. Robust processes yield accurate data and therefore upsert, partition, and recursive DataFlow are all typically preferred over append. Recursive DataFlows can be an important part of a data pipeline as long as total data size and growth rate are small. Recursive DataFlows will take longer and longer to run as record counts increase and ultimately, they may not be performant.\nUpsert vs Partition\nThe decision between partition and upsert comes down to a question of data granularity: If the data to be accumulated provides a good candidate partition key and if the process can be managed such that only whole partitions are processed, then partitioning is likely the preferred solution. A good partition key is one that creates tens to thousands of partitions containing tens to hundreds of thousands of records each. It is important only whole partitions be processed. That\u2019s because if an input DataSet contains a partial partition, the partial partition will completely replace whatever data exists in that partition in the output DataSet.\nIf optimal conditions for partitioning don\u2019t exist, and if the data has a unique key, upserting is recommended. In particular, upserting is preferred when the data to be accumulated doesn\u2019t contain a good candidate partition key. For example, SaleDate may be a good partition key but SaleTimestamp may not be. That\u2019s because a timestamp is far more granular than a date and would yield far more, far smaller partitions. Since upserting is a row-level operation (upserting updates a record if it already exists and inserts it if not) and a unique key is required.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "e2700faaece9-26", "text": "Note: When partitioning in Workbench, if a timestamp is selected as the partition key, Workbench will automatically use only the date portion of the timestamp for the partition key. Thus, timestamps may be appropriate partition keys in Workbech.", "source": "../../raw_kb/article/data_fundamentals_understanding_relational_data_domo_architecture_and_data_pipeline_optimization/index.html", "title": "Data Fundamentals: Understanding Relational Data, Domo Architecture, and Data Pipeline Optimization"}, {"objectID": "47c72766f457-0", "text": "TitleData Processing and Tools Best PracticesArticle BodyIntro\nThere are multiple ways to bring data into Domo and process or transform that data once it has been brought in. There are three stages in which you can choose to process your data: \nPre-Domo Processing Options: You\u2019ll generally use Workbench and Domo connectors to bring data into Domo, and you can perform some minimal transformations within Workbench and certain connectors before you upload the data into Domo.In-Domo Processing Options: After you\u2019ve brought your data into Domo, you\u2019ll generally use Magic Transforms (ETL, MySQL, Redshift, or DataFusion) to create and transform new DataSets within Domo. In-Card Processing Options: After you\u2019ve created DataSets and started to build cards with them, you can use Beast Mode within Analyzer to add any additional dimensions or calculations that you need. \u00a0\nEach tool has its own unique affordances as well as potential advantages or disadvantages depending on the type and size of data that you need to process and how quickly you want that data to be able to refresh or update. Review the following sections to identify which tool will work best for your current or future data processing needs. \nPre-Domo Processing Options\nConnectors\nWhen to Use Connectors\nDomo has developed hundreds of proprietary public-facing connectors that make it easy for you to bring in data from a host of popular applications. With Domo\u2019s developer kit, you\u2019re also able to build connectors of your own. \nDepending on the specific connector, you may have different options in terms of tailoring the reports or DataSets you would like to bring into Domo. Visit the documentation for each individual connector in the Help Center to learn more about what each connector offers you in terms of processing and transformation capability. \nPro Tips", "source": "../../raw_kb/article/data_processing_and_tools_best_practices/index.html", "title": "Data Processing and Tools Best Practices"}, {"objectID": "47c72766f457-1", "text": "Pro Tips\nTo the extent possible, preprocess your data before you bring your data into Domo using connectors, which will help minimize the need for creating DataFlows later and ultimately reduce processing time. If possible, it is better to remove or filter out columns or categories of data that don\u2019t need to be included in the DataSet to minimize load times and simplify later use of that DataSet.\nWorkbench\nWorkbench is a secure, client-side solution for routinely uploading your on-premise data to Domo. To learn more about Workbench, visit http://knowledge.domo.com?cid=workbench.\nWhen to Use Workbench\nYou are trying to automate data import into Domo from local files or internally managed databases or servers (CSV, Excel, Jira, ODBC, OLAP, Quickbooks, XML).You are trying to do basic data manipulations (lookup tables, simple calculations, change data formats, change data types, search/replace, rename column headers) before importing transformations into Domo.You want to utilize existing SQL queries to grab data from a database (uses an ODBC connection).You are trying to get data from a data warehouse cube (can only grab one slice at a time).You have pre-existing scripts (ex: Powershell, etc.) that are outside the default list of connections.You are willing to build custom plugins into Workbench if you cannot convert your data or systems into a generic database (ODBC, OLAP) or generic file type ( Excel, CSV, JSON, XML) or find matching connectors for your data systems/type.\nWhen NOT to Use Workbench\nYou are trying to do complex formulas (unless contained within a SQL query).You are not willing to build custom plugins in Workbench if you cannot convert your data or systems into a generic database (ODBC, OLAP) or generic file type (Excel, CSV, JSON, XML) or find matching connectors for your data systems/type. \nPro Tips", "source": "../../raw_kb/article/data_processing_and_tools_best_practices/index.html", "title": "Data Processing and Tools Best Practices"}, {"objectID": "47c72766f457-2", "text": "Pro Tips\nTo the extent possible, preprocess your data before bringing it into Workbench or use Workbench to process data, which will help minimize the need for creating DataFlows later and ultimately reduce processing time. If possible, it is better to remove or filter out columns or categories of data that don\u2019t need to be included in the DataSet to minimize load times and simplify later use of that DataSet.\nIn-Domo Processing Options \nETL\nYou can create ETL DataFlows using an intuitive drag-and-drop interface available in the Data Center of Domo. To learn more about ETL, visit http://knowledge.domo.com?cid=createmagic.\nWhen to Use ETL\nIf you aren\u2019t familiar with SQL-based languages and are more of a business user vs. technical/power user.If you want the DataSet to be added to or managed by someone who is unfamiliar with SQL-based languages.You need to combine dissimilar DataSets or normalize your data easily.You prefer to see a visual of how you want to make the DataFlow.If your inputs total less than 100 million rows.\nWhen NOT to Use ETL\nYour inputs total over 100 million rows.You need to utilize things such as current date or custom functions, statistical functions, windowing functions (ex: row numbers), and current date. \nPro Tip\nIt is better to remove or filter out columns or categories of data that don\u2019t need to be included in the DataFlow at the beginning of the process instead of at the end. For example, if you know that you will only need data for year 2016, filter the 2016 data out from the very beginning. \nDataFusion", "source": "../../raw_kb/article/data_processing_and_tools_best_practices/index.html", "title": "Data Processing and Tools Best Practices"}, {"objectID": "47c72766f457-3", "text": "DataFusion\nDataFusion lets you select and merge data sources so you can begin visualizing your data. DataFusion makes joining data simple, and there\u2019s no need for SQL or ETL expertise. To learn more about using DataFusion, visit http://knowledge.domo.com?cid=usingdatafusion.\nWhen to Use DataFusion\nYou have DataSets that you need to be instantaneously joined or unioned and updated often, and you need to rename columns or drop columns; it is the quickest method for joining inputs of 100+ million rows or larger.Because a Data Fusion is not processing or indexing the data, it renders more quickly inside of Domo.You want to be able to leverage creative Beast Mode calculations within Analyzer to specify which data you want to display and how. You don\u2019t need to manipulate data while joining.You are not familiar with writing SQL.Your DataSets are larger than 100 million rows.\nWhen NOT to Use DataFusion\nYour joins aren\u2019t straight-forward (ex: functions on join clauses, where clauses needed, order by needed, calculations needed).You need to use the DataFusion as an input to a DataFlow.You need a lot of back-end control over renaming or editing column names after the initial creation of the DataFusion.\nMySQL\nMySQL DataFlows allow for the use of common SQL commands so that data can be accessed, joined, cleansed, and transformed within Domo. To learn more about adding a SQL DataFlow in Domo, visit http://knowledge.domo.com?cid=createsqldf.\nWhen to Use MySQL", "source": "../../raw_kb/article/data_processing_and_tools_best_practices/index.html", "title": "Data Processing and Tools Best Practices"}, {"objectID": "47c72766f457-4", "text": "When to Use MySQL \nIf you are familiar with SQL-based languages or it is your current preference. You are familiar with optimization techniques and know how to create tailored indexes; for example, using indexes can reduce an 8-hour processing time to about 30 minutes. If you want to optimize processing; MySQL can often exceed the processing efficiency of Redshift if the calculation is optimized properly.You need to use custom functions, stored procedures, or want to have full control over indexing.When you can\u2019t get the desired result using Magic ETL, Beast Mode, or Fusion.\nWhen NOT to Use MySQL\nYour input DataSets are over 100 million rows.If you are dealing with DataSets that hold mass amounts of columns and/or rows that are being combined into a DataFlow (that also aren\u2019t going to be grouped down), it will take significantly longer to update these DataSets and cards built on those DataSets in Domo. It can sometimes take hours for these assets to update.If you plan to have multiple nested queries (performance is poor on these unless you pull the nested queries into their own transform, then index and reference them).You don\u2019t know how to optimize or index data.If others who will be maintaining, modifying, or needing to understand the DataFlow in the future don\u2019t prefer to use MySQL.\nPro Tip\nDocumentation on MySQL is readily available online, so if you run into issues, you\u2019ll be able to find the resources you need there. \nAmazon Redshift\nAmazon Redshift is a hosted data warehouse project that is part of the larger cloud computing platform Amazon Web Services. Redshift handles analytics workloads on large scale DataSets stored by a column-oriented DBMS principle. To learn more about using Redshift, visit http://knowledge.domo.com?cid=amazonredshift.\nWhen to Use Redshift", "source": "../../raw_kb/article/data_processing_and_tools_best_practices/index.html", "title": "Data Processing and Tools Best Practices"}, {"objectID": "47c72766f457-5", "text": "When to Use Redshift\nMost of your DataSets are 10 million rows or more.You have a DataSet of 100 million or more rows as an input.You need windowed functions (windowing functionality is available through MySQL as well, but is not built in natively).Customer is a heavy Redshift shop who wants to work with technology they\u2019re already familiar with so they can leverage existing expertise and easily copy/paste queries from their other systems into Domo without having to translate to MySQL, Magic, etc.In order to leverage Multiple Parallel Processing when intentionally creating massive Cartesians or working with any other joins between extremely large DataSets.\nWhen NOT to Use Redshift\nThe majority of your input DataSets are under 1 million rows.You need custom functions, indexes or stored procedures.Using sub-selects in your SQL (different than sub-queries). Performance is greatly reduced. See reference below:SUBSELECT (AVOID): select *, (select a from table) as content from tableSUBQUERY (OK TO USE): select * from (select a from table)When any other methods appropriately and adequately provide the solution.If others who will be maintaining, modifying, or needing to understand the DataFlow in the future don\u2019t prefer to use Redshift.\nPro Tip\nDocumentation on Redshift is readily available online, so if you run into issues, you\u2019ll be able to find the resources you need there. \nIn-Card Processing Options\nBeast Mode\nIn the Analyzer, you can add a calculation, or create column transforms, by defining calculations that reference other columns in your DataSet. To learn more about Beast Mode, visit http://knowledge.domo.com?cid=beastmodecalculation.\nWhen to Use Beast Mode", "source": "../../raw_kb/article/data_processing_and_tools_best_practices/index.html", "title": "Data Processing and Tools Best Practices"}, {"objectID": "47c72766f457-6", "text": "When to Use Beast Mode\nAnytime you need to create an instantaneous calculation or calculated column or you want your calculations to be dynamic (recalculates on the fly when using filters and analyzer), which is especially the case when you want to do a SUM, MIN, MAX, or AVG calculation.You want to split up your data into different groupings (ex: split age column into multiple series like 0-10, 11-30, 31+; set \u201cbelow goal\u201d and \u201cabove goal,\" etc.).You want to create comparison calculationss (ex: Year over Year, Period over Period) or create ratios between groups of data.You want to create multiple summaries or manipulate table cards using basic HTML.You want to optimize the time it takes for a query to complete. Beast Modes offer this advantage because these are not saved in the DataSet itself. \nWhen NOT to Use Beast Mode\nYou are trying to compare data from one row to another (other than using SUM, MIN, MAX, AVG).You need advanced statistical functions (some statistical functions exist through Beast Mode, so check the Functions list to see if the one you need is available).You want variables.You need a calculated date that can be recognized/used in a card\u2019s dynamic date filtering (use a DataFlow instead).\nPro Tip\nIf you need to apply a calculation to a DataSet, it is generally best to use Beast Mode, as it adds dynamic filtering capabilities to your calculation. Fusion can be used to quickly join large, historical pre-processed DataSets with the latest versions and make turnaround processing time much faster because you won\u2019t need to perform additional processing and indexing on the pre-processed DataSet each time.\nHelp Option \u2014 Domo ETL Team\nThis is a special team of data specialists that develop solutions when Domo tools may not offer the most efficient route.", "source": "../../raw_kb/article/data_processing_and_tools_best_practices/index.html", "title": "Data Processing and Tools Best Practices"}, {"objectID": "47c72766f457-7", "text": "When to Contact Domo ETL Team (extra fee may apply)\n\nYour data sources are hundreds of millions or even billions of rows.You are needing an \"outside of the box\" solution to receive/prep data for Domo. \nWhen NOT to Contact the ETL Team\nWhen any of the other methods listed above work efficiently\nTips for Processing Big Data\nWhen uploading data to Domo, break up the large data file by row so the file part is between 100 and 200 Mb.Upload your data using our multi-part streaming API. For more information, visit https://developer.domo.com/docs/streams-api-reference/streams. If an data part upload fails unexpectedly, you can re-upload that data part using the same part ID before closing the multi-part stream upload.", "source": "../../raw_kb/article/data_processing_and_tools_best_practices/index.html", "title": "Data Processing and Tools Best Practices"}, {"objectID": "b140f47a2ed1-0", "text": "Title\n\nData Science Home Page\n\nArticle Body\n\nIntro\nThe Data Science Home Page shows all of the powerful data science tools, all in one place. It gives you quick access to the data science tools, outlines the products and services, and outlines where the tools fit into the data science maturity journey.\nAccessing the Data Science Home Page\nTo access the Data Science Home Page, click on the More menu\u00a0in the top navigation pane and select the Data Science icon.\n\u00a0 \u00a0 \u00a0 >\u00a0 \nPrepare & Discover tab\nThe first tab of the Data Science Home Page is Prepare & Discover. This allows you to clean up your data and discover insights. The first step to a solid Data Science strategy is aligning your data to your organization's needs. These tools can help as you clean, prepare, and explore your data, identifying new correlations and relationships that you can leverage in the next phase.\n\nExperiment & Predict tab\nThe second\u00a0tab of the Data Science Home Page is Experiment & Predict. This allows you to develop models of what drives data.\u00a0With your data in Domo you can run experiments to understand what drives change in your data. These tools help you develop models that predict results with greater efficiency or classify with increased accuracy.\n\nDeploy & Distribute tab\nThe third tab of the Data Science Home Page is Deploy & Distribute. This allows you to gain actionable insights into people and systems.\u00a0Turn your experiments, models, and insights into action. In record time you can build rich stories, powerful apps, and intelligent alerts that are Machine Learning powered and always up to date or extend Data Science outside of your company with embedded analysis for customers and partners.\n\nSearch All Tools tab\nThe final tab of the Data Science Home Page is the Search All Tools. This allows you to see all of the capabilities in one place. You can search for a specific tool or see all tools under a specific use case, such as Hygiene, Insights, etc.", "source": "../../raw_kb/article/data_science_home_page/index.html", "title": "Data Science Home Page"}, {"objectID": "c439ef356129-0", "text": "TitleData Triage FlowchartArticle BodyUse this handy flowchart to aid in resolving issues with your data. To download this PDF to your computer, click here.", "source": "../../raw_kb/article/data_triage_flowchart/index.html", "title": "Data Triage Flowchart"}, {"objectID": "737fe55cccad-0", "text": "TitleData Validation QuestionsArticle BodyFind out what questions to consider when validating your Domo data. To download this PDF to your computer, click here.", "source": "../../raw_kb/article/data_validation_questions/index.html", "title": "Data Validation Questions"}, {"objectID": "c78f4be1639e-0", "text": "Title\n\nDate Difference for Business Days\n\nArticle Body", "source": "../../raw_kb/article/date_difference_for_business_days/index.html", "title": "Date Difference for Business Days"}, {"objectID": "c78f4be1639e-1", "text": "Intro\nIf you need to calculate the date difference between two dates for weekdays/business days, this can be done in Beast Mode or SQL DataFlow using a\u00a0query.\nSQL Query\nCopy and paste the following SQL query into your Beast Mode or DataFlow. Replace `Start Date` and `End Date` with the corresponding columns from your data.\nCASE WHEN\u00a0\n`Start Date` = `End Date` THEN 0\u00a0\nELSE\nDATEDIFF(CASE WHEN DAYOFWEEK(`End Date`) = 7 THEN SUBDATE(`End Date`, INTERVAL 1 DAY)WHEN DAYOFWEEK(`End Date`) = 1 THEN SUBDATE(`End Date`, INTERVAL 2 DAY) ELSE `End Date` END,CASE WHEN DAYOFWEEK(`Start Date`) = 1 THEN ADDDATE(`Start Date`,INTERVAL 1 DAY)WHEN DAYOFWEEK(`Start Date`) = 7 THEN ADDDATE(`Start Date`,INTERVAL 2 DAY) ELSE `Start Date` END)\u00a0\n-", "source": "../../raw_kb/article/date_difference_for_business_days/index.html", "title": "Date Difference for Business Days"}, {"objectID": "c78f4be1639e-2", "text": "-\n((CASE WHEN WEEK(CASE WHEN DAYOFWEEK(`Start Date`) = 1 THEN ADDDATE(`Start Date`,INTERVAL 1 DAY)WHEN DAYOFWEEK(`Start Date`) = 7 THEN ADDDATE(`Start Date`,INTERVAL 2 DAY) ELSE `Start Date` END) = WEEK(CASE WHEN DAYOFWEEK(`End Date`) = 7 THEN SUBDATE(`End Date`, INTERVAL 1 DAY)WHEN DAYOFWEEK(`End Date`) = 1 THEN SUBDATE(`End Date`, INTERVAL 2 DAY) ELSE `End Date` END) THEN 0 ELSE WEEK(CASE WHEN DAYOFWEEK(`End Date`) = 7 THEN SUBDATE(`End Date`, INTERVAL 1 DAY)WHEN DAYOFWEEK(`End Date`) = 1 THEN SUBDATE(`End Date`, INTERVAL 2 DAY) ELSE `End Date` END) END\n-\nCASE WHEN WEEK(CASE WHEN DAYOFWEEK(`Start Date`) = 1 THEN ADDDATE(`Start Date`,INTERVAL 1 DAY)WHEN DAYOFWEEK(`Start Date`) = 7 THEN ADDDATE(`Start Date`,INTERVAL 2 DAY) ELSE `Start Date` END) = WEEK(CASE WHEN DAYOFWEEK(`End Date`) = 7 THEN SUBDATE(`End Date`, INTERVAL 1 DAY)WHEN DAYOFWEEK(`End Date`) = 1 THEN SUBDATE(`End Date`, INTERVAL 2 DAY) ELSE `End Date` END) THEN 0 ELSE WEEK(CASE WHEN DAYOFWEEK(`Start Date`) = 1 THEN ADDDATE(`Start Date`,INTERVAL 1 DAY)WHEN DAYOFWEEK(`Start Date`) = 7 THEN ADDDATE(`Start Date`,INTERVAL 2 DAY) ELSE `Start Date` END) END) * 2)\nEND\nSolution Details and Steps", "source": "../../raw_kb/article/date_difference_for_business_days/index.html", "title": "Date Difference for Business Days"}, {"objectID": "c78f4be1639e-3", "text": "END\nSolution Details and Steps\n1. If the Start Date and End Date are the same, then it will return 0\nCASE WHEN\u00a0\n`Start Date` = `End Date` THEN 0\u00a0\n2.\u00a0If Start Date and End Date aren't the same, then the End Date is moved to Friday and Start Date is moved to Monday when either occurs on a weekend. Then, subtract them to find the date difference. This removes any weekend days from the Start and End Dates.\nELSE\nDATEDIFF(CASE WHEN DAYOFWEEK(`End Date`) = 7 THEN SUBDATE(`End Date`, INTERVAL 1 DAY)WHEN DAYOFWEEK(`End Date`) = 1 THEN SUBDATE(`End Date`, INTERVAL 2 DAY) ELSE `End Date` END,CASE WHEN DAYOFWEEK(`Start Date`) = 1 THEN ADDDATE(`Start Date`,INTERVAL 1 DAY)WHEN DAYOFWEEK(`Start Date`) = 7 THEN ADDDATE(`Start Date`,INTERVAL 2 DAY) ELSE `Start Date` END)\n3.\u00a0If the week number of the Start Date is equal to the week number of the End Date, then return 0. If they are not the same week, then return the week number of the End Date.\n-", "source": "../../raw_kb/article/date_difference_for_business_days/index.html", "title": "Date Difference for Business Days"}, {"objectID": "c78f4be1639e-4", "text": "-\n((CASE WHEN WEEK(CASE WHEN DAYOFWEEK(`Start Date`) = 1 THEN ADDDATE(`Start Date`,INTERVAL 1 DAY)WHEN DAYOFWEEK(`Start Date`) = 7 THEN ADDDATE(`Start Date`,INTERVAL 2 DAY) ELSE `Start Date` END) = WEEK(CASE WHEN DAYOFWEEK(`End Date`) = 7 THEN SUBDATE(`End Date`, INTERVAL 1 DAY)WHEN DAYOFWEEK(`End Date`) = 1 THEN SUBDATE(`End Date`, INTERVAL 2 DAY) ELSE `End Date` END) THEN 0 ELSE WEEK(CASE WHEN DAYOFWEEK(`End Date`) = 7 THEN SUBDATE(`End Date`, INTERVAL 1 DAY)WHEN DAYOFWEEK(`End Date`) = 1 THEN SUBDATE(`End Date`, INTERVAL 2 DAY) ELSE `End Date` END) END\n4.\u00a0If the week number of the Start Date is equal to the week number of the End Date, then return 0. If they are not the same week, then return the week number of the Start Date. This week number is subtracted from the End Date week number calculated in step 3. Then, we multiply by 2. This outputs the total number of weekend days that occur within the Start and End Date range.\n-", "source": "../../raw_kb/article/date_difference_for_business_days/index.html", "title": "Date Difference for Business Days"}, {"objectID": "c78f4be1639e-5", "text": "-\nCASE WHEN WEEK(CASE WHEN DAYOFWEEK(`Start Date`) = 1 THEN ADDDATE(`Start Date`,INTERVAL 1 DAY)WHEN DAYOFWEEK(`Start Date`) = 7 THEN ADDDATE(`Start Date`,INTERVAL 2 DAY) ELSE `Start Date` END) = WEEK(CASE WHEN DAYOFWEEK(`End Date`) = 7 THEN SUBDATE(`End Date`, INTERVAL 1 DAY)WHEN DAYOFWEEK(`End Date`) = 1 THEN SUBDATE(`End Date`, INTERVAL 2 DAY) ELSE `End Date` END) THEN 0 ELSE WEEK(CASE WHEN DAYOFWEEK(`Start Date`) = 1 THEN ADDDATE(`Start Date`,INTERVAL 1 DAY)WHEN DAYOFWEEK(`Start Date`) = 7 THEN ADDDATE(`Start Date`,INTERVAL 2 DAY) ELSE `Start Date` END) END) * 2)\nEND\n5. Lastly, the total number of weekend days is subtracted from the date difference calculated above in step 2.", "source": "../../raw_kb/article/date_difference_for_business_days/index.html", "title": "Date Difference for Business Days"}, {"objectID": "c78f4be1639e-6", "text": "Note: The SQL Query only works if the dates are within the same year. If you need to compare dates in different years, use the Magic ETL method instead.\n\n\n\nMagic ETL\nConnect your Input DatSet tile to a Date Operations tile.Configure the tile as shown here:\n\n\n\n\n\nNote: In Step 4, you will want to make sure to choose your end date first and then your start date as this function subtracts the dates from each other. If you enter the start date first it will result in a negative number.\n\n\n\nThe results will look like this:", "source": "../../raw_kb/article/date_difference_for_business_days/index.html", "title": "Date Difference for Business Days"}, {"objectID": "4329e65564e5-0", "text": "Title\n\nDate Format Specifier Characters for Beast Mode\n\nArticle Body\n\nIntro\nIn a Beast Mode calculation using a DATE_FORMAT function, you can specify the format to use for a date or time column by specifying the column and the date or time string, as in DATE_FORMAT(`datecolumn`,'format') where datecolumn is the column containing a date value and where format is the string containing specifier characters to use in formatting the date value.\nFor example, using DATE_FORMAT(`MyDate`,'%Y-%m-%d %h:%i %p'), the date in the MyDate date column uses this format: 2013-04-17 10:10 AM.\nThe \"%\" character is required before format specifier characters.\nIf necessary, you can convert date string values in columns to datetime values using the STR_TO_DATE function.\n\n\n \n\n\nNote:\u00a0In a Beast Mode calculation using a TIME_FORMAT function, you can specify the format to use for time values in a time column by specifying the column and the time string, as in TIME_FORMAT(`datetimecolumn`,'format') where datetimecolumn is the column containing a time value and where format is the string containing specifier characters to use in formatting the time value.The format specifiers used in DATE_FORMAT may be used with TIME_FORMAT, but specifiers other than hours, minutes, seconds and microseconds produce a NULL value or 0.", "source": "../../raw_kb/article/date_format_specifier_characters_for_beast_mode/index.html", "title": "Date Format Specifier Characters for Beast Mode"}, {"objectID": "4329e65564e5-1", "text": "Sample Formats\nFormatResultDATE_FORMAT(`MyDateColumn`,'%m-%d-%Y')11-04-2008DATE_FORMAT(`MyDateColumn`,'%d %b %y')04 Nov 08DATE_FORMAT(`MyDateColumn`,'%d %b %Y %T')04 Nov 2008 11:45:34DATE_FORMAT(`MyDateColumn`, '%m/%d/%Y')01/13/2013DATE_FORMAT(`MyDateColumn`, '%M %d, %Y')August 1, 2013DATE_FORMAT(`MyDateColumn`, '%b %d, %Y')Aug 1, 2013DATE_FORMAT(`MyDateColumn`,'%b %d %Y %h:%i %p')Nov 04 2008 11:45 PMDATE_FORMAT(`MyDateColumn`,'%Y-%m-%d %h:%i:%s')2013-11-06 09:38:10DATE_FORMAT(STR_TO_DATE(`MyDateColumn`,'%m,%d,%Y'),'%m/%d/%Y')11/04/2008\nFor more information about\u00a0adding Beast Mode calculations, see\u00a0Adding a Beast Mode Calculation to Your Chart.\nDate format specifiers\nAll examples assume a date and time of April 15th, 2013, at 11:44:15 PM.", "source": "../../raw_kb/article/date_format_specifier_characters_for_beast_mode/index.html", "title": "Date Format Specifier Characters for Beast Mode"}, {"objectID": "4329e65564e5-2", "text": "SpecifierDescriptionSampleResult%aAbbreviated weekday name (Sun..Sat)DATE_FORMAT(`MyDate`,'%a')Mon\u00a0%bAbbreviated month name (Jan..Dec)DATE_FORMAT(`MyDate`,'%b')Apr%dDay of month, numeric (00-31)DATE_FORMAT(`MyDate`,'%d')13%fMicroseconds (000000..999999)DATE_FORMAT(`MyDate`,'%f')300000%HHour (00-23)DATE_FORMAT(`MyDate`,'%H')23%hHour (01-12)DATE_FORMAT(`MyDate`,'%h')11%iMinutes, numeric (00-59)DATE_FORMAT(`MyDate`,'%i')44%jDay of year (001-366)DATE_FORMAT(`MyDate`, '%j')105%MMonth name (January..December)", "source": "../../raw_kb/article/date_format_specifier_characters_for_beast_mode/index.html", "title": "Date Format Specifier Characters for Beast Mode"}, {"objectID": "4329e65564e5-3", "text": "Note: This option is padded with trailing whitespace to 9 characters.\n\n\nDATE_FORMAT(`MyDate`,'%M')April%mMonth, numeric (00-12)DATE_FORMAT(`MyDate`,'%m')04%pAM or PMDATE_FORMAT(`MyDate`,'%p')PM%rTime, 12-hour (hh:mm:ss followed by\u00a0AM or PM)DATE_FORMAT(`MyDate`,'%r')11:44:15 PM%sSeconds (00-59)DATE_FORMAT(`MyDate`,'%s')15%TTime, 24-hour (hh:mm:ss)DATE_FORMAT(`MyDate`,'%T')23:44:15%vISO week number of the year (the first Thursday of the new year is in week 1)DATE_FORMAT(`MyDate`,'v')16%WWeekday name (Sunday..Saturday)DATE_FORMAT(`MyDate`,'%W')Monday%wDay of the week (1-7; Sunday is 1)DATE_FORMAT(`MyDate`,'w')2%xISO year (4 or more digits)DATE_FORMAT(`MyDate`,'x')2013%YYear (4 or more digits)DATE_FORMAT(`MyDate`,'%Y')2013%yLast two digits of yearDATE_FORMAT(`MyDate`,'%y')13\nUnit type values\nYou can specify unit type values for date or datetime expressions in ADDDATE, DATE_ADD, DATE_SUB, and SUBDATE functions, including those listed in the following table.\n\n\n \n\n\nNote:\u00a0In Beast Mode calculations, the unit type value is case insensitive.", "source": "../../raw_kb/article/date_format_specifier_characters_for_beast_mode/index.html", "title": "Date Format Specifier Characters for Beast Mode"}, {"objectID": "4329e65564e5-4", "text": "Note:\u00a0In Beast Mode calculations, the unit type value is case insensitive.\n\n\n\nUnit Type ValueExpected Expression FormatMICROSECONDMICROSECONDSSECONDSECONDSMINUTEMINUTESHOURHOURSDAYDAYSWEEKWEEKSMONTHMONTHSQUARTERQUARTERSYEARYEARSSECOND_MICROSECOND'SECONDS.MICROSECONDS'MINUTE_MICROSECOND'MINUTES:SECONDS.MICROSECONDS'MINUTE_SECOND'MINUTES:SECONDS'HOUR_MICROSECOND'HOURS:MINUTES:SECONDS.MICROSECONDS'HOUR_SECOND'HOURS:MINUTES:SECONDS'HOUR_MINUTE'HOURS:MINUTES'DAY_MICROSECOND'DAYS HOURS:MINUTES:SECONDS.MICROSECONDS'DAY_SECOND'DAYS HOURS:MINUTES:SECONDS'DAY_MINUTE'DAYS HOURS:MINUTES'DAY_HOUR'DAYS HOURS'YEAR_MONTH'YEARS-MONTHS'", "source": "../../raw_kb/article/date_format_specifier_characters_for_beast_mode/index.html", "title": "Date Format Specifier Characters for Beast Mode"}, {"objectID": "2a4cf5c9cf14-0", "text": "Title\n\nDate Selector Card\n\nArticle Body\n\nIntro\nDate Selector\u00a0Cards display a calendar-style range of dates. When you add them\u00a0to a Card Page in Domo, users visiting that Page can select a range of dates from the calendar as a filter. Alternatively, they can choose from a number of preset date filters such as \"Last year,\" \"Last 2 months,\" etc. The date filter you choose is applied to all other Cards on the Page. For example, if you selected all dates from April 1st to July 31st on the calendar, all other Cards in the Page would be filtered to show just the data for these dates.\u00a0All Date Selector Cards have a 25,000 row limit.\n\nFor information about other Filter Card types, see the following:\nSlicer, Checkbox, and Radio Button CardsRange Selector Card\nVideo - Filter Chart Types", "source": "../../raw_kb/article/date_selector_card/index.html", "title": "Date Selector Card"}, {"objectID": "2a4cf5c9cf14-1", "text": "Powering Date Selector Cards\nThis type of graph requires two columns or rows of data from your DataSet. One of these contains a\u00a0date column. You drop this column into the \"Date\" field in Analyzer.\u00a0\n\u00a0\nThe other field is called \"Group By.\" Though this field is required to power up the Card, in general it doesn't matter what column you drop here. If you need to aggregate the data, you can use this column to do so. For example, if the date column is duplicated in your DataSet, you can make the dates unique by aggregating.\u00a0\nFor more information about choosing data columns, see\u00a0Applying DataSet Columns to Your Chart.\nFor more information about formatting charts in the Analyzer, see\u00a0Visualization Card Building Part 2: The Analyzer.\nCustomizing Date Selector Cards\nYou can customize the appearance of a Date Selector Card\u00a0by editing its Chart Properties. For information about all chart properties, see Chart Properties. Unique properties of Date Selector Cards include the following:", "source": "../../raw_kb/article/date_selector_card/index.html", "title": "Date Selector Card"}, {"objectID": "2a4cf5c9cf14-2", "text": "PropertyDescriptionExampleGeneral > Selected ColorLets you change the highlight color used when you select a range of calendar dates in the Card.\u00a0General > Highlight TodayWhen checked, a box appears around today's date in the calendar. In the example at right, the date of \"July 25\" is highlighted.\u00a0General > ViewDetermines the calendar view that is used. The following options are available:Auto (Default). The view is determined automatically based on your data. For example, if you had at least a year's worth of data in your date column, a yearly view would be used.Last Year Rolling. The calendar shows dates\u00a0from a year ago up until the current month. This is the option shown in the example at right.Next Year Rolling. The calendar shows dates from the current month up until a year from now.\u00a0Year. The current year is shown.Month. The current month is shown.General > Dates FromSpecifies whether the dates that appear in the calendar are based on the current date, are automatically determined based on your data, or are user-specified (in which case, you", "source": "../../raw_kb/article/date_selector_card/index.html", "title": "Date Selector Card"}, {"objectID": "2a4cf5c9cf14-3", "text": "data, or are user-specified (in which case, you will enter the date parameters in the\u00a0Year, Selected Year, and Month\u00a0fields.\u2014General > YearDetermines whether the year shown in the calendar is the current year, the previous year, or the next year. You can also specify a year by selecting\u00a0Specified\u00a0and entering the desired year in the\u00a0Selected Year\u00a0field. This option is only available when\u00a0Dates From\u00a0is set to\u00a0User Specified.\u2014General > Selected YearLets you enter a specific year to be shown in the calendar.\u00a0This option is only available when\u00a0Dates From\u00a0is set to\u00a0User Specified\u00a0and\u00a0Year\u00a0is set to\u00a0Specified.\u00a0\u2014General > MonthLets you enter a specific month to be shown in the calendar.\u00a0This option is only available when\u00a0Dates From\u00a0is set to\u00a0User Specified\u00a0and\u00a0View\u00a0is set to Month.\u2014General > First Day of WeekLets you select the first day of the week to be shown in your calendar. In the example at right, the first day of the week is Monday.General > First Month DisplayedLets you select the first month of the", "source": "../../raw_kb/article/date_selector_card/index.html", "title": "Date Selector Card"}, {"objectID": "2a4cf5c9cf14-4", "text": "First Month DisplayedLets you select the first month of the year to be shown in your calendar. This option is only available when\u00a0Year\u00a0is selected in the\u00a0View\u00a0menu.\u00a0\u2014", "source": "../../raw_kb/article/date_selector_card/index.html", "title": "Date Selector Card"}, {"objectID": "f5abbca18d3d-0", "text": "TitleDayforce ConnectorArticle BodyIntro\nDayforce is an industry-leading, online human capital management platform. To learn more about the Dayforce API, visit their page (https://usconfigr57.dayforcehcm.com/api/ddn/swagger/).\nYou connect to your Dayforce account in the Data Center. This topic discusses the fields and menus that are specific to the Dayforce connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Dayforce account and create a DataSet, you must have the following:\nYour Dayforce username and passwordThe client name space that uniquely identifies the Dayforce instance.\u00a0\nConnecting to your Dayforce account\nThis section enumerates the options in the Credentials and Details panes in the Dayforce Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials pane\nThis pane contains fields for entering credentials to connect to your Dayforce account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter your Dayforce username.PasswordEnter your Dayforce password.Client Name SpaceEnter the client name space that uniquely identifies your Dayforce instance.\nOnce you have entered valid Dayforce credentials, you can use the same account any time you go to create a new Dayforce DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/dayforce_connector/index.html", "title": "Dayforce Connector"}, {"objectID": "f5abbca18d3d-1", "text": "MenuDescriptionReportSelect the Dayforce report you want to run.\u00a0The following reports are available:Get ReportRetrieves the data of the report customized on Dayforce.DocumentsRetrieves a list of documents.Employee AvailabilityRetrieves employee availability information.Employee Compensation SummaryRetrieves compensation information.Employee DetailsRetrieves details for a given employee.Employee EthnicityRetrieves employee ethnicity information.Employee LocationsRetrieves employee location data.Employee ManagersRetrieves a list of employee managers.\u00a0Employee Org InfoRetrieves data for where employees fall in the organization.Employee PunchRetrieves employee punchcard data.\u00a0Employee Raw PunchRetrieves employees' punchcard data in raw format.Employee ScheduleRetrieves employee scheduling information.Employee Time Away from WorkRetrieves data for how much time employees are spending away from work.Employee UnionRetrieves employee union data.Employee Work AssignmentsRetrieves employee work assignment information.Employee Work ContractsRetrieves a list of employee work contracts.Employee Job PostingsRetrieves a list of job postings.X Ref Code TypeSelect whether you want to pull data for all employees (All) or for a specific employee based on their employee code (Select XRef Code).\u00a0Skip Employee Without Documents?Check this box if you want your report to ignore employees who do not have documents in the system.Employee CodeSelect the code for the employee you want to retrieve data for.Report CodeSelect the report you want to import into Domo.StatusSelect the status you want to filter your report by.Include Date Filter?Check this box if you wish to filter your report by date.DatesSelect the desired date range for your report.\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/dayforce_connector/index.html", "title": "Dayforce Connector"}, {"objectID": "9305eb925b0f-0", "text": "TitleDealing with Attribution WindowsArticle Body\"Attribution windows\" are essentially periods in which some companies (such as Facebook and YouTube) reserve the right to restate their metrics over a period of days. For example, Youtube has a 35-day attribution window,\u00a0meaning\u00a0they can restate their numbers anytime within this window. Likewise, Facebook has a 28-day window.\nYou can deal with such attribution windows by setting up a rolling file and a historical file. The rolling file should be set to replace and should cover the attribution window. The historical file should be set to contain the growing history and should execute once a day to capture just a single day\u2019s data in an append.\nThere are multiple ways to combine these files and some of it depends on API timing. If both of these calls (the rolling and append daily) happen quickly and early, the fastest and simplest approach is to use DataFusion. The files\u00a0will contain the same schema because they are the\u00a0same call, covering different date ranges. If both calls are made early and run quickly, you will only have minimal minutes each day where the calls are executing, and you could possibly have a gap between the rolling\u00a0file and the historical file completing with a single day missing.\nIf the DataSets\u00a0are small and you need to account for the slight chance of a missing day during the run, you\u2019ll want to set up an\u00a0ETL like the following:", "source": "../../raw_kb/article/dealing_with_attribution_windows/index.html", "title": "Dealing with Attribution Windows"}, {"objectID": "9305eb925b0f-1", "text": "In the above screenshot you have a top branch (historical) that contains the history including day 35 as an append each day. In the bottom branch you have a daily replace file of the last 35 days. So there is overlap of day 35 in both files. Because of this, you need to do a filter on one of the branches removing the 35th day so it is not counted twice. As long as one of these DataSets is running quickly and reliably, you will have this data overlapped then filtered to ensure reliable, contiguous reporting.\u00a0\nHere are the parameters for each transform:", "source": "../../raw_kb/article/dealing_with_attribution_windows/index.html", "title": "Dealing with Attribution Windows"}, {"objectID": "0fb6f5bb05a7-0", "text": "TitleDecipher ConnectorArticle BodyIntro\nDecipher is a mobile-friendly, online survey software and reporting solution for insights pros who need to execute sophisticated quantitative research. To learn more about the Decipher API, visit their page (https://support.focusvision.com/Deci...r_REST_API_2.0).\nYou connect to your Decipher account in the Data Center. This topic discusses the fields and menus that are specific to the Decipher connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in adding\u00a0 a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Decipher account and create a DataSet, you must have the following:\nYour Decipher API key. To generate an API key, navigate to\u00a0https://support.focusvision.com/Deci...r_REST_API_2.0 then scroll down to the section called \"Generate an API Key from the Research\u00a0Hub\" and follow the steps.Your Decipher host URL. By default this is set to https://v2.decipherinc.com, but you can modify this if necessary.\nConnecting to Your Decipher Account\nThis section enumerates the options in the Credentials and Details panes in the Decipher Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Decipher account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAPI KeyEnter your Decipher API key.\u00a0Host URLEnter your Decipher host URL.", "source": "../../raw_kb/article/decipher_connector/index.html", "title": "Decipher Connector"}, {"objectID": "0fb6f5bb05a7-1", "text": "Once you have entered valid Decipher credentials, you can use the same account any time you go to create a new Decipher DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Decipher report you want to run.\u00a0The following reports are available:All SurveysReturns a\u00a0collection of available surveys.Crosstab ConfigurationRetrieves segments and globally available weighting variables configured for a survey.DatamapRetrieves the datamap of a survey, explaining questions and variables.DatasourceReturns the definition of a single datasource.Datasource DataReturns all datasource data.DatasourcesReturns a list of current datasources and their data.FeedRetrieves new data.\u00a0Feed StateReturns the internal state of a data feed.LayoutsReturns a list of data layouts for a survey.QuotaRetrieves quota cell definitions.\u00a0Saved ReportsLists \u201csaved reports\u201d and \u201csaved crosstabs\u201d created on a survey.Survey Data\n\u00a0Returns respondent data for survey.", "source": "../../raw_kb/article/decipher_connector/index.html", "title": "Decipher Connector"}, {"objectID": "0fb6f5bb05a7-2", "text": "Survey PathSelect the survey path you want to retrieve data for.Datasource ListSelect the datasource you want to pull data from.FeedEnter the ID of the feed you want to pull data from.Layout (Optional)Enter the number of the layout you want to retrieve data for.StateEnter the survey state.Fields (Optional)Enter the names of the fields you want to return, separated by commas. For example: uuid,idDatamap KeySelect the datamap key to pull data for, either\u00a0Questions\u00a0or\u00a0Variables.Quota KeySelect the key for your \"Quota\" report. Available keys include the following:DefinesMaps a marker name to an object.MarkersMaps each quota cell marker name to an object describing its current state.SheetsMaps sheet names to definitions. Each sheet definition is an array of tables.StoppedReturns an object with the quota cell markers of cells that were stopped in the field report and the user who stopped the cell.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/decipher_connector/index.html", "title": "Decipher Connector"}, {"objectID": "0fb6f5bb05a7-3", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\n\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/decipher_connector/index.html", "title": "Decipher Connector"}, {"objectID": "b2d986187387-0", "text": "Title\n\nDefault Security Role Reference\n\nArticle Body\n\nIntro\nYou can access content and perform actions in Domo, depending on the grants of your security role. For information about features for controlling access, see Controlling Access in Domo.\nBelow are the various default security roles in Domo:\nAdmin\nPrivileged\nEditor\nParticipant\nSocial\nEach security role has a set of associated grants. Note that these are the default security roles available in Domo. (They are also known as \"Domo roles\"). You can also create custom security roles in More > Admin > Governance > Roles. This is discussed in detail in\u00a0Managing Roles.\u00a0\nIf you have an \"Admin\" security role or a customized role with the \"Manage all roles\" privilege, you can manage grants and security roles of other users in Domo. For information about assigning security roles, see Assigning a Security Role to a User.\n\n\n\u00a0\n\nNote: If a user's access level is changed, they may not be able to see certain assets anymore, depending on their ownership of those assets. For example, if a Privileged user who owns a Page or Card is reassigned to the level of Participant user, he or she will still have access to that Card or Page. If on the other hand a Card or Page was merely shared with that user, they would no longer have access. It will essentially disappear from their view.", "source": "../../raw_kb/article/default_security_role_reference/index.html", "title": "Default Security Role Reference"}, {"objectID": "b2d986187387-1", "text": "Grant Levels for Default Security Roles\nThe following sections describes the level of grants for the default security roles.\nAdmin\nUsers\u00a0with this default security role have all grants.\nPrivileged", "source": "../../raw_kb/article/default_security_role_reference/index.html", "title": "Default Security Role Reference"}, {"objectID": "b2d986187387-2", "text": "Admin\nUsers\u00a0with this default security role have all grants.\nPrivileged\nGrantDescriptionAlertsCreate, edit and share any Alerts to which you have access.View, edit, add, or delete Alert Actions on any Alert of which the user is owner.AppstoreInstall and use Apps from the Appstore. Uninstall Apps that you previously installed.View the Appstore and App information.Cards and DashboardsUse Domo Everywhere to embed cards and dashboards externally, with or without authentication.Create, edit and delete cards, Drill views, and Beast Modes to which you have access. Share cards to which you have access.Create and edit DataSet views in AnalyzerCreate, edit and delete dashboards, styles, templates, and layouts.Create Scheduled Reports and export to email, print, PowerPoint, CSV, and Excel.Create and edit natural language generated narratives in notebook cards.DataCreate data in AppDB.Create, edit and delete DataSets to which you have access.Export data from DataSets you have access to.Create accounts for DataSets in your instance.Allow Workbench download.Create, edit and delete DataFlows to which you have access. Requires the Edit DataSet grant.Train AutoML models. Run DataFlows containing AutoML Inference actions.Create, edit and run Magic ETL v2 DataFlows to which you have access.Create, edit and delete Jupyter Workspaces to which you have access. Requires the Edit DataSet grant.DomoAppsView custom DomoApps to which you have access.Create, edit and upload custom DomoApps.GoalsCreate and edit any Goals to which you have access.Pipeline Executor ServiceCreate, edit, or remove custom pipeline processes.Users and GroupsAdd new people.Create, edit and delete groups the person has been granted access to. Add and remove people from groups the person with the grant has access to.Create, edit, delete and assign achievements.WorkflowsSubmit a certification request for content you have access to.Create workflow models.\nEditor", "source": "../../raw_kb/article/default_security_role_reference/index.html", "title": "Default Security Role Reference"}, {"objectID": "b2d986187387-3", "text": "Editor\nGrantDescriptionAlertsCreate, edit and share any Alerts to which you have access.View, edit, add, or delete Alert Actions on any Alert of which the user is owner.AppstoreInstall and use Apps from the Appstore. Uninstall Apps that you previously installed.View the Appstore and App information.Cards and DashboardsUse Domo Everywhere to embed cards and dashboards externally, with or without authentication.Create, edit and delete cards, Drill views, and Beast Modes to which you have access. Share cards to which you have access.Create and edit DataSet views in AnalyzerCreate, edit and delete dashboards, styles, templates, and layouts.Create Scheduled Reports and export to email, print, PowerPoint, CSV, and Excel.Create and edit natural language generated narratives in Notebook Cards.DataCreate data in AppDB.Create, edit and delete DataSets to which you have access.Export data from DataSets you have access to.Create accounts for DataSets in this instance.Allow Workbench download.Create, edit and delete DataFlows to which you have access. Requires the Edit DataSet grant.Train AutoML models. Run DataFlows containing AutoML Inference actions.Create, edit and run Magic ETL v2 DataFlows to which you have access.Create, edit and delete Jupyter Workspaces to which you have access. Requires the Edit DataSet grant.DomoAppsView custom DomoApps to which you have access.Create, edit and upload custom DomoApps.GoalsCreate and edit any Goals to which you have access.WorkflowsSubmit a certification request for content you have access to.Create workflow models.\nParticipant", "source": "../../raw_kb/article/default_security_role_reference/index.html", "title": "Default Security Role Reference"}, {"objectID": "b2d986187387-4", "text": "Participant\nGrantDescriptionAlertsCreate, edit and share any Alerts to which you have access.AppstoreView the Appstore and App information.Cards and DashboardsCreate Scheduled Reports and export to email, print, PowerPoint, CSV, and Excel.DomoAppsView custom DomoApps to which you have access.GoalsCreate and edit any Goals to which you have access.WorkflowsSubmit a certification request for content you have access to.\nSocial\nThis default security role is for trial users and allows access only to Buzz, user profiles, and Projects and Tasks. These users can also create, edit, and share any Alerts to which they have access.\u200b\u200b\u200b\u200b\nGrants\nA list of the available grants can be found in your Domo instance by following these steps:\nClick on More > Admin > Governance tab.Select the Roles sub-tab.Select the Grants tab.  (Optional) Click on the grant for a description of what it controls and what roles have been assigned the grant.\nSetting the Default Security Role\nAs an Admin, you can set the default security role that will be assigned to new users who are invited to the instance. The default role is set to Privileged unless it is changed by an Admin.\nTo change the default role for your instance,\nNavigate to the More > Admin > Governance > Roles subtab.Click the Default role: dropdown in the top right-hand corner.Select the role you would like to be the default when new users are added to Domo.", "source": "../../raw_kb/article/default_security_role_reference/index.html", "title": "Default Security Role Reference"}, {"objectID": "e7bf279f1201-0", "text": "Title\n\nDeleting a Document Version\n\nArticle Body\n\nYou can delete unwanted versions of a Doc\u00a0Card\u00a0file. You do this in the Details view for the Doc Card. In the Details view, all versions of the file\u00a0are stored as thumbnails. You can delete any version by locating the thumbnail for that version and clicking the\u00a0icon.\nTo delete a version of a file,\nClick the Doc Card\u00a0to open its\u00a0Details view.Locate the\u00a0thumbnail for the version of the document you want to\u00a0delete and mouse\u00a0over it.Click\u00a0.\nThe version is deleted.", "source": "../../raw_kb/article/deleting_a_document_version/index.html", "title": "Deleting a Document Version"}, {"objectID": "4f6a71c0bef7-0", "text": "Title\n\nDeleting Cards from Domo\n\nArticle Body\n\nYou can permanently delete cards from Domo.\n\n\n \n\n\nNote:\u00a0You can remove a card from a page but not delete the card from Domo so the card may appear on other pages. For more information about removing a card from a page\u00a0Removing Cards from a Page. \u00a0\n\n\n\nWhen you permanently delete a card from Domo, the card no longer exists and cannot be added to pages. Deleting a card removes all copies of the card from Domo. It does\u00a0not\u00a0remove duplicates of the card. For more information about copying and duplicating cards, see Duplicating Cards (\"Save As\")\u00a0respectively.\nYou can delete a card from Domo in the page view, in the card\u00a0Details\u00a0page, or in the\u00a0Admin Settings. In the Admin Settings, you can delete multiple cards at once.\nVideo - Removing and Deleting Cards in Domo", "source": "../../raw_kb/article/deleting_cards_from_domo/index.html", "title": "Deleting Cards from Domo"}, {"objectID": "4f6a71c0bef7-1", "text": "Deleting from the page view or card Details view\nYou can delete a card from the page view or the card's Details\u00a0view. To do this you must be the card owner or have an \"Admin\" default security role or a custom role with \"Manage All Cards and Pages\" enabled. For more information about default security roles, see\u00a0Managing Custom Roles.\nTo delete a card from the page view or Details view,\nDo one of the following:In the page view for the card, mouse over the card, click , then select Delete.In the\u00a0Details\u00a0view for the card, click\u00a0, then select\u00a0Delete.A dialog appears.Click\u00a0Delete Card.\nAll copies of the card are deleted.\nDeleting from the\u00a0Admin Settings\nUsers with an \"Admin\" default security role or a custom role with \"Manage All Cards and Pages\" enabled can delete any card from Domo in\u00a0Admin Settings. When you use this method, you can delete multiple cards with a single action. (When you delete cards using the previously described method, you must delete them one at a time.) For more information about default security roles, see\u00a0Managing Custom Roles.\nTo delete cards in the  Admin Settings,\nClick\u00a0More\u00a0>  Admin.The\u00a0Admin Settings appears.Click Content > Cards.A list of all cards in your instance appears.Check the boxes for all cards you want to delete.You can use the column filters to filter the cards in the list. You can also select or deselect all cards passed through your selected filters.Select Edit > Delete Cards.Click\u00a0Delete Cards to confirm.\u00a0All copies of the card are\u00a0deleted.", "source": "../../raw_kb/article/deleting_cards_from_domo/index.html", "title": "Deleting Cards from Domo"}, {"objectID": "c28e4c2e4b81-0", "text": "TitleDeleting DataSetsArticle BodyYou can delete a DataSet\u00a0from your Domo instance if you are the DataSet owner or have an \"Admin\" default security role or a custom role with the \"Manage DataSets\" privilege enabled. For more information about default security roles, see\u00a0Changing the Owner of a DataSet.\nYou can delete DataSets one at a time or in bulk. To take bulk actions on DataSets, you must\u00a0have an \"Admin\" default security role or a custom role with the \"Manage DataSets\" privilege enabled.\nTo delete a DataSet from your Domo instance,\nClick Data in the toolbar at the top of the screen.\tThe Data Center opens, with the\u00a0Data Warehouse\u00a0tab opened by default.\u00a0Click the\u00a0\u00a0icon on the left side of the screen to open the\u00a0DataSets\u00a0tab.Locate the DataSet\u00a0you want to delete.\tFor more information about searching and filtering DataSets in the Data Center, see\u00a0Data Center Layout.Mouse over the row for the DataSet\u00a0and click the\u00a0\u00a0icon.\u00a0Select\u00a0Delete.Click\u00a0Delete\u00a0to confirm.\nYou can also delete a DataSet from the details view for that DataSet by selecting\u00a0Delete\u00a0in the wrench menu. For more information about the details view, see the\u00a0View Details\u00a0heading of\u00a0Data Center Layout.\nTo delete\u00a0DataSets in bulk,", "source": "../../raw_kb/article/deleting_datasets/index.html", "title": "Deleting DataSets"}, {"objectID": "c28e4c2e4b81-1", "text": "To delete\u00a0DataSets in bulk,\nClick Data in the toolbar at the top of the screen.\tThe Data Center opens, with the\u00a0Data Warehouse\u00a0tab opened by default.\u00a0Click the\u00a0\u00a0icon on the left side of the screen to open the\u00a0DataSets\u00a0tab.Locate one of the DataSets you want to delete.\tFor more information about searching and filtering DataSets in the Data Center, see\u00a0Data Center Layout.Mouse over the row for the DataSet and click the circled checkmark that pops up over the connector icon.\u00a0  In the blue bar that appears at the top of the screen, click the\u00a0\u00a0icon.Select\u00a0Delete.  \nAll of the DataSets you have selected should now be deleted.", "source": "../../raw_kb/article/deleting_datasets/index.html", "title": "Deleting DataSets"}, {"objectID": "b3cdcc7cf018-0", "text": "TitleDeltek Vision ConnectorArticle BodyIntro\nDeltek Vision is an ERP solution specifically for the project-focused firms, providing visibility into project budgets, schedules, and other project data. Use Domo's Deltek connector to retrieve data about campaigns, opportunities, employee expenses and repayments, and other Deltek entities. To learn more about the Deltek API, visit their page ( https://deltek.custhelp.com/app/answers/detail/a_id/62647/kw/web%20services%20api ).\nYou connect to your Deltek account in the Data Center. This topic discusses the fields and menus that are specific to the Deltek connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nYou can also connect to Deltek\u00a0 through the database that sits on premise on your network. In this case, use of SQL is required.\nPrerequisites\nTo connect to your Deltek account and create a DataSet, you must have the following:\nYour Deltek usernameYour Deltek passwordThe domain name of your Deltek instance (for instance, if the instance was mycompany.deltek.com, the domain name would be mycompany)The name of the database you want to retrieve data fromYour Deltek host nameYour Deltek host port number\nConnecting to Your Deltek Account\nThis section enumerates the options in the Credentials and Details panes in the Deltek Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Deltek account. The following table describes what is needed for each field:", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-1", "text": "FieldDescriptionUserEnter your Deltek username.PasswordEnter your Deltek password.DomainEnter your Deltek domain name.DatabaseEnter the name of the Deltek database you want to retrieve data from.HostEnter your Deltek host name.PortEnter the port number of your Deltek host.\nOnce you have entered valid Deltek credentials, you can use the same account any time you go to create a new Deltek DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Deltek report you want to run.\u00a0The following reports are available:ActivitiesReturns data on activities.AP DisbursementsReturns data on AP disbursements.AP VouchersReturns data on AP vouchers.Cash DisbursementsReturns data on cash disbursements.Cash ReceiptsReturns data on cash receipts.ClientsReturns data on clients.ContactsReturns data on contacts.EmployeeExpensesReturns data on employee expenses.EmployeeRepaymentsReturns data on employee repayments.EmployeesReturns data on employees.InvoicesReturns data on invoices.JournalEntriesReturns data on journal entries.LaborAdjustmentsReturns data on labor adjustments.LeadsReturns data on leads.MiscellaneousExpensesReturns data on miscellaneous expenses.MktCampaignsReturns data on market campaigns,OpportunitiesReturns data on opportunities.PrintandReproductionsReturns data on print and reproductions.ProjectsReturns data on projects.TextLibrariesReturns data on text libraries.TimesheetsReturns data on timesheets.UnitsReturns data on units.UnitsByProjectReturns data on units by project.Rolling Months of DataEnter the number of months back you want to pull data for.\nOther Panes", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-2", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nQueries\nThe Deltek Vision connector uses the following queries:", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-3", "text": "APVouchers  create index cbatchindex on APVouchersControl(Batch);create index mbatchindex on APVouchersMaster(Batch);create index mmasterkeyindex on APVouchersMaster(MasterPKey);create index dbatchindex on APVouchersDetail(Batch);create index dmasterkeyindex on APVouchersDetail(MasterPKey);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.DefaultLiab, c.DefaultBank, c.DefaultDate, c.DefaultTaxCode, c.PostPeriod, c.PostSeq, c.Company, c.DefaultCurrencyCode, c.DefaultTax2Code, c.SubmittedBy, c.SubmittedDate, m.Vendor, m.InvoiceDate, m.Invoice, m.TransDate, m.LiabCode, m.BankCode, m.PayTerms, m.PayDate, m.Seq as MasterSeq, m.Voucher, m.CurrencyCode, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.BarCode, m.PaymentExchangeOverrideMethod, m.PaymentExchangeOverrideDate, m.PaymentExchangeOverrideRate, m.PaymentCurrencyCode, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.PKey, d.Seq as DetailSeq, d.Description, d.WBS1, d.WBS2, d.WBS3, d.Account, d.Amount, d.SuppressBill, d.TaxCode, d.NetAmount, d.TaxAmount, d.CurrencyExchangeOverrideRate, d.PONumber,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-4", "text": "d.TaxAmount, d.CurrencyExchangeOverrideRate, d.PONumber, d.PaymentExchangeRate, d.PaymentAmount, d.PaymentExchangeInfo, d.ExpenseCode, d.Tax2Code, d.Tax2Amount, d.CompoundTax from APVouchersControl c join APVouchersMaster m on c.Batch = m.Batch join APVouchersDetail d on m.Batch = d.Batch and m.MasterPKey = d.MasterPKey   APDisbursements  create index cbatchindex on APDisbursementsControl(Batch);create index mbatchindex on APDisbursementsMaster(Batch);create index mmasterkeyindex on APDisbursementsMaster(MasterPKey);create index dbatchindex on APDisbursementsDetail(Batch);create index dmasterkeyindex on APDisbursementsDetail(MasterPKey);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.DefaultLiab, c.DefaultBank, c.DefaultDate, c.DefaultTaxCode, c.PostPeriod, c.PostSeq, c.Company, c.DefaultCurrencyCode, c.DefaultTax2Code, c.SubmittedBy, c.SubmittedDate, m.Vendor, m.InvoiceDate, m.Invoice, m.TransDate, m.LiabCode, m.BankCode, m.PayTerms, m.PayDate, m.Posted, m.Seq as MasterSeq, m.Voucher, m.CurrencyCode, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.BarCode,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-5", "text": "m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.BarCode, m.PaymentExchangeOverrideMethod, m.PaymentExchangeOverrideDate, m.PaymentExchangeOverrideRate, m.PaymentCurrencyCode, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.PKey, d.Seq as DetailSeq, d.Description, d.WBS1, d.WBS2, d.WBS3, d.Account, d.Amount, d.SuppressBill, d.TaxCode, d.NetAmount, d.TaxAmount, d.CurrencyExchangeOverrideRate, d.PONumber, d.PaymentExchangeRate, d.PaymentAmount, d.PaymentExchangeInfo, d.ExpenseCode, d.Tax2Code, d.Tax2Amount, d.CompoundTax from APDisbursementsControl c join APDisbursementsMaster m on c.Batch = m.Batch join APDisbursementsDetail d on m.Batch = d.Batch and m.MasterPKey = d.MasterPKey   CashDisbursements  create index cbatchindex on CashDisbursementsControl(Batch);create index mbatchindex on CashDisbursementsMaster(Batch);create index mchecknoindex on CashDisbursementsMaster(CheckNo);create index dbatchindex on CashDisbursementsDetail(Batch);create index dchecknoindex on CashDisbursementsDetail(CheckNo);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.DefaultBank, c.DefaultTaxCode, c.PostPeriod,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-6", "text": "c.Total, c.DefaultBank, c.DefaultTaxCode, c.PostPeriod, c.PostSeq, c.Company, c.DefaultTax2Code, c.SubmittedBy, c.SubmittedDate, m.TransDate, m.BankCode, m.Payee, m.Posted, m.Seq as MasterSeq, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.CheckNo, d.PKey, d.Seq as DetailSeq, d.Description, d.WBS1, d.WBS2, d.WBS3, d.Account, d.Amount, d.TaxCode, d.NetAmount, d.TaxAmount, d.CurrencyExchangeOverrideRate, d.LinkCompany, d.Tax2Code, d.Tax2Amount, d.CompoundTax from CashDisbursementsControl c join CashDisbursementsMaster m on c.Batch = m.Batch join CashDisbursementsDetail d on m.Batch = d.Batch and m.CheckNo = d.CheckNo  CashReceipts  create index cbatchindex on CashReceiptsControl(Batch);create index mbatchindex on CashReceiptsMaster(Batch);create index mmasterkeyindex on CashReceiptsMaster(RefNo);create index dbatchindex on CashReceiptsDetail(Batch);create index dmasterkeyindex on CashReceiptsDetail(RefNo);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-7", "text": "c.Creator, c.Period, c.EndDate, c.Total, c.DefaultBank, c.PostPeriod, c.PostSeq, c.Company, c.SubmittedBy, c.SubmittedDate, m.TransDate, m.BankCode, m.TransComment, m.Posted, m.Seq as MasterSeq, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.RefNo, d.PKey, d.Seq as DetailSeq, d.Description, d.WBS1, d.WBS2, d.WBS3, d.Invoice, d.Account, d.Amount, d.Interest, d.TaxCode, d.TaxBasis, d.Retainer, d.CurrencyExchangeOverrideRate, d.SourceAmount, d.SourceExchangeInfo, d.LinkCompany from CashReceiptsControl c join CashReceiptsMaster m on c.Batch = m.Batch join CashReceiptsDetail d on m.Batch = d.Batch and m.RefNo = d.RefNo   EmployeeExpenses  create index cbatchindex on EmployeeExpensesControl(Batch);create index mbatchindex on EmployeeExpensesMaster(Batch);create index mmasterkeyindex on EmployeeExpensesMaster(RefNo);create index dbatchindex on EmployeeExpensesDetail(Batch);create index dmasterkeyindex on EmployeeExpensesDetail(RefNo);select c.Recurring, c.Creator, c.Period\", c.EndDate, c.Total,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-8", "text": "c.Creator, c.Period\", c.EndDate, c.Total, c.PostPeriod, c.PostSeq, c.DefaultTaxCode, c.AdvanceAmount, c.Company, c.DefaultCurrencyCode, c.DefaultTax2Code, c.SubmittedBy, c.SubmittedDate, m.Employee, m.ReportDate, m.ReportName, m.Posted, m.Seq as MasterSeq, m.AdvanceAmount, m.BarCode, m.DefaultCurrencyCode, m.PaymentExchangeOverrideMethod, m.PaymentExchangeOverrideDate, m.PaymentExchangeOverrideRate, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.PKey, d.Seq as DetailSeq, d.TransDate, d.WBS1, d.WBS2, d.WBS3, d.Account, d.Amount, d.Description, d.SuppressBill, d.TaxCode, d.NetAmount, d.TaxAmount, d.PaymentExchangeRate, d.PaymentAmount, d.PaymentExchangeInfo, d.CurrencyExchangeOverrideRate, d.CurrencyCode, d.Tax2Code, d.Tax2Amount, d.CompoundTax from EmployeeExpensesControl c join EmployeeExpensesMaster m on c.Batch = m.Batch join EmployeeExpensesDetail d on m.Batch = d.Batch and m.RefNo = d.RefNo   Invoices  create index cbatchindex on InvoicesControl(Batch);create index mbatchindex on InvoicesMaster(Batch);create index", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-9", "text": "index mbatchindex on InvoicesMaster(Batch);create index minvoiceindex on InvoicesMaster(Invoice);create index mwbs1index on InvoicesMaster(WBS1);create index mwbs2index on InvoicesMaster(WBS2);create index mwbs3index on InvoicesMaster(WBS3);create index dbatchindex on InvoicesDetail(Batch);create index dinvoiceindex on InvoicesDetail(Invoice);create index dwbs1index on InvoicesDetail(WBS1);create index dwbs2index on InvoicesDetail(WBS2);create index dwbs3index on InvoicesDetail(WBS3);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.RetTotal, c.DefaultTaxCode, c.PostPeriod, c.PostSeq, c.Company, c.SubmittedBy, c.SubmittedDate, m.TransDate, m.TransComment, m.Posted, m.Seq as MasterSeq, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.CurrencyExchangeOverrideRate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.Invoice, d.WBS1, d.WBS2, d.WBS3, d.PKey, d.Seq as DetailSeq, d.InvoiceSection, d.Account, d.Amount, d.RetAmount, d.TaxCode, d.TaxBasis, d.Retainer, d.LinkWBS1, d.LinkWBS2, d.LinkWBS3 from", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-10", "text": "d.LinkWBS2, d.LinkWBS3 from InvoicesControl c join InvoicesMaster m on c.Batch = m.Batch join InvoicesDetail d on m.Batch = d.Batch and m.Invoice = d.Invoice and m.WBS1 = d.WBS1 and m.WBS2 = d.WBS2 and m.WBS3 = d.WBS3   JournalEntries  create index cbatchindex on JournalEntriesControl(Batch);create index mbatchindex on JournalEntriesMaster(Batch);create index mrefnoindex on JournalEntriesMaster(RefNo);create index dbatchindex on JournalEntriesDetail(Batch);create index drefnoindex on JournalEntriesDetail(RefNo);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.PostPeriod, c.PostSeq, c.Company, c.DefaultCurrencyCode, c.SubmittedBy, c.SubmittedDate, m.TransDate, m.Type, m.Description as MasterDescription, m.Posted, m.Seq as MasterSeq, m.UpdateBilled, m.CurrencyCode, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.RefNo, d.PKey, d.Seq as DetailSeq, d.WBS1, d.WBS2, d.WBS3, d.Account, d.DebitAmount, d.CreditAmount, d.Description", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-11", "text": "d.Account, d.DebitAmount, d.CreditAmount, d.Description as DetailDescription, d.TaxCode, d.TaxType, d.TaxDebitAmount, d.TaxCreditAmount, d.SuppressBill, d.CurrencyExchangeOverrideRate, d.LinkCompany, d.Tax2Code, d.Tax2DebitAmount, d.Tax2CreditAmount, d.DebitAmountFunctionalCurrency, d.CreditAmountFunctionalCurrency, d.DebitAmountProjectCurrency, d.CreditAmountProjectCurrency, d.CompoundTax from JournalEntriesControl c join JournalEntriesMaster m on c.Batch = m.Batch join JournalEntriesDetail d on m.Batch = d.Batch and m.RefNo = d.RefNo   LaborAdjustments  create index cbatchindex on LaborAdjustmentsControl(Batch);create index mbatchindex on LaborAdjustmentsMaster(Batch);create index memployeeindex on LaborAdjustmentsMaster(Employee);create index dbatchindex on LaborAdjustmentsDetail(Batch);create index demployeeindex on LaborAdjustmentsDetail(Employee);select c.Recurring, c.Creator, c.Period, c.StartDate, c.EndDate, c.RegHrsTotal, c.OvtHrsTotal, c.RegAmtTotal, c.OvtAmtTotal, c.BillExtTotal, c.SpecialOvtHrsTotal, c.SpecialOvtAmtTotal, c.PostPeriod, c.PostSeq, c.Company, c.SubmittedBy, c.SubmittedDate, m.Posted, m.Seq as MasterSeq,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-12", "text": "c.SubmittedDate, m.Posted, m.Seq as MasterSeq, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.Employee, d.PKey, d.Seq as DetailSeq, d.TransDate, d.WBS1, d.WBS2, d.WBS3, d.LaborCode, d.RegHrs, d.OvtHrs, d.RegAmt, d.OvtAmt, d.BillExt, d.SuppressBill, d.TransComment, d.SpecialOvtHrs, d.SpecialOvtAmt, d.BillCategory, d.LinkCompany from LaborAdjustmentsControl c join LaborAdjustmentsMaster m on c.Batch = m.Batch join LaborAdjustmentsDetail d on m.Batch = d.Batch and m.Employee = d.Employee   MiscellaneousExpenses  create index cbatchindex on MiscellaneousExpensesControl(Batch);create index mbatchindex on MiscellaneousExpensesMaster(Batch);create index mrefnoindex on MiscellaneousExpensesMaster(RefNo);create index dbatchindex on MiscellaneousExpensesDetail(Batch);create index drefnoindex on MiscellaneousExpensesDetail(RefNo);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.PostPeriod, c.PostSeq, c.Company, c.DefaultCurrencyCode, c.SubmittedBy, c.SubmittedDate, m.TransDate, m.Description as MasterDescription, m.Posted, m.Seq as", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-13", "text": "m.Description as MasterDescription, m.Posted, m.Seq as MasterSeq, m.CurrencyCode, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.RefNo, d.PKey, d.Seq as DetailSeq, d.WBS1, d.WBS2, d.WBS3, d.Account, d.Amount, d.Description as DetailDescription, d.CurrencyExchangeOverrideRate from MiscellaneousExpensesControl c join MiscellaneousExpensesMaster m on c.Batch = m.Batch join MiscellaneousExpensesDetail d on m.Batch = d.Batch and m.RefNo = d.RefNo   PrintandReproductions  create index cbatchindex on PrintandReproductionsControl(Batch);create index mbatchindex on PrintandReproductionsMaster(Batch);create index mrefnoindex on PrintandReproductionsMaster(RefNo);create index dbatchindex on PrintandReproductionsDetail(Batch);create index drefnoindex on PrintandReproductionsDetail(RefNo);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.PostPeriod, c.PostSeq, c.Company, c.DefaultCurrencyCode, c.SubmittedBy, c.SubmittedDate, m.TransDate, m.Description as MasterDescription, m.Posted, m.Seq as MasterSeq, m.CurrencyCode, m.CurrencyExchangeOverrideMethod,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-14", "text": "as MasterSeq, m.CurrencyCode, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.RefNo, d.PKey, d.Seq as DetailSeq, d.Originals, d.Copies, d.WBS1, d.WBS2, d.WBS3, d.Account, d.Amount, d.Description as DetailDescription, d.CurrencyExchangeOverrideRate from PrintandReproductionsControl c join PrintandReproductionsMaster m on c.Batch = m.Batch join PrintandReproductionsDetail d on m.Batch = d.Batch and m.RefNo = d.RefNo   Timesheets  create index cbatchindex on TimesheetsControl(Batch);create index mbatchindex on TimesheetsMaster(Batch);create index memployeeindex on TimesheetsMaster(Employee);create index dbatchindex on TimesheetsDetail(Batch);create index demployeeindex on TimesheetsDetail(Employee);select c.Recurring, c.Creator, c.Period, c.StartDate, c.EndDate, c.RegHrsTotal, c.OvtHrsTotal, c.SpecialOvtHrsTotal, c.PostPeriod, c.PostSeq, c.Company, c.SubmittedBy, c.SubmittedDate, m.Posted, m.Seq as MasterSeq, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.Employee, d.PKey,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-15", "text": "m.ModUser, m.ModDate, d.Employee, d.PKey, d.Seq as DetailSeq, d.TransDate, d.WBS1, d.WBS2, d.WBS3, d.LaborCode, d.RegHrs, d.OvtHrs, d.TransComment, d.SpecialOvtHrs, d.BillCategory, d.Locale, d.RegAmt, d.OvtAmt, d.SpecialOvtAmt, d.ExchangeInfo, d.RegAmtProjectCurrency, d.OvtAmtProjectCurrency, d.SpecialOvtAmtProjectCurrency, d.ProjectExchangeInfo, d.BillExt, d.OvtPct, d.SpecialOvtPct, d.Rate, d.OvtRate, d.SpecialOvtRate, d.RateProjectCurrency, d.OvtRateProjectCurrency, d.SpecialOvtRateProjectCurrency from TimesheetsControl c join TimesheetsMaster m on c.Batch = m.Batch join TimesheetsDetail d on m.Batch = d.Batch and m.Employee = d.Employee   Units  create index cbatchindex on UnitsControl(Batch);create index mbatchindex on UnitsMaster(Batch);create index mmasterkeyindex on UnitsMaster(Unit);create index dbatchindex on UnitsDetail(Batch);create index dmasterkeyindex on UnitsDetail(Unit);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.PostPeriod, c.PostSeq, c.Company, c.SubmittedBy, c.SubmittedDate, m.Posted,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-16", "text": "c.Company, c.SubmittedBy, c.SubmittedDate, m.Posted, m.Seq as MasterSeq, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.Unit, d.PKey, d.Seq as DetailSeq, d.TransDate, d.WBS1, d.WBS2, d.WBS3, d.TableNo, d.Account, d.Description, d.Quantity, d.CurrencyExchangeOverrideRate from UnitsControl c join UnitsMaster m on c.Batch = m.Batch join UnitsDetail d on m.Batch = d.Batch and m.Unit = d.Unit   UnitsbyProject  create index cbatchindex on UnitsbyProjectControl(Batch);create index mbatchindex on UnitsbyProjectMaster(Batch);create index mmasterkeyindex on UnitsbyProjectMaster(RefNo);create index dbatchindex on UnitsbyProjectDetail(Batch);create index dmasterkeyindex on UnitsbyProjectDetail(RefNo);select c.Recurring, c.Creator, c.Period, c.EndDate, c.Total, c.PostPeriod, c.PostSeq, c.Company, c.SubmittedBy, c.SubmittedDate, m.Posted, m.Seq as MasterSeq, m.CurrencyExchangeOverrideMethod, m.CurrencyExchangeOverrideDate, m.Status, m.AuthorizedBy, m.RejectReason, m.ModUser, m.ModDate, d.RefNo, d.PKey,", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "b3cdcc7cf018-17", "text": "m.ModUser, m.ModDate, d.RefNo, d.PKey, d.Seq as DetailSeq, d.TransDate, d.WBS1, d.WBS2, d.WBS3, d.Unit, d.TableNo, d.Account, d.Description, d.Quantity, d.CurrencyExchangeOverrideRate from UnitsbyProjectControl c join UnitsbyProjectMaster m on c.Batch = m.Batch join UnitsbyProjectDetail d on m.Batch = d.Batch and m.RefNo = d.RefNo", "source": "../../raw_kb/article/deltek_vision_connector/index.html", "title": "Deltek Vision Connector"}, {"objectID": "318f062f7784-0", "text": "Title\n\nDemandMetrics Connector\n\nArticle Body\n\nImportant:\u00a0Demandsphere\u00a0has changed the name of\u00a0GinzaMetrics to DemandMetrics. DemandMetrics is still your one-stop-shop for SEO, content, and competitor analytics. There should be no change to the Login User ID, Password, API Key, or Site Key.", "source": "../../raw_kb/article/demandmetrics_connector/index.html", "title": "DemandMetrics Connector"}, {"objectID": "318f062f7784-1", "text": "Intro\nDemandMetrics is a business intelligence and SEO platform designed to help businesses determine, design and implement changes and improvements to their site structure. Use Domo's DemandMetrics connector to manage and retrieve search rankings, optimization recommendations, social signals and web analytics data. To learn more about the DemandMetrics\u00a0API, visit their page (https://www.ginzametrics.com/api/api-documentation/).\nYou connect to your DemandMetrics account in the Data Center. This topic discusses the fields and menus that are specific to the DemandMetrics connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your DemandMetrics account and create a DataSet, you must have a DemandMetrics API key. If you do not have an API key, you can request one by emailing support@ginzametrics.com.\u00a0\nConnecting to Your DemandMetrics Account\nThis section enumerates the options in the Credentials and Details panes in the DemandMetrics Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your DemandMetrics account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAPI KeyEnter your DemandMetrics API key.\u00a0If you do not have an API key, you can request one by emailing support@ginzametrics.com.\u00a0\nOnce you have entered valid DemandMetrics credentials, you can use the same account any time you go to create a new DemandMetrics DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane", "source": "../../raw_kb/article/demandmetrics_connector/index.html", "title": "DemandMetrics Connector"}, {"objectID": "318f062f7784-2", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/demandmetrics_connector/index.html", "title": "DemandMetrics Connector"}, {"objectID": "318f062f7784-3", "text": "MenuDescriptionReportSelect the DemandMetrics report you want to run.\u00a0The following reports are available:AccountsReturns a list of sites in your account, along with metadata such as global keys and conversion events.Competitors RankingReturns competitors' keyword rankings over time.Integrated Keyword PerformanceReturns statistics about keyword rankings for a given search engine.\u00a0Keyword Ranking TrendsReturns information about your keyword rankings over time.Report TypeSelect how your report data is to be broken down. If you select\u00a0Aggregate, the data in the report is aggregated. If you select\u00a0Granular, the data is broken down by the date grain you have selected in the\u00a0Granular\u00a0menu.GranularitySelect the date granularity for your report data. For example, if you select\u00a0Weekly, the data will be broken down by week.\u00a0Site Global KeyEnter the global key for the site you want to retrieve data for.Search Engine Global KeyEnter the global key for the search engine you want to retrieve data for.\u00a0Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for", "source": "../../raw_kb/article/demandmetrics_connector/index.html", "title": "DemandMetrics Connector"}, {"objectID": "318f062f7784-4", "text": "range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/demandmetrics_connector/index.html", "title": "DemandMetrics Connector"}, {"objectID": "318f062f7784-5", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits I should be aware of?\nSome features of the DemandMetrics dashboard may not be available through the API.", "source": "../../raw_kb/article/demandmetrics_connector/index.html", "title": "DemandMetrics Connector"}, {"objectID": "c42a6cec3dae-0", "text": "TitleDenodo ConnectorArticle BodyIntro\nDenodo is a platform for data virtualization. Denodo enables real-time access to integrated data across an organization's diverse data sources without the need for data replication. Domo's Denodo connector allows you to import any data from Denodo into Domo for in-depth analysis and visualization. To learn more about Denodo Platform, visit their website, Denodo Platform Overview.\nThe Denodo connector is a \"Database\" connector, meaning it retrieves data from different data sources.\u00a0\nYou connect to your Denodo account in the Data Center. This topic discusses the fields and menus that are specific to the Denodo connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesPulling data files from Denodo into Domo.Average Implementation TimeThe time taken to configure depends on the amount of data that is uploaded to Denodo.Ease of Use (on a 1-to-10 scale with 1 being easiest)2\nPrerequisites\nTo connect to your Denodo account and create a DataSet, you must have the following:\nDenodo hostname you need to use to access\u00a0your Denodo account.\u00a0Denodo port number you need to use to access your Denodo account.Denodo Database NameDenodo username.Denodo password.\nConnecting to Your Denodo Account\nThis section enumerates the options in the\u00a0Credentials and Details panes in the Denodo Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane", "source": "../../raw_kb/article/denodo_connector/index.html", "title": "Denodo Connector"}, {"objectID": "c42a6cec3dae-1", "text": "Credentials Pane\nThe Domo Denodo connector uses Denodo Hostname, Denodo Port number, Denodo database name, Denodo username, and Denodo password to connect within Domo. Click Connect (or select\u00a0Add Account if\u00a0you have an existing\u00a0Denodo account in Domo)\u00a0to open the Denodo login screen where you can enter the email address and password associated with your Denodo account. Once you have entered valid Denodo credentials, you can use the same account any time you go to create a new\u00a0Box DataSet. You can manage connector accounts in the Accounts tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nThis pane includes the following fields:\nFieldDescriptionHostEnter the Denodo hostname. For example,\u00a0myserver.domain.com.PortEnter the Denodo port number.Database Name\u00a0Enter the Denodo Database Name.UsernameEnter the Denodo Username.PasswordEnter the Denodo password.\nDetails Pane\nThis pane includes the following fields and menus:\nMenuDescriptionReportSelect the report you want to run. The following reports are available:Custom QueryEnter the query to execute.Query HelperSelect a view and fields to auto-generate your query.User QueryEnter Denodo VQL query to execute.Fetch SizeEnter the fetch-size for memory performance. If it is blank, the default value will be 1000.Query TimeoutEnter the query timeout value in minute(s). If it is blank, the default value set by Denodo will be 15 minutes.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector. \u00a0\nFAQ\nWhen can I use this connector?\nAllow real-time access to integrated data across an organization's diverse data sources without the need for data replication by using the connector.\nHow often can the data be updated?", "source": "../../raw_kb/article/denodo_connector/index.html", "title": "Denodo Connector"}, {"objectID": "c42a6cec3dae-2", "text": "How often can the data be updated?\nAs often as needed.\nWhat is the default fetch-size for memory performance?\nThe default value is 1000.\nWhat is the default query timeout value?\nThe default value set by Denodo is 15 minutes.\nWhat kind of credentials do I need to power up this connector?\nYou need to obtain the hostname, port number, database account, account username, and account password.", "source": "../../raw_kb/article/denodo_connector/index.html", "title": "Denodo Connector"}, {"objectID": "6218721e580e-0", "text": "Title\n\nDependency Gantt Chart\n\nArticle Body\n\nIntro\nDependency Gantt charts are used to illustrate project schedules. Each task in a project is represented as a horizontal bar with a start date and end date. Tasks are represented along the vertical axis, and dates are represented along the horizontal axis. You can break tasks down by project, in which case each project is shown in a different color and appears as a legend item. However, projects are optional in Gantt charts.\u00a0The Dependency Gantt chart handles changes in dates for you and gives many new display options to communicate project timelines.\nPowering Dependency Gantt charts\nDependency Gantt charts require three columns or rows of data from your DataSet\u2014one for tasks, one with start dates for each task, and one for end dates for each task.\u00a0If you want, you can also add an additional column containing project category data to a Gantt chart. Adding project category data to a Gantt chart causes the bars for each category to appear in different colors, indicating multiple projects. Also, a legend is added to the chart showing you which project category is which.\nFor information about value, category, and series data, see Understanding Chart Data.\nIn the Analyzer, you choose the columns containing the data for your Gantt chart. For more information about choosing data columns, see\u00a0Applying DataSet Columns to Your Chart.\nFor more information about formatting charts in the Analyzer, see\u00a0Visualization Card Building Part 2: The Analyzer.\nThe following graphic shows how the data looks when displayed in the Card Details view of the chart:\n\nThe following graphic shows how the data looks inside of the Analyzer view of the chart:", "source": "../../raw_kb/article/dependency_gantt_chart/index.html", "title": "Dependency Gantt Chart"}, {"objectID": "6218721e580e-1", "text": "The following graphic shows how the data looks inside of the Analyzer view of the chart:\n\nCustomizing Dependency Gantt charts\nYou can customize the appearance of a Dependency Gantt chart\u00a0by editing its Chart Properties. For information about all chart properties, see\u00a0 Chart Properties.\nUnique properties of Dependency Gantt charts include the following:\nPropertyDescriptionExample (s)Time Scale (X) > Show LabelHides or shows time scale labels. Labels are shown by default.Hover Text Settings > Text\u00a0Lets you replace the default hover text with your own custom text.\u00a0You can insert any of a variety of macros to reference the start date, percent of completion, category, etc. For more information, see\u00a0Formatting Data Label and Hover Text in Your Chart.\nIn the example at right, the user replaced the default hover text with the following macro: %_PERCENT_COMPLETE% complete.", "source": "../../raw_kb/article/dependency_gantt_chart/index.html", "title": "Dependency Gantt Chart"}, {"objectID": "4aec546645ac-0", "text": "Title\n\nDeploying Apps from the Appstore\n\nArticle Body\n\nIn the Appstore, you can deploy an app to your Domo instance. If any installations of the app have already been deployed to your company Domo instance, you are given the option of requesting access to an existing version or deploying a new version of your own. \u00a0\u00a0\nWhen you deploy an app, Domo first installs the Cards with sample data. Cards with sample data appear with a label reading Demo Data to indicate they are not yet live.\n\nYou can then take your time in looking over the Cards to make sure this app is what you need. When you are ready, you can power up the Cards in the app with live data by connecting to a third-party Connector account (for Dashboard apps) or DataSet with matching columns (for all other kinds of apps), or you can assign another user to power it up.\nVideo - Advanced Deploy for All Apps\n\nVideo - Appstore Column Mapping\n\n\u00a0\n\u00a0\u00a0\u00a0\nTo\u00a0deploy an app to Domo,\nClick Appstore.Identify the app you want to add.You can use the various filter and sorting options to locate the desired app. For more information, see Appstore Layout.Click the app to open its Details view.In the Details view, review the requirements and notes for the\u00a0app.Click Try It.\u00a0\n\n\n \n\nNote:\u00a0Users with a \"Participant\" default security role\u00a0(or users with a custom role that does not include the \"Use Appstore\" grant) do not see the\u00a0Try It\u00a0button, as they are not able to deploy Appstore apps. Instead, they will see a\u00a0Request\u00a0button. Clicking this button opens a dialog in which the user can reach out to his/her MajorDomo\u00a0and request that the app be installed. For more information about default security roles, see\u00a0Managing Custom Roles.", "source": "../../raw_kb/article/deploying_apps_from_the_appstore/index.html", "title": "Deploying Apps from the Appstore"}, {"objectID": "4aec546645ac-1", "text": "(Conditional) If any versions of this app have already been installed, you are given the option of requesting access to an existing version or adding a new version. For information about requesting access to an existing version, see Requesting Access to an app; otherwise click Add New then proceed to the next step.Give your app a unique name, then click Save & Finish.A new Domo page is added for the app. All Cards for the app are installed with sample data and appear with a blue bar at the top with text reading \"Example\" to indicate that they are not yet live. You are also given the option to power up this app yourself or assign it to a data specialist.\u00a0Review the app(s) in the page to make sure\u00a0these are the metrics you want to show.Do one of the following:To assign the powering of this app to another user...Click Assign to Data Specialist.The Assign to data owner\u00a0dialog appears, with fields for selecting a data specialist and entering a custom message.\u00a0In the\u00a0Select data specialist\u00a0field, enter", "source": "../../raw_kb/article/deploying_apps_from_the_appstore/index.html", "title": "Deploying Apps from the Appstore"}, {"objectID": "4aec546645ac-2", "text": "custom message.\u00a0In the\u00a0Select data specialist\u00a0field, enter the name of the user you want to assign to power this app.\u00a0(Optional) Enter an email message to be sent to the data specialist\u00a0if you want.Click Send.The user you have assigned will receive an email notification\u00a0informing him or her about the assignment.\u00a0To power up the app yourself...Click\u00a0Do It Yourself.The blue bar updates to show Connect buttons for all DataSets powering the app, along with the total number of DataSets and Cards associated with this app. For example, the app in the following screenshot is powered off of 2 DataSets and contains 11 Cards, and both DataSets have Connect\u00a0buttons so you can power them up individually.Click Connect.What happens next depends on the type of app you are deploying. If the app is a Dashboard app, to power a Card you will be asked to connect to one of Domo's third-party connectors, usually by entering credentials and/or other information.If the app is a Card Builder app or custom app, you will be asked to select", "source": "../../raw_kb/article/deploying_apps_from_the_appstore/index.html", "title": "Deploying Apps from the Appstore"}, {"objectID": "4aec546645ac-3", "text": "or custom app, you will be asked to select an existing DataSet in Domo with column names matching those in the app.(Conditional) If you are deploying a Dashboard app, follow the steps in the wizard for connecting to data.\u00a0The specific steps in the wizard differ depending on the Connector. In most cases you are asked to select a Connector account, or to add a new account if\u00a0one doesn't exist yet. For some connectors (such as NOAA) you are asked to enter other information, such as your zip code. You can also assign another user to power the app if you want.\u00a0(Conditional) If you are deploying a Card Builder or custom app...In pane 1,\u00a0select the DataSet you want to use to power up the app.Drag and drop DataSet\u00a0columns from pane 1 onto the matching columns in pane 2.\u00a0If the data type matches for the two columns, a blue outline will appear around the column in pane 2 when you drag the column on top of it.If the data type does not match, no outline appears. If you", "source": "../../raw_kb/article/deploying_apps_from_the_appstore/index.html", "title": "Deploying Apps from the Appstore"}, {"objectID": "4aec546645ac-4", "text": "type does not match, no outline appears. If you attempt to drop a column onto a non-matching column anyway, no match occurs.Once you have matched all columns in pane 2, click the orange Connect button.The Cards in the app now populate with live data from your DataSet.\u00a0You can go in and update the DataSet and/or columns populating the Cards in this app anytime by opening the app, selecting Do It Yourself, clicking\u00a0Change, then making the desired changes.", "source": "../../raw_kb/article/deploying_apps_from_the_appstore/index.html", "title": "Deploying Apps from the Appstore"}, {"objectID": "2bfa1e05c4bb-0", "text": "Title\n\nDeprecated Chart Properties\n\nArticle Body", "source": "../../raw_kb/article/deprecated_chart_properties/index.html", "title": "Deprecated Chart Properties"}, {"objectID": "2bfa1e05c4bb-1", "text": "The following chart properties have been deprecated. Note that for properties you applied by checking a box, if you applied the property before it became deprecated, the property still appears in your Chart Properties and is checked. However, if you uncheck the box and then resave the chart, the option is no longer available.", "source": "../../raw_kb/article/deprecated_chart_properties/index.html", "title": "Deprecated Chart Properties"}, {"objectID": "2bfa1e05c4bb-2", "text": "PropertyDescriptionLegend PositionDetermined whether legends appear above or below certain charts, if at all.\u00a0Zero Line Width (for Bubble and Scatter Plot graphs)Determined the width of the \"zero line\" on most single-axis charts, in pixels.Show Data LabelTurned on data labels for most chart types.Show One LabelTurned on a \"total\" label for vertical Stacked Bar charts. This has been replaced by the\u00a0Show Total Label\u00a0property, which is available for vertical as well as horizontal Stacked Bar charts.\u00a0Disable InteractivityTurned off interactivity in many chart types.Title PositionDetermined whether value scale labels in some types of Vertical Bar charts appeared to the left of the chart or above it.Use Legacy Funnel\u00a0When this box was checked, the legacy version of the Funnel graph was used. This version showed both value and percent columns in the legend, and percentages for each layer in the legend were derived from the total value of the chart,\u00a0not from the previous layer.Allow Wide BarsDetermined whether the bar in a Vertical Bar graph with a single bar used the full width of the available", "source": "../../raw_kb/article/deprecated_chart_properties/index.html", "title": "Deprecated Chart Properties"}, {"objectID": "2bfa1e05c4bb-3", "text": "a single bar used the full width of the available space. If this box was checked, the bar took up all of the space in the graph. Otherwise, a normal-sized bar appeared in the center of the chart space.\u00a0Allow Tall BarsDetermined whether the bar in a Horizontal Bar graph with a single bar used the full height of the available space. If this box was checked, the bar took up all of the space in the graph. Otherwise, a normal-sized bar appeared in the center of the chart space.\u00a0Series 1-16Let you specify custom colors for up to sixteen series in most kinds of graphs. With the Analyzer release in August 2017, these properties have been replaced by the \"Color Rules\" functionality.\u00a0For more information, see\u00a0Setting Color Rules for a Chart .Minimum ValueLet you set the minimum value on a scale marker range. Replaced by \"Lower Value.\"\u00a0Maximum ValueLet you set the maximum value on a scale marker range. Replaced by \"Upper Value.\"\u00a0Show LinesDetermined\u00a0whether red dotted lines demarcated\u00a0the\u00a0minimum\u00a0and maximum values of", "source": "../../raw_kb/article/deprecated_chart_properties/index.html", "title": "Deprecated Chart Properties"}, {"objectID": "2bfa1e05c4bb-4", "text": "dotted lines demarcated\u00a0the\u00a0minimum\u00a0and maximum values of your scale marker range.Fill OutliersDetermined whether outliers were filled in when scale markers were set.", "source": "../../raw_kb/article/deprecated_chart_properties/index.html", "title": "Deprecated Chart Properties"}, {"objectID": "38353e5adfa4-0", "text": "TitleDetermining When to Use Cards vs. AppsArticle BodyUse the following flowchart to help determine whether your data is better suited for a card or an app. To download this PDF to your computer, click here.", "source": "../../raw_kb/article/determining_when_to_use_cards_vs_apps/index.html", "title": "Determining When to Use Cards vs. Apps"}, {"objectID": "5aacb01e5b30-0", "text": "TitleDeveloper Portal OverviewArticle BodyRefer to the following video for an overview of the Developer portal.", "source": "../../raw_kb/article/developer_portal_overview/index.html", "title": "Developer Portal Overview"}, {"objectID": "02a1092ff5ae-0", "text": "TitleDHL Writeback ConnectorArticle BodyIntro\nDHL\u00a0is the global market leader in the logistics industry.\u00a0DHL\u00a0commits its expertise in\u00a0the International express deliveries; global freight forwarding by air, sea, road and rail; warehousing solutions from packaging, to repairs, to storage; mail deliveries worldwide; and other customized logistic services. Use Domo's DHL Writeback connector to get detailed tracking status information for your tracking numbers. To learn more about the DHL API, visit their page https://www.dhl.com/global-en/home/our-divisions/ecommerce/integration/api.html.\nYou connect to your DHL Writeback account in the Data Center. This topic discusses the fields and menus that are specific to the DHL Writeback connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\n\n\n\n\nNote: The owner of a writeback dataset must also be an owner or co-owner of the input dataset.", "source": "../../raw_kb/article/dhl_writeback_connector/index.html", "title": "DHL Writeback Connector"}, {"objectID": "02a1092ff5ae-1", "text": "Prerequisites\nTo connect to your DHL Writeback account and get detailed tracking status information for your tracking numbers, you must have the following:\nDHL API key (Consumer Key)\nPlease find below the steps to get the DHL API key:\n1) Create anActive customer accountwithDHL Express.2) Select theAPI service typefrom the API dropdown and then click on the Invoice push API (https://developer.dhl.com/api-catalog?f%5B0%5D=api_catalog_service%3A40).3) In the Authentication, section click on theMy app screenlink and create a new app.\nTo view your API credentials :\nFrom the My Apps screen, click on the name of your app.The Details screen appears.If you have access to more than one API, click the name of the relevant API.\u00a0Note: \u00a0The APIs are listed under the \u201cCredentials\u201d section.Click the Show link below the asterisks that are hiding the Consumer Key.\u00a0The Consumer Key appears.\u00a0\nConnecting to Your DHL Writeback Account\nThis section enumerates the options in the Credentials and Details panes in the DHL Writeback Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains the credentials required to connect to your DHL Writeback account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionDHL API KeyEnter your Consumer Key\nYou can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains the input fields that\u00a0get detailed tracking status information for your tracking numbers.", "source": "../../raw_kb/article/dhl_writeback_connector/index.html", "title": "DHL Writeback Connector"}, {"objectID": "02a1092ff5ae-2", "text": "This pane contains the input fields that\u00a0get detailed tracking status information for your tracking numbers.\nMenuDescriptionInput DataSet IDEnter your Domo dataset ID (GUID) located in the dataset url. Example-https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settingsTracking Number Column NameEnter the column name of the tracking numbers.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/dhl_writeback_connector/index.html", "title": "DHL Writeback Connector"}, {"objectID": "77e9118ea3d7-0", "text": "Title\n\nDial800 Connector\n\nArticle Body", "source": "../../raw_kb/article/dial800_connector/index.html", "title": "Dial800 Connector"}, {"objectID": "77e9118ea3d7-1", "text": "Intro\nDial800 is a call performance marketing and communication tool. To learn more about the Dial800 API, visit their page (https://www.dial800.com/solutions/call-tracking/).\nYou connect to your Dial800 account in the Data Center. This topic discusses the fields and menus that are specific to the Dial800 connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Dial800 account and create a DataSet, you must have a Dial800 username and password.\nConnecting to Your Dial800 Account\nThis section enumerates the options in the Credentials and Details panes in the Dial800 Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Dial800 account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter your Dial800 username.PasswordEnter your Dial800 password.\nOnce you have entered valid Dial800 credentials, you can use the same account any time you go to create a new Dial800 DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/dial800_connector/index.html", "title": "Dial800 Connector"}, {"objectID": "77e9118ea3d7-2", "text": "MenuDescriptionReportSelect the Dial800 report you want to run.\u00a0The following reports are available:AnalyticsReturns Dial800 analytics information.CallersReturns a list of callers.CallsReturns a list of calls.Client SearchesReturns a list of client searches.Client TargetReturns a list of client targets.CountriesReturns a list of countries.DNIsReturns a list of DNIS.DNISesReturns a list of DNISes.DomainReturns a list of domains.Legacy DNIsReturns a list of legacy DNIs.MessagesReturns a list of messages.OwnerReturns a list of owners.ProfilesReturns a list of profiles.ReportsReturns a list of reports.Routing ProfilesReturns a list of routing profiles.TagsReturns a list of tags.TargetsReturns a list of targets.Client IDSelect the ID of the client you want to retrieve information for.Search IDSelect the search ID you want to retrieve information for.Tag TypeSelect the tag type you want to retrieve information for.DNISEnter the DNIS you want to retrieve information for.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/dial800_connector/index.html", "title": "Dial800 Connector"}, {"objectID": "551c06ac445b-0", "text": "TitleDialpad Enterprise ConnectorArticle BodyIntro\nDialpad Enterprise is a cloud-based phone system, allowing you to make and receive calls and messages. Dialpad provides you an all-in-one workspace with a great connectivity. Experience truly unified communications built-in-AI, scale faster, deploy your global teams together, and get enterprise class security. Use this connector to receive data about your phone system usage, including users, offices, departments, and more. To learn more about the Dialpad API, visit their page (https://developers.dialpad.com/docs).\nThe Dialpad Enterprise Connector is a \"Cloud App\" Connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the Connector page for this and other Cloud App Connectors by clicking Cloud App in the toolbar at the top of the window.\nYou connect to your Dialpad Enterprise account in the Data Center. This topic discusses the fields and menus that are specific to the Dialpad Enterprise Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Dialpad Enterprise account and create a DataSet, you must have your Dialpad API Key associated with your Enterprise account.\nConnecting to Your Dialpad Enterprise Account\nThis section enumerates the options in the Credentials and Details panes in the Dialpad Enterprise Connector page. The components of the other panes in this page, Scheduling and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Dialpad Enterprise\u00a0account. The following table describes what is needed for each field:\nFieldDescriptionAPI KeyEnter your Dialpad API Key for your Enterprise account.", "source": "../../raw_kb/article/dialpad_enterprise_connector/index.html", "title": "Dialpad Enterprise Connector"}, {"objectID": "551c06ac445b-1", "text": "FieldDescriptionAPI KeyEnter your Dialpad API Key for your Enterprise account.\nOnce you have entered valid Dialpad Enterprise credentials, you can use the same account any time you go to create a new Dialpad Enterprise DataSet. You can manage Connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary Reports menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/dialpad_enterprise_connector/index.html", "title": "Dialpad Enterprise Connector"}, {"objectID": "551c06ac445b-2", "text": "MenuDescriptionReportSelect the Dialpad Enterprise report you want to run. The following reports are available:OfficesGet a list of all offices.Call CentersList all the call centers in an Office.OperatorsList all the pperators in a given call center. Each entry contains a key parameter that uniquely identifies each operator.StatsGet summary stats and call/text logs.OfficeSelect an office.Target TypeSelect the target Type for an ID.Call CenterSelect a call center.Stat TypeSelect a stat type.Export TypeSelect whether you want to export stats or records.Today's DataSelect 'Yes' to retrieve the data for the current day. Note that, you cannot combine Today's Data with the historical data. Select 'NO' to see date range options.Date SelectionSelect the date format for your data.Single DateSelect whether the report data is for a specific date or for a relative number of days back from today.Specific DateSelect the specific date using the date selector.Relative DateEnter the number of days back that you would like to get data for in the\u00a0Days Back\u00a0field. Specify either today or 0, yesterday or 1, or today-7 or 7 to get data for 7 days into the past.Date RangeSelect the specific or relative date range.Start Date - SpecificSelect\u00a0the first date in your date range using the date selector.End Date - SpecificSelect the last date in your date range\u00a0using the date selector.Start Date - RelativeEnter the number of days back that you would like to get data from (start day). Combine with\u00a0End Date\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Start Date\u00a0and\u00a05\u00a0for\u00a0End Date, the report would contain data for\u00a010 days ago up until\u00a05 days ago.End Date - RelativeEnter the number of days back that you would like to get data to (end day). Combine with\u00a0Start Date\u00a0to create a range of represented days.", "source": "../../raw_kb/article/dialpad_enterprise_connector/index.html", "title": "Dialpad Enterprise Connector"}, {"objectID": "551c06ac445b-3", "text": "For example, if you entered\u00a010\u00a0for\u00a0Start Date\u00a0and\u00a05\u00a0for\u00a0End Date, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see Adding a DataSet Using a Data Connector.\nFAQs\nWhat version of the Dialpad API does this Connector use?\nThis Connector uses version 2 of the Dialpad API (https://dialpad.com/api/v2).\nWhich endpoint(s) does each report call in this Connector?\nReport NameEndpoint URL(s)Offices/officesCall Centers/offices/{office_id}/callcentersOperators/callcenters/{callcenter_id}/operatorsStats/stats\nWhat kind of credentials do I need to power up this Connector?\nYou need your Dialpad API Key associated with your Enterprise account.\nAre there any API limits I should be aware of?\nNo\nHow often can the data be updated?\nAs often as needed.\nTroubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/dialpad_enterprise_connector/index.html", "title": "Dialpad Enterprise Connector"}, {"objectID": "37a50536694c-0", "text": "Title\n\nDisabling or Deleting a DataFlow\n\nArticle Body\n\nYou can disable or delete a DataFlow in Domo. Disabling it causes any DataSets powered by the DataFlow to stop working (along with any KPI cards powered by those DataSets) but keeps the DataFlow itself intact. Deleting it disables the DataSets and cards and also removes it from Domo.\nTo disable or delete a DataFlow,\nIn Domo, click\u00a0Data\u00a0in the toolbar at the top of the screen.Click \u00a0in the left-hand navigation pane.Locate the DataFlow you want to disable or delete.\tYou can use the filter options to narrow down the DataFlows that appear in the list.Mouse over the desired DataFlow then click\u00a0\u00a0to display a list of options.(Conditional) To disable the DataFlow, click Disable. To delete the DataFlow, click Delete.\nYou can also access this option from the Details view for the DataFlow.\nTo delete multiple DataFlows,\nIn Domo, click\u00a0Data\u00a0in the toolbar at the top of the screen.Click\u00a0\u00a0in the left-hand navigation pane.\tThe\u00a0DataFlows\u00a0tab opens.Locate one of the DataFlows you want to delete.\tFor more information about searching and filtering DataFlows in the Data Center, see\u00a0Data Center Layout.Mouse over the row for the DataFlow and click the circled checkmark that pops up over the connector icon.\u00a0  In the blue bar that appears at the top of the screen, click the\u00a0\u00a0icon.Select\u00a0Delete.  \nAll of the DataFlows\u00a0you have selected should now be deleted.", "source": "../../raw_kb/article/disabling_or_deleting_a_dataflow/index.html", "title": "Disabling or Deleting a DataFlow"}, {"objectID": "ef0de16d019f-0", "text": "Title\n\nDisabling Username Autocomplete in the Login Screen\n\nArticle Body\n\nNote: This feature is available on demand.\u00a0\nTo request that this feature be enabled,\ncontact Technical Support.\u00a0For information on how to contact Support, please see Getting Help.reach out to your Domo Customer Success Manager or Technical Consultant.\nDepending on the feature, you may be required to complete training before you can use the feature.\n\n\n\n\nYou can disable username autocomplete in the Domo login screen by hiding the \"Remember me\" checkbox. When this box is disabled, users must always log in manually. You do this in More > Admin\u00a0> Authentication > Authentication.\u00a0You cannot access this feature unless you have\u00a0an \"Admin\" default security role or a custom role in which the \"Manage All Company Settings\" privilege has been enabled. For more information about default security roles, see\u00a0Managing Roles.\nTo disable username autocomplete,\nClick\u00a0More\u00a0> Admin\u00a0>\u00a0Authentication.\u00a0In the Authentication\u00a0sub-tab, check Disable username autocomplete.", "source": "../../raw_kb/article/disabling_username_autocomplete_in_the_login_screen/index.html", "title": "Disabling Username Autocomplete in the Login Screen"}, {"objectID": "eba82c2ee27c-0", "text": "Title\n\nDistrict M Connector\n\nArticle Body", "source": "../../raw_kb/article/district_m_connector/index.html", "title": "District M Connector"}, {"objectID": "eba82c2ee27c-1", "text": "Intro\nDistrict M is a digital media company that offers programmatic solutions to advertisers and publishers. To learn more about the District M API, visit their page (https://www.districtm.net/).\nYou connect to your District M account in the Data Center. This topic discusses the fields and menus that are specific to the District M connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your District M account and create a DataSet, you must have the following:\nA District M client IDA District M client secret\nThese credentials are provided upon account creation.\nConnecting to Your District M Account\nThis section enumerates the options in the Credentials and Details panes in the District M Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your District M account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionClient IDEnter your District M client ID.Client SecretEnter your District M client secret.\nOnce you have entered valid District M credentials, you can use the same account any time you go to create a new District M DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains two primary menus, Reports and Fields, along with various other options for configuring your report.", "source": "../../raw_kb/article/district_m_connector/index.html", "title": "District M Connector"}, {"objectID": "eba82c2ee27c-2", "text": "MenuDescriptionReportSelect the District M report you want to run. Currently only one report is available.QueryLets you build out a customized report by selecting any of a number of available fields.FieldsSelect all of the fields you want to appear in your customized report. You can select as many fields as you want.Filter by Date?Select whether you want to retrieve results for a specified time frame.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.", "source": "../../raw_kb/article/district_m_connector/index.html", "title": "District M Connector"}, {"objectID": "eba82c2ee27c-3", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Placement ID (Optional)Enter a placement ID if you want to filter by placement ID.Domain Name (Optional)Enter a domain name if you want to filter by domain name.Zone Name (Optional)Enter a zone name if you want to filter by zone name.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/district_m_connector/index.html", "title": "District M Connector"}, {"objectID": "007dc955a38b-0", "text": "Title\n\nDocebo Connector\n\nArticle Body", "source": "../../raw_kb/article/docebo_connector/index.html", "title": "Docebo Connector"}, {"objectID": "007dc955a38b-1", "text": "Intro\nDocebo is a SaaS platform that offers a learning portal for companies and their employees, partners and customers. Use Domo's Docebo connector to retrieve\u00a0reports for courses, enrollment, learning plans, and more. To learn more about the Docebo API, visit their page (https://www.docebo.com/lms-docebo-ap...y-integration/).  \nYou connect to your Docebo account in the Data Center. This topic discusses the fields and menus that are specific to the Docebo connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Docebo account and create a DataSet, you must have the following:\nYour Docebo username and password.The API base URL for your Docebo software. For example:\u00a0https://doceboapi.docebosaas.comA Docebo client IDA Docebo client secret\nFor information about generating your client ID and client secret, visit\u00a0https://www.docebo.com/knowledge-base/how-to-activate-and-manage-the-sso-and-api-app/.\u00a0Note that when configuring the app, the\u00a0Grant Type\u00a0must\u00a0be set to\u00a0Resource Owner Password Credentials.\u00a0\nConnecting to Your Docebo Account\nThis section enumerates the options in the Credentials and Details panes in the Docebo Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Docebo account. The following table describes what is needed for each field:", "source": "../../raw_kb/article/docebo_connector/index.html", "title": "Docebo Connector"}, {"objectID": "007dc955a38b-2", "text": "FieldDescriptionUsernameEnter your Docebo username.PasswordEnter your Docebo password.API URLEnter the API base URL for your Docebo software. Example: https://doceboapi.docebosaas.comClient IDEnter your Docebo\u00a0client ID.Client SecretEnter your Docebo client secret.\nOnce you have entered valid Docebo credentials, you can use the same account any time you go to create a new Docebo DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/docebo_connector/index.html", "title": "Docebo Connector"}, {"objectID": "007dc955a38b-3", "text": "MenuDescriptionReportSelect the Docebo report you want to run.\u00a0The following reports are available:Courses DetailsReturns details about the courses.Course ListReturns a list of Docebo courses.Course SessionsReceives data about Docebo Course Sessions.EcommerceReceives data about Docebo Ecommerce reports.EnrollmentReceives data about Docebo Enrollment reports.Learning PlanReceives data about Docebo Learning Plan reports.UserReceives data about Docebo User reports.ReportsReceives data about Docebo reports.Replace custom fields with actual names?Custom fields are labeled with field_numbers by default. Select this check box to replace the custom fields with the actual names.FilterSelect the desired filter for your Docebo report.Enrollment filtering methodSelect a method to filter the enrollments. 'All Enrollments' option will bring the data for all available enrollments. 'Select Dates' option will allow you to fetch the enrollment data for the specified date range.Individual columns for additional fieldsSelect this checkbox to get the additional fields as individual columns.Expand dataSelect Available Seats field that you would like to get expanded in the data. This will cause multiple rows", "source": "../../raw_kb/article/docebo_connector/index.html", "title": "Docebo Connector"}, {"objectID": "007dc955a38b-4", "text": "get expanded in the data. This will cause multiple rows to show each object in the field selected.Select ReportSelect the desired Docebo sub report.\u00a0Date SelectionSelect the date format for your data.Single DateSelect whether the report data is for a specific date or for a relative number of days back from today.Specific DateSelect the specific date using the date selector.Relative DateEnter the number of days back that you would like to get data for in the\u00a0Days Back\u00a0field. Specify either today or 0, yesterday or 1, or today-7 or 7 to get data for 7 days into the past.Date RangeSelect the specific or relative date range.Start Date - SpecificSelect\u00a0the first date in your date range using the date selector.End Date - SpecificSelect the last date in your date range\u00a0using the date selector.Start Date - RelativeEnter the number of days back that you would like to get data from (start day). Combine with\u00a0End Date\u00a0to create a range of represented days.", "source": "../../raw_kb/article/docebo_connector/index.html", "title": "Docebo Connector"}, {"objectID": "007dc955a38b-5", "text": "For example, if you entered\u00a010\u00a0for\u00a0Start Date\u00a0and\u00a05\u00a0for\u00a0End Date, the report would contain data for\u00a010 days ago up until\u00a05 days ago.End Date - RelativeEnter the number of days back that you would like to get data to (end day). Combine with\u00a0Start Date\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Start Date\u00a0and\u00a05\u00a0for\u00a0End Date, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Time PeriodSpecify the time period that you would like to receive data for.Starting Day of the WeekSelect the day you would like your week to start with.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector. \u00a0\nFAQ\nHow often can the data be updated?\nDatasets should be updated once per day only, due to Docebo's API call limits.\nAre there any API limits that I need to be aware of?\nDocebo API calls are limited to 1000 call per hour per IP address.\nWhat version of the Docebo API does this connector use?\nThis connector uses version 1 of the Docebo API (https://[customerdomain].docebosaas.com/api/v1).\nWhich endpoint(s) does each report call in this connector?\nReport NameEndpoint URL(s)Course Details/coursesCourse List/learnCourse Session/courses/sessionsEcommerce/ecommerceEnrollment/learnLearning Plan/learnReports/reportUser/manage", "source": "../../raw_kb/article/docebo_connector/index.html", "title": "Docebo Connector"}, {"objectID": "195c15944c7c-0", "text": "TitleDocuSign ConnectorArticle BodyIntro\nDocuSign provides services for facilitating electronic exchanges of contracts and signed documents. Domo's DocuSign connector allows you to speed up transactions and cut costs by retrieving data about DocuSign entities such as users, folders, templates, and so on. To learn more about the DocuSign API, visit their website. (https://www.docusign.com/developer-center/documentation)\nThe DocuSign connector is a \"Cloud App\" connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the connector page for this and other Cloud App connectors by clicking Cloud App in the toolbar at the top of the window.\nYou connect to your\u00a0DocuSign account\u00a0in the Data Center. This topic discusses the fields and menus that are specific to the\u00a0DocuSign connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector lets you retrieve data for DocuSign users and documents managed using DocuSign.Primary MetricsSigned/unsigned documentsTemplate documentsRejected document signingsPrimary Company RolesLegalHRSalesRealtorsAverage Implementation TimeLess than an hourEase of Use (on a 1-to-10 scale with 1 being easiest)2\nBest Practices\nMake sure the credentials and the report configuration are valid.Use \"Replace.\" If you use \"Append,\" you may need to deduplicate using a DataFlow.The connector pages data back for date-driven reports. Entering the number of days back will pull that many days back but will also increase the connector runtime. Only pull the data you need. If you have data older than the days back you have entered, the connector will keep going without getting any more data before it finishes.\nPrerequisites", "source": "../../raw_kb/article/docusign_connector/index.html", "title": "DocuSign Connector"}, {"objectID": "195c15944c7c-1", "text": "Prerequisites\nTo connect to your\u00a0DocuSign\u00a0account and create a DataSet, you must have the email address and password you use to sign in to your DocuSign account.\nConnecting to Your DocuSign\u00a0Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the\u00a0DocuSign\u00a0Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your\u00a0DocuSign account. The following table describes what is needed for each field:\u00a0\nFieldDescriptionEmail AddressEnter the email address you use to log into your DocuSign account.PasswordEnter the password you use to log into your\u00a0DocuSign account.\nOnce you have entered valid\u00a0DocuSign credentials, you can use the same account any time you go to create a new\u00a0DocuSign DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains\u00a0two primary menus in which you can select a DocuSign report and pertinent accounts.", "source": "../../raw_kb/article/docusign_connector/index.html", "title": "DocuSign Connector"}, {"objectID": "195c15944c7c-2", "text": "MenuDescriptionReportSelect\u00a0a\u00a0DocuSign report.\u00a0The following reports are available:Account  Returns information about the selected account.Account   Billing PlanReturns billing plan information about the selected account.Brands Profile  Returns\u00a0a list of brand profiles associated with the selected account.EnvelopesReturns envelope status changes for all envelopes in the selected account.\u00a0Envelopes Audit EventsReturns\u00a0audit events for all envelopes in the selected account.Envelopes Custom FieldsReturns custom field information for all envelopes in the selected account.Envelopes DocumentsReturns a list of documents associated with all envelopes in the selected account.Envelopes NotificationsReturns reminder and expiration information for all envelopes in the selected account.Envelopes StatusReturns the overall status for all envelopes in the selected account.FoldersReturns a list of folders for the selected account.GroupsReturns a list of user groups in the selected account.Permission ProfilesReturns a list of permission profiles for the selected account.TemplatesReturns the list of templates for the selected account.Unsupported File TypesReturns a list of file types that are not supported for uploads in the selected account.UsersReturn\u00a0a list of users in the selected account.Users  Returns information for all users in the selected account, including name, ID, user type, email, etc.\u00a0AccountSelect the DocuSign account you want to pull information for.\u00a0Start Days\u00a0Enter the number of past days for which you want to pull data.\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQ\nDo I need a certain kind of account to set up this connector?\nAny DocuSign account can be used to set up the connector.\nHow frequently will my data update?\nAs often as needed.\nCan I use the same DocuSign account for multiple DataSets?\nYes.", "source": "../../raw_kb/article/docusign_connector/index.html", "title": "DocuSign Connector"}, {"objectID": "73a6740986c1-0", "text": "Title\n\nDomoStats Connector\n\nArticle Body\n\nIntro\nThe DomoStats Connector gives you the freedom to explore how your team is using Domo. Dive in to your team's activities, Projects and Tasks, DataSets, Buzz conversations, and more to find insights and shape your company\u2019s Domo experience.\u00a0\nYou must have an Admin security role to access the DomoStats Connector or to connect to the DomoStats apps. However, once the DataSets have been created or the app has been deployed, they can be shared with anyone in your organization.\u00a0\u00a0\u00a0\nYou import DomoStats DataSets in the Data Center. This topic discusses the fields and menus that are specific to the DomoStats Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-1", "text": "Prerequisites\nTo pull in DomoStats DataSets using this Connector, you must have Admin security access. No credentials are required for connecting.\u00a0\nConnecting to DomoStats DataSets\nThis section enumerates the options in the Details pane\u00a0in the DomoStats Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most Connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nDetails Pane\nThis pane contains a single\u00a0Report\u00a0menu from which you select your desired DomoStats DataSet.\nReportDescriptionAccountsReturns information about the Accounts that are in the instance.Accounts with PermissionsReturns information about the Accounts that are in the instance and what users have access to them.Activity LogReturns information about team member activities within Domo such as logins, card creation, card viewing, DataSet creation, etc.\nSee Activity LogDataSet below.AutoML Training JobsIncluded with data science package.Beast Modes used in Cards\u00a0Returns information about which Beast Modes are currently in use in cards.BuzzReturns information about Buzz usage within Domo such as conversation participants, number of messages posted, etc.Card DatasourceReturns the DataSets that power a card.Card PagesReturns the dashboards where a card displays.Card PermissionsReturns information about card permissions.See Card Permissions DataSet below.DataFlow Input DataSourcesReturns the IDs of inputs to DataFlows.DataFlow Output DataSourcesReturns the IDs of outputs from DataFlows.\u00a0DataFlowsReturns information about DataFlows within your Domo\u00a0instance, such as DataFlow name, type, status, last updated date, etc.\n\u00a0\nSee DataFlows DataSet below.DataFlows HistoryReturns DataFlow run history information for your Domo instance, such as DataFlow name, run start and end time, whether runs were successful, etc.\nSee DataFlows History DataSet below.", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-2", "text": "See DataFlows History DataSet below.\n\u00a0\nNote: This report may have up to a 24-hour window without data. For example, if your report ran at 1 a.m. on 10/10, the report may not pull anything more recent than 1 a.m. on 10/09.DataSet AccessReturns which users or groups have access to which DataSets and what permission level they have.DataSet TagsReturns which tags are assigned to which DataSets.DataSetsReturns information about DataSets within your Domo instance, such as DataSet name, owner, connector, last run date, etc.\nSee DataSets DataSet below.DataSets Execution ErrorReturns DataSets with errors.DataSets HistoryReturns DataSet\u00a0run history information for your Domo instance, such as DataSet name, run start and end time, whether runs were successful, etc.\nSee DataSets History DataSet below.ML Inference JobsIncluded with data science package.Mobile ActivityReturns a user ID, the user's mobile device, and their session start date and time.\u00a0\u00a0OKR Key Results (included with Domo Goals)Returns information about the key results/metrics that have been created in Domo Goals.\nSee OKR Key Results DataSet below.OKR Objectives (included with Domo Goals)Returns information about the objectives/goals that have been created in Domo Goals.\nSee OKR Objectives DataSet below.PagesReturns information regarding Pages.Pages with Multiple OwnersReturns information of each owner of a Page broken out per row.PeopleReturns information about users in your Domo instance, such as user name, security role, contact information, last login date, etc.\nSee People DataSet below.Project StagesReturns information about project stages in your Domo instance.\nSee Project Stages DataSet below.ProjectsReturns information about projects in your Domo instance, such as project name, owner, associated tasks, etc.", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-3", "text": "See Projects DataSet below.PublishIncluded with Domo Everywhere.Publish Event LogsIncluded with Domo Everywhere.Task OwnersReturns information about task owners in your Domo instance.\nSee Task Owners DataSet below.TasksReturns information about tasks in your Domo instance, such as task name, parent project, assignees, etc.\nSee Tasks DataSet below.User ActivityReturns User Activity, similar to the Activity Log in the Admin section.Variables\u00a0Returns information about existing Variables in your instance.Variables used in Beast ModesReturns information about which Variables are currently in use in a Beast Mode.WorkbenchReturns information about Workbench job statuses, last run times, which machines are running the jobs, what DataSets are powered by the jobs, and schedule type.\nSee Workbench DataSet below.\n\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-4", "text": "Locating DomoStats DataSets\nAfter a DataSet is created, you can locate the DataSet through filters and tagging. This helps keep DataSets organized and accessible.\u00a0\n\u00a0\n1. In the navigation header, select Data.\u00a0\n2. Select DataSets from the  navigation rail. \u00a0\n3. In the Search bar, type the name of your DataSet. You can also locate DataSets using filters. \n3. In the Search bar, select Add Filter.\u00a0\n4. Choose Type and enter DomoStats.\nA list of DomoStats DataSets will generate. Choose your DataSet from the list.\u00a0\n\u00a0\nTagging a DomoStats DataSet\nLocating and organizing your DataSets is made possible by tagging. Tagging groups the DataSets in collections that can be found using the Tag filter.\u00a0\n\u00a0\n1. Locate the DataSet in Domo. (See the previous section, Locating DomoStats DataSets, for a step-by-step guide.)\u00a0\n2. Select + Add Tag under the DataSet's name.\u00a0\n\n3. Choose from the list of available tags, or create a new tag using the search feature.\u00a0\n\n4. Select + next to Create new tag.\u00a0\n5. Select Save.\u00a0\nThe DataSet is now tagged and can be\u00a0found using the Tags filter.\u00a0\n\nDataSet Fields", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-5", "text": "This section gives field-by-field descriptions for certain DomoStats reports DataSets.\n\u00a0\nActivity Log DataSet\nFieldDescriptionObject_NameThe name of the object that received an action.User_IDThe ID of the person or thing that performed an action.Source_IDThe ID of the type. For a simplified example, let's say a user added a card to a page. You would get these two actions on an event, and both would be returned on the DataSet.\"User updated page.\" This returns the following in the DataSet:\nUserId: user_idType: USERSource_Id: userIdaction: updatedobject_type: pageobject_id: page_id\"Card added to page.\" This returns the following in the DataSet:\nUserId: user_idType: CARDSource_Id: card_idaction: added_toobject_type: pageobject_id: page_idNameThe name of the object that performed an action.ActionThe event that took place. The following actions may be logged here:\nLOGGEDINLogged when\u00a0someone successfully logs in from the login page using their password. Mobile logins are recorded when a person logs out of the app and back into the app. If a person opens and closes the app, that is not counted toward logins.LOGGEDIN_SSOLogged when someone successfully logs in from the login page using SSO.LOGGEDIN_TWO_FACTORLogged when a two-factor code is successfully used to login.PAGE VIEWCounted each time a page renders.CARD VIEWCounted only if a user clicks into the Details view for a Card.Object TypeClarifies what the object is.Object IDThe ID of the object.TypeThe type of person or thing that performed an action.Event TimeThe time at which the event occurred. The audit endpoint records/returns it in UTC, but the connector converts it to the user's time zone.\n\u00a0\nCard Permissions DataSet", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-6", "text": "Card Permissions DataSet\nFieldDescriptioncardIdThe ID of the card to check permissions for.entity typeThe entity (user or group) for whom permissions exist.entityIdThe user or group ID of the entity for whom permissions exist.permission maskA numerical representation of the set of permissions granted to a user or group.PermissionA permission mask followed by a visual breakdown of the permissions granted to a user or group.R = ReadW = Write\nD = DeleteS = Share\nA = Admin\n\u00a0\nDataFlows DataSet\n\u00a0\nFieldDescriptionCurrent VersionReturns the version number of the DataFlow.IDReturns the DataFlow ID number.DescriptionReturns the description for the DataFlow if one has been specified.TypeThe type of DataFlow.StatusThe status for the DataFlow. The following statuses may appear here:\nSuccessThe DataFlow was executed successfully.Not Runnable/RejectedThere is a problem with the DataFlow definition; please view the DataFlow details.CanceledA user manually canceled the execution of the DataFlow.EnabledThe DataFlow\u00a0has been built but not yet run.FailedThere was an internal problem executing the DataFlow.RunningThe DataFlow is currently being executed.SuccessExecuted successfully.Owner IDReturns the user ID of the DataFlow owner.\u00a0This ID is the same as the User ID in the DomoStats People DataSet.Display NameReturns the current name of the DataFlow.Last Executed DateReturns a timestamp for the time of the last execution.Last Updated DateReturns a timestamp for the time when the DataFlow was last updated.Last Updated ByReturns the user ID of the last person to edit the DataFlow.\n\u00a0\nDataFlows History DataSet", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-7", "text": "DataFlows History DataSet\n\u00a0\nFieldDescriptionIDAn ID for a specific run of the DataFlow.DataFlow IDThe ID number for the DataFlow.Display NameThe name of the DataFlow.TypeReturns the DataFlow type, either ETL, MYSQL, or REDSHIFT.StatusReturns the status for the DataFlow\u00a0run, either CANCELLED, FAILED, REJECTED, or SUCCESS.Execution TypeReturns the type of execution, either DATA_UPDATE, MANUAL, RETRY, or SCHEDULED.Start TimeA timestamp for the time the DataFlow started executing.End TimeA timestamp for the time the DataFlow stopped executing.\n\u00a0\nDataSets DataSet\nFieldDescriptionStatusThe status for the DataSet. The following statuses may appear here:\nErrorError during the import.IdleA stream has been created but the DataSet has not yet been executed.SuccessThe import completed successfully.Active (or blank)The import is in process.InvalidThere may be another execution still running and this has been deactivated until the previous one is complete.IDThe unique identifying value for the DataSet.Owner_User_IDThe unique identifying value for the person who owns the DataSet. \u00a0A DataSet owner is either the person who created the DataSet or the person whom ownership was assigned to.Created_DateThe date\u00a0on which the DataSet was run for the first time.Last_Run_DateThe date on which the DataSet last successfully completed a run.NameThe current name given to the DataSet.ScheduleThe schedule option that has been configured for this DataSet. The following options may appear here:\nManualThe schedule has been set so that it only updates when you manually go in and run the DataSet.DayThe DataSet\u00a0is set to run once per day.MinuteThe DataSet is set to run within the given minute interval.HourThe DataSet is set to run once per hour.BlankThe DataSet\u00a0has been created but no schedule has been set.Import_TypeThe source for the DataSet. Could be a Connector, DataFlow, API, etc.Source_TypeUNDER CONSTRUCTION", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-8", "text": "DataSets History DataSet\n\u00a0\nFieldDescriptionStream IDA Domo-specific ID to map to the stream.Execution IDThe ID for a specific run of the DataSet. \u00a0Datasource\u00a0IDA Domo-specific datasource\u00a0IDStatusThe status for the DataSet history item. The following statuses may appear here:\nSuccessData was imported successfully.ErrorError during the import.InvalidThere may be another execution still running and this has been deactivated until the previous one is complete.\n\u00a0Execution TypeThe type of execution. The following execution types may appear here:\nManuallyThe import was executed manually.AutomatedThe import was executed on a schedule.RetryThe import failed and Domo automatically tried to execute it again.ReplayA previously imported DataSet was rerun.BatchData was imported in batches.ExtendedThe data import continued into the next scheduled run.Start TimeThe time the DataSet started running.End TimeThe time the DataSet\u00a0finished running, went into an error state, or became invalid.\nOKR Objectives DataSet\nThe fields in this section use \"Objectives\" and \"Key Results\".\u00a0Your organization may refer to them as goals, metrics, or something else depending on your Goals Admin Settings. For more information about using the DomoStats Connector with Goals, see Accessing Goals Data.\u00a0\nFieldDescriptionIDThe unique identifier of the objective.NameThe name of the objective.DescriptionThe description of the objective.StateThe current state of the objective. Possible states are listed below: \n\u00a0\nOPEN: The objective's expiration date has not elapsed and the objective has not been marked complete. \n\u00a0\nEXPIRED: The objective's expiration date has elapsed.", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-9", "text": "EXPIRED: The objective's expiration date has elapsed. \n\u00a0\nCLOSED: The objective has been marked complete.Owner IDThe ID of the individual user or group that owns the objective. When multiple individuals or groups own an objective, the ID of the first owner is populated in this field.Owner TypeIndicates whether or not the objective owner is an individual user or group. Possible types are listed below: \n\u00a0\nUSER: A Domo user \n\u00a0\nGROUP: A Domo groupTypeThe type of objective. Possible values are:\n PERSONAL \nTEAM \nCOMPANYTeam IDThe ID of the Domo group that is assigned to the objective. \n(For Team-type objectives)Parent IDThe ID of the parent objective, if applicable.Parent IDsThe IDs of the parent objectives if the objective has multiple parents, if applicable.\u00a0Multiple ParentsIndicates whether or not the objective has multiple parents.Period IDThe ID of the period that the objective is active (based on Start and expiration date).Period IDsThe IDs of periods\u00a0if\u00a0an objective is active for multiple periods.\u00a0Company IDThe ID of the organization that is assigned to the objective. \n(For Company-Type objectives)Company IDsThe ID of each Company-type objective if linked to multiple Company-type objectives.\u00a0Self AssessmentThe optional self assessment submitted when marking an objective complete.ConfidenceA numerical value based on the objective status. Values with their associated status are listed below:\n\u00a0\n1: ON TRACK \n0.5: NEEDS ATTENTION", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-10", "text": "1: ON TRACK \n0.5: NEEDS ATTENTION \n0: AT RISKLikesThe number of likes the objective has received.DislikesThe number of dislikes the objective has received.SubscriptionsThe number of users who have subscribed to the objective.Company ObjectiveIndicates if the objective is a Company-type objective.Start DateThe start date of the objective.Expire DateThe end date of the objective.\u00a0Impact Start Date(DEPRECATED) The date when the impact of the achieved objective is expected to begin.Impact End Date(DEPRECATED) The date when the impact of the achieved objective is expected to endTagsA list of tags associated with the objective. This list is separated by commas.\u00a0Tag CategoriesA list of tag categories associated with items in the Tags field. This list is separated by commas. Created DateThe date the objective was created.Creator IDThe ID of the user who created the objective.Modified DateThe date/time the objective was last modified. If the objective has not been modified, this will be the date and time the objective was originally created.Modifier IDThe ID of the user who modified the objective. If the objective has not been modified, this will be the ID of the user who created the objective.OwnersA comma-separated list of the ID and Type of the user or group that owns the objective. \nFor example, if user ID 12345678 owns the objective, the field will contain the value 12345678:USER. \nIf group 54231564 owns the objective then the field will contain the value 54231564:GROUP.Assigned Team IDsA comma-separated list of the Domo group IDs that the objective has been assigned to._BATCH_ID_An incremental batch number representing each DataSet update._BATCH_LAST_RUN_The date and time that the DataSet was last run.\n\u00a0\nOKR Key Results DataSet", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-11", "text": "OKR Key Results DataSet\u00a0\nThe fields in this section use \"Objectives\" and \"Key Results\".\u00a0Your organization may refer to them as goals, metrics, or something else depending on your Goals Admin Settings. For more information about using the DomoStats Connector with Goals, see Accessing Goals Data.\u00a0\nFieldDescriptionIDThe unique identifier of the key result.NameThe name of the key result.DescriptionThe description of the key result.\u00a0StateThe current state of the key result. Possible states are listed below: \n\u00a0\nOPEN: The key result's expiration date has not elapsed and the key result has not been marked complete.\n\u00a0\nEXPIRED: The key result's expiration date has elapsed.\n\u00a0\nCLOSED: The objective that the key result belongs to has been marked complete.\u00a0Owner IDThe ID of the individual user or group that owns the key result. When multiple individuals or groups own a key result, the ID of the first owner is populated in this field.Owner TypeIndicates whether or not the key result owner is an individual user or group. Possible types are listed below: \n\u00a0\nUSER: A Domo user \nGROUP: A Domo groupObjective IDThe ID of the objective that the key result belongs to.Card IDThe ID of the card that the key result is linked to (for automatic/card-based key results).\u00a0Alert IDA unique ID generated for each key result that is linked to a card.CompletionA percentage representing the progress of the key result as measured by the relationship of the actual value to the target value. \n\u00a0\nThe completion is calculated in the following ways: \n\u00a0\nAutomatic, card-based key results: Progress is driven based on values in the card relative to a target. \n\u00a0\nManual/percent completion-based key results: Value is determined based on the percent completion specified in the key result slider bar.", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-12", "text": "Manual/choose your own value key results: Progress is determined based on the actual value of the key result divided by the target of the key result.\n\u00a0\n Projects and Tasks-based key results: Progress is determined by how many projects/tasks have been completed completed in the Projects and Tasks feature.\u00a0PacingWhen enabled, this field shows the completion of the key result as a percentage compared to the percentage of time that the key result has been open (time between start and end date).Start ValueThe initial value of the key result.Current ValueThe current value of the key result.Target ValueThe target value of the key resultWeight(NOT USED)Fixed Weight(NOT USED)Manual TypeFor manually-managed key results, this field indicates the type of the key result. \n\u00a0\nPossible values are listed below: \n\u00a0\nVALUE: Key result measured with a single value.\nSCALE: Key result measured using a percent completion scale.\nPROJECT_AND_TASK: Key result associated to a Domo project.Unit Type\u00a0The data type of the key result's value and target. Possible values include: \n\u00a0\nNUMERICAL", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-13", "text": "NUMERICAL \nPERCENTProject IDThe ID of the project that is linked to the key result (only applicable to Projects and Tasks-based key results).\u00a0LikesThe number of likes the key result has received.DislikesThe number of dislikes the key result has received.Start DateThe start date of the key result.Expire DateThe end date of the key result.Cure PeriodIndicates when changes to the key result will no longer be tracked.\u00a0TagsA comma-separated list of tags associated with objective.Tag CategoriesA comma-separated list of tags associated with objective.Created DateThe date the objective was created.Creator IDThe ID of the user who created the objective.Modified DateThe date/time the objective was last modified. If the objective has not been modified, this will be the date and time the objective was originally created.Modifier IDThe ID of the user who modified the key result. If the objective has not been modified, this will be the ID of the user who created the it.OwnersA comma-separated list of the ID and Type of the user or group that owns the key result. \nFor example, if user ID 12345678 owns the objective, the field will contain the value 12345678:USER. \nIf group 54231564 owns the key result, the field will contain the value 54231564:GROUP._BATCH_ID_An incremental batch number representing each DataSet update._BATCH_LAST_RUN_The date and time that the DataSet was last run.\n\u00a0\nPeople DataSet\nIf any of the \"personal information\" fields (name, email, location, etc.) return blank in this DataSet, you can fill in the missing information in the user's Profile page or their page in\u00a0Admin Settings > People.\u00a0For more information, see\u00a0Specifying Your Personal Profile  .", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-14", "text": "FieldDescriptionDepartmentThe user's department within your organization.Display NameThe name used to represent the user within Domo.RoleThe user's role within your organization.TitleThe user's title within your organization.EmailThe user's email address.Alternate EmailThe user's alternate email address.Phone NumberThe user's personal phone number.Desk Phone NumberThe user's desk phone number.Employee NumberThe user's employee number within your organization.LocationThe company location the user has been assigned to.\u00a0LocaleUNDER CONSTRUCTIONTimezoneThe user's time zone.Role IDA Domo-generated ID for a specific role.User IDA Domo-generated user ID for each person.Last LoginA timestamp for the last time this user logged in.\u00a0Login is measured by any regular, SSO, or mobile login. To count as a login for mobile, a person must actually log out of the instance and log back in. Opening and closing the app does not count as a login.Password CreatedA timestamp for when a user's password was created. This gets updated when the password is updated.Account LockedReturns a \"True\" value if the account is locked.Profile PictureReturns a \"True\" value if a profile picture has been added in the user's Profile page or on his/her page in\u00a0Admin Settings > Governance > People.Org ChartReturns a \"True\" value if the user has been added as a report on their company org chart in the Profile page.\u00a0Two Factor EnabledReturns a \"True\" value if the\u00a0company has enabled two-factor authentication as an option in Admin Settings > Authentication > Authentication\u00a0and the user has chosen to have two-factor enabled in their Settings menu (available via clicking the Gear menu under their profile picture).Created DateA timestamp for the date the user was added to the system.Last Updated DateA timestamp for the date the user's information was last updated.\n\u00a0\nProject Stages DataSet", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-15", "text": "Project Stages DataSet\n\u00a0\nFieldDescriptionPriorityDefines the order in which the lists or stages are shown. Smaller numbers are on the left.Project_idID number for the project.\u00a0Project_nameName of the project.List_nameName of the list.List_typeType of the list.Project_list_idID number of the list.CreatedPerson who created the project.UpdatedThe time when the list was last edited (e.g. moved, renamed, etc.).ActiveReturns \"True\" if the list or stage hasn't been archived.\n\u00a0\nProjects DataSet\n\u00a0\nFieldDescriptionCreated_by_userThe person who created the project.Project_idThe ID number for the project.CreatedProject creation date and time.DueDateDue date for the project.Created_byUser ID of the person who created the project.Project_nameName of the project.DescriptionDescription for the project, if one has been specified.ActiveReturns \"True\" if the project is currently viewable.DeletedReturns \"True\" if the project has been deleted.\n\u00a0\nTask Owners DataSet\n\u00a0\nFieldDescriptionProjectTaskNameName of the task.ProjectNameName of the parent project for the task.ProjectListIdID number of the list in which the task is found.ProjectListNameName of the list in which the task is found.ProjectListTypeType of list in which the task is found.ProjectTaskIDID number for the task.ProjectIDID number for the parent project of the task.CreatedDate and time the task was created.AssignedByID number of the person who assigned the task.AssignedByNameName of the person who assigned the task.AssignedToID number of the person the task has been assigned to.AssignedToNameName of the person the task has been assigned to.\n\u00a0\nTasks DataSet", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-16", "text": "Tasks DataSet\n\u00a0\nFieldDescriptionDueDateDue date for the task.Project_idID number for the parent project for the task.ProjectListIdID number of the list the task is found in.ProjectListNameName of the list the task is found in.ProjectListTypeType of the list the task is found in.ProjectTaskIdID number for the task.ProjectTaskNameName of the task.CreatedDate and time the task was created.UpdatedDate and time the task was last updated.ProjectNameName of the parent project for the task.CreatedByUser ID of the person who created the task.CreatedByNameName of the person who created the task.OwnerThe ID number for the task owner. The task owner is the creator by default, but can be changed to anyone in the instance.OwnerNameThe name of the person who created the task.\u00a0In the case that the original owner reassigned ownership, then it is the name of the new owner.PriorityThe order in which the task\u00a0shows up on the task board. Lower priority numbers are above higher priority numbers.ActiveReturns \"True\" if the task hasn't been archived.DescriptionThe description of the task, if one has been specified.ContributorsIndividuals assigned to work on the task.\n\u00a0\nWorkbench DataSet", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "73a6740986c1-17", "text": "Workbench DataSet\n\u00a0\n Field  Description  Workbench Agent  The name of the machine running the Workbench job.  Owner ID  The ID number of the owner of the job.  DataSet ID  ID for the DataSet.  DataSet Name  Name of the DataSet.  Job Name  Name of the job.  Job Status  Status of the job (success, fail, etc.)  Created Date  Date the job was created.  Last Run  When the job ran last.  Next Run  When the job will run again.  Last Successful Run  When the job ran last successfully.  Update Method  How it updates the DataSet (Append, Replace, etc.)  Rows Updated  The number of rows that were updated.  Schedule Type  Shows the schedule type (Manual Schedule, Custom Schedule, etc.)", "source": "../../raw_kb/article/domostats_connector/index.html", "title": "DomoStats Connector"}, {"objectID": "5a302ca71550-0", "text": "TitleDomoStats - Activity LogArticle BodyIntro\nDomoStats gives you the freedom to explore how your team is using Domo.\u00a0Because DomoStats is powered by a live Domo DataSet with no configuration, you can easily build cards, alerts, and workflows around the metrics you want to see as well as use the pre-built dashboard.\nThe DomoStats Activity Log App provides key insights into how your organization is using Domo. See activities like how many people are logging in each day, the\u00a0top favorited cards, and how many DataSets were created last week. Stay up to date and informed on all the activity happening in your Domo instance.\u00a0\n\n\n\u00a0\n\nNote: There are known limitations with using the Activity Log App to track certifications for cards and DataSets. To join the Beta for a DomoStats report that tracks certifications, contact your Customer Success Manager (CSM) or Account Executive (AE).", "source": "../../raw_kb/article/domostats__activity_log/index.html", "title": "DomoStats - Activity Log"}, {"objectID": "5a302ca71550-1", "text": "Requirements\nYou must have admin-level access to download the DomoStats Activity Log App from the Appstore.\u00a0\nUsing the DomoStats App\nYou can use the pre-built dashboard or build any of your own content using the DomoStats DataSets in the Data Center. When you first download the app, a new page is created titled DomoStats - Activity Log. You'll need to power up the dashboard with the following steps:\nNavigate to the DomoStats - Activity Log page.Select Connect Data.  In the drop-down, select Connect to connect the page to the DomoStats DataSets or select Assign if you want to assign another Domo user to set up the data.  In the last window, select Connect.  The pre-built dashboard is now powered up and ready to use.\nThe dashboard\nData\nThe following data is included in the download of the DomoStats - Activity Log app and can be found in the Data Center as a 'DomoStats' type DataSet.\nDataSets include:\nPeopleActivity Log\nDataFlows include:\nActivity Log - User Inactivity\t\u00a0\n\t\n\n\u00a0\n\nNote:\u00a0All DataSets powering the app run once a day. Runtime is every 24 hours from when the app is first deployed. While the run frequency cannot be changed, you can edit the\u00a0runtime in the DataSet settings. For more information on how to schedule DataSets, see DataSet Scheduling.", "source": "../../raw_kb/article/domostats__activity_log/index.html", "title": "DomoStats - Activity Log"}, {"objectID": "5a302ca71550-2", "text": "Cards\nThe following cards are included in the pre-built dashboard.", "source": "../../raw_kb/article/domostats__activity_log/index.html", "title": "DomoStats - Activity Log"}, {"objectID": "5a302ca71550-3", "text": "CollectionCardDescriptionUsersDistinct User LoginsDistinct user logins over time.Login Rate - Last 14 DaysPercentage of users that logged into Domo during the given time period. The default date range is set to the last 14 days but can be adjusted.Daily User LoginsTotal number of logins per day.Most Active UsersMost active users based on the number of actions they've taken.User InactivitySorts users based on the length of time since their last login.Zero LoginsDisplays the users that have been invited to Domo, but have yet to login.Failed LoginsNumber of failed logins over time.Password ResetsTop users based on the number of times they've reset their password.User Action LogTracks every user action in your Domo instance.Login TrendTracks total logins over time.ContentTop 10 Favorited CardsTitles of the top 10 most favorited cards.Card Detail Views TrendShows the trend of card detail views over time.Card Detail Views by UserNumber of card detail views and the distinct number of users who have viewed each card.Top 10 Most Viewed CardTop 10 cards with the most detail views and counts the", "source": "../../raw_kb/article/domostats__activity_log/index.html", "title": "DomoStats - Activity Log"}, {"objectID": "5a302ca71550-4", "text": "10 cards with the most detail views and counts the total number of detail views for each.Top 10 Card ViewersTop 10 viewers based on their total number of card detail views.Top 25 Card Creators25 users that have created the highest number of cards.Page Views TrendTrend of page views over time.Page Views by UserNumber of times each page has been viewed and the number of distinct users who have viewed the page.Top 10 Most Viewed PagesTop 10 most viewed pages, and the total view count for each.Top 10 Page ViewersTop 10 viewers based on total page views.Top Page CreatorsShows the 25 users who have created the most cards.App Deployment TrendShows the trend of app deployments over time.Top Apps DeployedDisplays the apps that have been deployed, the users that have deployed the apps, and the number of times each app has been deployed.DataDatasource CreationTracks the number of datasources created over time, organized by creator.DataFlow CreationTracks the number of DataFlows created over time, organized by creator.Datasource EditsDisplays the amount of datasource edits in the specified", "source": "../../raw_kb/article/domostats__activity_log/index.html", "title": "DomoStats - Activity Log"}, {"objectID": "5a302ca71550-5", "text": "the amount of datasource edits in the specified time frame, grouped by edit type.DataFlow EditsDisplays the amount of DataFlow edits in the specified time frame, grouped by edit type.Top Beast Mode CreatorsThe users that have created the most calculated fields in the specified time frame.Top Datasource CreatorsTop 15 users that have created the most datasources in the specified time frame.Top DataFlow CreatorsUsers that have created the most DataFlows in the specified time frame.Time Since Last UpdateTracks the number of datasources bucketed by the number of days since their last update.Datasource Creation LogDisplays datasource and DataFlow creation events over time.", "source": "../../raw_kb/article/domostats__activity_log/index.html", "title": "DomoStats - Activity Log"}, {"objectID": "a777aa0cc58b-0", "text": "Title\n\nDomoStats - DataSets and DataFlows\n\nArticle Body\n\nIntro\n\n\nDomoStats gives you the freedom to explore how your team is using Domo.\u00a0Because DomoStats is powered by a live Domo DataSet with no configuration, you can easily build cards, alerts, and workflows around the metrics you want to see as well as use the pre-built dashboard.\nThis DomoStats app provides key insights into your organization's DataSets and DataFlows. Make sure your data is always fresh and contains the most relevant and up-to-date information. With this DomoStat's app, watch your DataSet and DataFlow success rates, get notified when DataSets fail to run, and see which DataSets are orphaned, duplicated, or broken to keep your Domo instance clean.\nVideo\u00a0- DomoStats DataSets and DataFlows Overview", "source": "../../raw_kb/article/domostats__datasets_and_dataflows/index.html", "title": "DomoStats - DataSets and DataFlows"}, {"objectID": "a777aa0cc58b-1", "text": "Requirements\u00a0\u00a0\nYou must have Admin level access to download from the Appstore.\u00a0\nUsing the DomoStats App\u00a0\nYou can use the pre-built dashboard or build any of your own content using the DomoStats DataSets in the Data Center. When you first download the app, a new page is created titled DomoStats - DataSets and DataFlows. You'll need to power up the dashboard with the following steps:\nNavigate to the\u00a0DomoStats - DataSets and DataFlows\u00a0page.Select\u00a0Connect Data.In the drop-down, select\u00a0Connect\u00a0to connect the page to the DomoStats DataSets or select\u00a0Assign\u00a0if you want to assign another Domo user to set up the data.In the last window, select Connect.\nThe Dashboard\nData\nThe following data is included in the download of the DomoStats - DataSets and DataFlows app and can be found in the Data Center as a 'DomoStats' type DataSet.\nDataFlows include:\nDomoStats - DataSets & DataFlows with Owners v3DomoStats\u00a0- DataSet and DataFlow History with DataSet Names and Owners v3DomoStats\u00a0- DataSets with Owners v3\u00a0\n\n\n \n\nNote:\u00a0All DataSets powering the app run once a day. Runtime is every 24 hours from when the app is first deployed. While the run frequency cannot be changed, you can edit the\u00a0runtime in the DataSet settings. For more information on how to schedule DataSets, see DataSet Scheduling.\n\n\n\nCards\nThe following cards are included in the pre-built dashboard.\nCollectionCardDescriptionData QualityAvg Object HealthShows the average object health score across your Domo instance.Unhealthy ObjectsDisplays DataSets below 100% health. Each DataSet begins with a score of 100%. For each point in each health category, the DataSet's health score is reduced by 10%.", "source": "../../raw_kb/article/domostats__datasets_and_dataflows/index.html", "title": "DomoStats - DataSets and DataFlows"}, {"objectID": "a777aa0cc58b-2", "text": "Note:\u00a0For more information on the point system for this card, please refer to the card description once downloaded.", "source": "../../raw_kb/article/domostats__datasets_and_dataflows/index.html", "title": "DomoStats - DataSets and DataFlows"}, {"objectID": "a777aa0cc58b-3", "text": "Broken ObjectsDisplays the number of DataSets and DataFlows with a broken or not broken classification when last run.Behind Schedule ObjectsThe number of DataSets and DataFlows that did not update within the scheduled window. These objects may appear to be running successfully, but in reality, they are behind schedule and need attention.\u00a0Orphaned ObjectsThe number of DataSets and DataFlows that have an owner assigned vs those that do not have an owner assigned.Duplicate NamesThe number of DataSets and DataFlows that share duplicate names in your instance.Data AvailabilityPriority Broken DataSetsDisplays DataSets that threw an \"ERROR\" or \"INVALID\" status when last run sorted by the number of cards the DataSet is powering.% Broken by OwnerLists the DataSet and DataFlow owners with the percentage of their total DataSets that have a current \"ERROR\", \"INVALID\", \"NOT_RUNNABLE\", \"CANCELLED\", \"FAILED\", or \"REJECTED\" status.Broken Objects by Schedule TypeDisplays the number of DataSets and DataFlows for a given schedule type as well as the percentage of those objects that created an \"ERROR\", \"INVALID\",", "source": "../../raw_kb/article/domostats__datasets_and_dataflows/index.html", "title": "DomoStats - DataSets and DataFlows"}, {"objectID": "a777aa0cc58b-4", "text": "of those objects that created an \"ERROR\", \"INVALID\", \"NOT_RUNNABLE\", \"CANCELLED\", \"FAILED\", or \"REJECTED\" status when last run.Broken DataSets by Connector TypeDisplays DataSets that generated an \"ERROR\", \"INVALID\", \"NOT_RUNNABLE\", \"CANCELLED\", \"FAILED\", or \"REJECTED\" status when they last tried to run grouped by DataSet connector type.Broken Obejcts\u00a0TrendShows DataSets or DataFlows that are reporting an \"ERROR\", \"INVALID\", \"NOT_RUNNABLE\", \"CANCELLED\", \"FAILED\", or \"REJECTED\" status grouped by their scheduled run type and plotted by the last run date.Data UpdatesScheduled UpdatesShows the times of day that DataSets and DataFlows have initiated updates over the last 7 days. The date range may need to be adjusted in Analyzer to include all of the data for the current day.Update TrendsThe percentage of DataSet and DataFlow updates that were successful over time.Update DetailsProvides the history of update details for all DataSets and DataFlows in the selected date range sorted by Start Time.Connector Run TimesDisplays the range of run times (in minutes) for", "source": "../../raw_kb/article/domostats__datasets_and_dataflows/index.html", "title": "DomoStats - DataSets and DataFlows"}, {"objectID": "a777aa0cc58b-5", "text": "TimesDisplays the range of run times (in minutes) for DataSets of each data import type. Includes a breakdown of the fastest, slowest, and median run times for all DataSets of each import type.Object Run TimesDisplays the number of minutes used to run each DataSet and DataFlow. Includes a breakdown of the fastest, slowest, and median run times for each DataSet and DataFlow.Data Ownership and ExpertiseInactive User OwnershipThe number of objects owned by Domo users that have not logged in for more than 30 days.Top 20 Object OwnersShows the number of DataSets and DataFlows owned by each of the top 20 DataSet owners.Owner by Connector TypeThe number of DataSets each user has created by DataSet connector type.Ownership by DepartmentThe number of objects owned by each department.Department by Connector TypeThe number of DataSets each department has created by DataSet type.Ownership by RoleShows the number of objects owned by users of each role.Role by Connector TypeThe number of DataSets each role has created by connector type.Objects and OwnersShows the details of all DataSets and DataFlows stored", "source": "../../raw_kb/article/domostats__datasets_and_dataflows/index.html", "title": "DomoStats - DataSets and DataFlows"}, {"objectID": "a777aa0cc58b-6", "text": "the details of all DataSets and DataFlows stored in the instance.Data CreationDataSet Creation TrendThe number of DataSets created in each department by day.Top ConnectorsThe number of existing objects by connector import type.DataSets Running TotalDisplays a running total of all DataSets created in your instance.DataSet Rows UpdatedShows the number of rows updated on DataSets over time.Creators This WeekThe number of DataSets of each type that have been created per DataSet owner this week.New Department DataSetsShows the number of DataSets of each type that have been created by each department this week.", "source": "../../raw_kb/article/domostats__datasets_and_dataflows/index.html", "title": "DomoStats - DataSets and DataFlows"}, {"objectID": "13cd80a6c415-0", "text": "Title\n\nDomoStats - People\n\nArticle Body\n\nIntro\u00a0\n\n\nDomoStats gives you the freedom to explore how your team is using Domo.\u00a0Because DomoStats is powered by a live Domo DataSet with no configuration, you can easily build Cards, alerts, and workflows around the metrics you want to see as well as use the pre-built dashboard.\nThis DomoStats - People App gives Admins a list of all their users\u00a0so they can see who is in Domo and their level of activity all in one place.\u00a0Watch adoption levels and take action quickly to help people get onboard by knowing login trends and seeing who still needs to log in. Keep your instance secure and make sure everyone is working at the best level for them.\nVideo\u00a0- DomoStats - People App\n\u00a0\n\n\u00a0\nRequirements\u00a0\nYou must have Admin-level access to download from the Appstore.\u00a0\nDeploying\u00a0the DomoStats App\nYou can use the pre-built dashboard or build any of your own content using the DomoStats DataSets in the Data Center.\nYou can download and power up the dashboard by following these steps:\nSearch for \"DomoStats People\" in the Appstore.Click the orange Get\u00a0button. (Conditional) If there are already any versions of the App installed on your Domo instance, click\u00a0Add New.Once the sample page is ready, click\u00a0Connect to connect the Page to your DomoStats DataSets.\u00a0Or select Assign if you want to assign another Domo user to set up the data.  \nThe Dashboard\nData\nThe following data is included in the download of the DomoStats - People App and can be found in the Data Center as a \"DomoStats\"-type DataSet.\nDataSets include:\nPeople\nDataFlows include:\nDomoStats\u00a0People with Activity Log", "source": "../../raw_kb/article/domostats__people/index.html", "title": "DomoStats - People"}, {"objectID": "13cd80a6c415-1", "text": "Note:\u00a0All DataSets powering the App run once a day. Runtime is every 24 hours from when the App is first deployed. While the run frequency cannot be changed, you can edit the\u00a0runtime in the DataSet settings. For more information on how to schedule DataSets, see DataSet Scheduling.\n\n\n\nCards\nThe following Cards are included in the pre-built dashboard.\nCollectionCardDescriptionUsersUser Departments & RolesShows the number of Domo users in each role by department.Running Total of UsersThe running total of users in the instance over time.\u00a0\n\n\n\n\nNote:   does not include users that have been removed from the instance.\n\n\nUser RolesTracks the number of users in your Domo instance, grouped by role.User DepartmentsShows the number of Domo users in each of your departments.User DirectoryDisplays a list of all Domo users, sorted alphabetically by first name.LoginsLogin Rate - Last 14 DaysShows the percentage of users that logged into Domo during the given time period.Zero LoginsDisplays the users that have been invited to Domo, but have yet to login.Days Since Last LoginThe number of users whose last login was within the time range of the given buckets.Most Recent LoginsTracks the most recent login date of all users over time. Darker shades represent higher numbers of users that last logged in on a given date.User Last LoginsShows the number of days since each user has logged in.User ActivityRole ActivitiesThe number and type of actions taken by each role in the last 14 days.Top Content CreatorsDisplays the individuals responsible for creating the most content in your instance.\n\n\n\n\nNote:\u00a0For more information on the point system for this Card, please refer to the Card description once downloaded.\n\n\nTop Security AdminsDisplays the individuals in your instance that have been most involved in security/admin type actions.", "source": "../../raw_kb/article/domostats__people/index.html", "title": "DomoStats - People"}, {"objectID": "13cd80a6c415-2", "text": "Note:\u00a0For more information on the point system for this Card, please refer to the Card description once downloaded.\n\n\nTop Card ManagersDisplays the individuals in your instance that have been most involved in Card Management type actions.\n\n\n\n\nNote:\u00a0For more information on the point system for this Card, please refer to the Card description once downloaded.\n\n\nRole ParticipationShows the number of Cards created and the number of Cards viewed by users of each role. Bubble size denotes the number of participating users of the specified role.", "source": "../../raw_kb/article/domostats__people/index.html", "title": "DomoStats - People"}, {"objectID": "1c10fc58e2e0-0", "text": "TitleDomoStats - Projects and TasksArticle BodyIntro\u00a0\nDomoStats gives you the freedom to explore how your team is using Domo.\u00a0Because DomoStats is powered by a live Domo DataSet with no configuration, you can easily build cards, alerts, and workflows around the metrics you want to see as well as use the pre-built dashboard.\nThis DomoStats app provides key insights into your organization's Domo Projects and Tasks. Ensure projects will be delivered on time by monitoring all your open projects and tasks in a single dashboard. Identify at-risk projects, address overdue tasks, and manage team workloads by tracking assignee productivity and efficiency.\u00a0\nVideo - Domostats - Projects and Tasks App\n\u00a0\n\nRequirements\u00a0\nYou must have Admin level access to download from the Appstore.\u00a0\nUsing the DomoStats App\nYou can use the pre-built dashboard or build any of your own content using the DomoStats DataSets in the Data Center. When you first download the app, a new page is created titled DomoStats - Projects and Tasks. You'll need to power up the dashboard with the following steps:\nNavigate to the DomoStats - Projects and Tasks page.Select Connect Data.In the drop-down, select Connect to connect the page to the DomoStats DataSets or select Assign if you want to assign another Domo user to set up the data.In the last window, select Connect.\nThe dashboard\nData\nThe following data is included in the download of the DomoStats - Projects and Tasks app and can be found in the Data Center as a 'DomoStats' type DataSet.\nDataSets include:\nProjectsTasksTask Owners\nDataFlows include:\nProjects and TasksTasks + Task Owners", "source": "../../raw_kb/article/domostats__projects_and_tasks/index.html", "title": "DomoStats - Projects and Tasks"}, {"objectID": "1c10fc58e2e0-1", "text": "Note:\u00a0All DataSets powering the app run once a day. Runtime is every 24 hours from when the app is first deployed. While the run frequency cannot be changed, you can edit the\u00a0runtime in the DataSet settings. For more information on how to schedule DataSets, see DataSet Scheduling.\n\n\n\nCards\nThe following cards are included in the pre-built dashboard.\nCollectionCardDescriptionProjectsProject TrackerDisplays the start date and due date for each project and measures the number of completed tasks against the number of total tasks to give an estimated completion percentage for each project.\n\n\n\n\nNote:\u00a0For best results, be sure to include a project due date for each of your projects within your Domo Projects and Tasks. When a project due date is not defined, the current date will be displayed as the due date on this card.", "source": "../../raw_kb/article/domostats__projects_and_tasks/index.html", "title": "DomoStats - Projects and Tasks"}, {"objectID": "1c10fc58e2e0-2", "text": "Project ProgressionShows the number of to-do, working on, and completed tasks per project.Project Due DatesShows the projects due on each day of the calendar.Projects Due - Next 10 DaysThe projects that will come due in the next 10 days.Projects Past DueThe projects that are past due and the number of days since they were due.Newest ProjectsThe project name, created date, creator name, and project description of the 15 most recently created projects.Project Creation TrendThe number of projects created over time.Top Project CreatorsThe number of projects created by users.Largest ProjectsThe number of tasks created across all active projects.TasksProject Task CompletionMeasures the number of completed tasks against the number of total tasks within each active project.Task Completion by ProjectThe number of tasks completed on each project over time.Tasks Completed (30 Days)The trend of tasks completed over the last 30 days.Tasks Due CalendarThe tasks that are due each day.Overdue TasksShows the tasks that are past due, the projects those tasks belong to, and the number of days since the tasks were due.Task BreakdownDisplays the number of tasks", "source": "../../raw_kb/article/domostats__projects_and_tasks/index.html", "title": "DomoStats - Projects and Tasks"}, {"objectID": "1c10fc58e2e0-3", "text": "the tasks were due.Task BreakdownDisplays the number of tasks listed as to-do, working on, and completed across all active projects.AssigneesAssignee ProductivityThe number of tasks each member has been assigned and the number of tasks each member has completed. Bubble sizes represent the percentage of assigned tasks that have been completed.Assignee WorkloadsShows the number of tasks each member has been assigned in each project.Top 10 Assignee WorkloadsThe number of tasks each member has been assigned and the number of tasks each member has completed. Displays the top 10 assignees by the number of tasks.Top 25 AssigneesThe number of tasks each member has been assigned.Lightest WorkloadsThe 5 assignees with the fewest number of \"to-do\" or \"working-on\" tasks assigned them.Assignee EfficiencyShows the average number of days each assignee's tasks have had a \"to-do\" or \"working-on\" status. Also displays the average number of days each assignee spent on tasks that have been completed.Assignee Tasks To-doShows the number of to-do tasks assigned to each member and the average number of days these tasks have had a", "source": "../../raw_kb/article/domostats__projects_and_tasks/index.html", "title": "DomoStats - Projects and Tasks"}, {"objectID": "1c10fc58e2e0-4", "text": "the average number of days these tasks have had a to-do status.Assignee Tasks Working-onThe number of tasks each member is working on and the average number of days these tasks have had a \"working-on\" status.Assignee Tasks CompletedThe number of tasks each member has completed and the average number of days spent completing these tasks.", "source": "../../raw_kb/article/domostats__projects_and_tasks/index.html", "title": "DomoStats - Projects and Tasks"}, {"objectID": "29a0734317e7-0", "text": "TitleDomo APIsArticle BodyDomo Application Program Interfaces (APIs) makes it easier to administer your data and users, giving you the power and flexibility to get the most out of Domo.\u00a0Domo APIs provide the secure environment you need to automate users and groups or manage data imports and exports. Domo has a wide variety of APIs, and more are always being published.\nDomo's API documentation is housed in our Developer Portal, which you can visit at\u00a0https://developer.domo.com/manage-domo.", "source": "../../raw_kb/article/domo_apis/index.html", "title": "Domo APIs"}]
