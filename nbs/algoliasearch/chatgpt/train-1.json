[{"objectID": "e10fd4c37904-0", "text": "TitleAmazon Redshift Upsert ConnectorArticle BodyIntro\nAmazon Redshift is a hosted data warehouse project that is part of the larger cloud computing platform Amazon Web Services. Redshift handles analytics workloads on large scale DataSets stored by a column-oriented DBMS principle. With the Amazon Redshift Upsert Connector you have the ability to schedule your data using the merge (upsert) option.\u00a0\nYou connect to your Amazon Redshift Upsert Account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon Redshift Upsert connector interface. General information for adding DataSets and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\u00a0\nPrerequisites\nTo connect to Amazon Redshift so you can begin creating Amazon Redshift Upsert DataSets, you must have the following:\nJDBC DriverThe host name\u00a0for the Redshift databaseThe database name for the Redshift databaseYour Redshift username and passwordThe port number of your Redshift databaseThe format of your certificate. This must match what you enter in the Certificate field.The text for your CA certificate or enter the URL where your certificate is located. By default, no certificate is required.\nWhitelisting\nBefore you can connect to a Redshift database, you must also whitelist a number of IP addresses on your database server on the port you want to connect to. For the full list of IP addresses, see Whitelisting IP Addresses for Connectors.\u00a0\nConnecting To Your Amazon Redshift Upsert Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the Amazon Redshift Upsert Connector page. The components of the other panes in this page,\u00a0 Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\u00a0\nCredentials Pane", "source": "../../raw_kb/article/amazon_redshift_upsert_connector/index.html", "title": "Amazon Redshift Upsert Connector"}, {"objectID": "e10fd4c37904-1", "text": "Credentials Pane\nThis pane contains fields for entering credentials to connect to your Amazon Redshift Upsert account. The following table describes what is needed for each field:\nFieldDescriptionJDBC DriverSelect the JDBC driver to use.HostEnter the host name for the SQL database. If you do not know the host name, contact your Amazon Redshift Database Administrator.DatabaseEnter the name of the SQL database. If you do not know the database name, contact your Amazon Redshift Database Administrator.UsernameEnter your Redshift username. If you do not know your username, contact your Amazon Redshift Database Administrator.PasswordEnter your Redshift password. If you do not know your\u00a0username, contact your Amazon Redshift Database Administrator.\u00a0Database PortEnter the port number of your Redshift database. If you do not know the port number, contact your Amazon Redshift Database Administrator.Certificate FormatSelect a certificate format. This must match what you enter in the\u00a0Certificate\u00a0field. If you are not aware of a special certificate, select no certificate.CertificatePaste the text for your CA certificate or enter the URL where your certificate is located. By default, no certificate is required.\n\u00a0\nOnce you have entered valid credentials, you can use the same account any time you go to create a new Amazon Redshift Upsert DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\u00a0\nDetails Pane\nThis pane contains various options for specifying the data you want to pull into Domo\nMenuDescriptionQuery TypeSelect a query type\nQuery: Regular SQL query without parameters.", "source": "../../raw_kb/article/amazon_redshift_upsert_connector/index.html", "title": "Amazon Redshift Upsert Connector"}, {"objectID": "e10fd4c37904-2", "text": "MenuDescriptionQuery TypeSelect a query type\nQuery: Regular SQL query without parameters.\nQuery Parameter: SQL query with parameters.QueryEnter the SQL query to use in selecting the data you want. For example, SELECT * FROM EmployeeFetch SizeEnter the fetch size for memory performance. If it is blank, the default value will be 1000. If it throws out of memory for a value, decrease fetch size.Query ParameterEnter the query parameter value. It is the initial value for query parameter. The last run date is optional and by default is '02/01/1700' if it is not provided. For example, !{lastvalue:_id}!=1,!{lastrundate:start_date}!=02/01/1944Validate TypeSelect the validation type\nNo Validation: It will not do any validation.\nValidate Schema: It will validate the previous schema with current schema, and it will fail the job run if the schema is not the same.Validate BySelect the validation cases\nColumn Case Sensitive: If previous and current column names are the same but it is different by case sensitivity, it will not consider them as the same column name.\nColumn by Order: If previous columns and current columns are not in the same order, it will not consider them as the same column name.Boolean FormatSelect the boolean format for the data to return\n1 = true, 0 = false: It will return 1 for true, otherwise 0 for all the boolean data column.\nt = true, f = false: It will return t for true, otherwise f for all the boolean data column.\nT = true, F = false: It will return T for true, otherwise F for all the boolean data column.\ntrue = true, false = false: It will return true for true, otherwise false for the boolean data column.Timeout Query (Optional)Enter the desired query timeout value.\n\u00a0\nScheduling", "source": "../../raw_kb/article/amazon_redshift_upsert_connector/index.html", "title": "Amazon Redshift Upsert Connector"}, {"objectID": "e10fd4c37904-3", "text": "Scheduling\nThe unique scheduling feature of this connector is the ability to merge (upsert) your data. This method will replace all updated rows, while any new rows will be appended.", "source": "../../raw_kb/article/amazon_redshift_upsert_connector/index.html", "title": "Amazon Redshift Upsert Connector"}, {"objectID": "e10fd4c37904-4", "text": "To be able to Merge your updated data correctly, you must identify a Merge Key in your data. A Merge Key can be your primary key or a combination of columns that is unique in the DataSet and will be used to compare rows between different versions of your DataSet. You can use a CONCAT() function in the SELECT statement to establish a complex Merge Key. If you are unsure where the Merge Key is for this DataSet, please consult the DBA who owns the database or your Domo Administrator.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure standard scheduling options, retry, and update options, see Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_redshift_upsert_connector/index.html", "title": "Amazon Redshift Upsert Connector"}, {"objectID": "30d6fc292216-0", "text": "Title\n\nAmazon Redshift v1 Connector\n\nArticle Body\n\nIntro\nAmazon Redshift is a hosted data warehouse project that is part of the larger cloud computing platform Amazon Web Services.\u00a0Redshift handles analytics workloads on large scale DataSets stored by a column-oriented DBMS principle. You can use Domo's\u00a0Amazon Redshift\u00a0Connector to pull data from your\u00a0Redshift\u00a0database and compile custom reports. You indicate the data you want by inputting an SQL query. For more information about the\u00a0Redshift API, visit their website. (https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-sql.html)\nThe Amazon Redshift connector is a \"Database\" connector, meaning it retrieves data from a database based on a query. In the Data Center, you can access the connector page for this and other Database connectors by clicking Database in the toolbar at the top of the window.\nYou connect to your\u00a0Redshift database in the\u00a0Data Center. This topic discusses the fields and menus that are specific to the\u00a0Redshift connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\n\n \n\n\nNote: Depending on your network's structure, you may need to whitelist any IP addresses necessary for the data connector to retrieve data. Contact your IT administrator for assistance.", "source": "../../raw_kb/article/amazon_redshift_v1_connector/index.html", "title": "Amazon Redshift v1 Connector"}, {"objectID": "30d6fc292216-1", "text": "Primary Use CasesPulling data out of a Redshift data warehouse or data mart.Primary MetricsThis depends on the data stored in the Redshift instance.Primary Company RolesBI leadsIT rolesAverage Implementation TimeAnywhere between 5 and 40 hours, depending on configuration settings and the amount of data you are pulling.Ease of Use (on a 1-to-10 scale with 1 being easiest)6\n\u00a0\nBest Practices\nTry to limit the results set size as much as possible. To speed the acquisition of the data, \"fetch size\" can be increased; however, setting it too high can cause connector run failures as well as performance problems with the Redshift instance. 10000 is a good place to start.\nPrerequisites\nTo connect to a\u00a0Redshift database and create a DataSet, you must have the following:\nThe username and password you use to log into your\u00a0Redshift databaseThe host name for the database server (for example,\u00a0db.company.com).The port number for the databaseThe database name\nCA certificate text or URL path is required\u00a0only\u00a0if you select\u00a0Certificate\u00a0String\u00a0or\u00a0URL Path,\u00a0respectively, in the\u00a0Certificate\u00a0Format\u00a0menu.\nCreating a Redshift account", "source": "../../raw_kb/article/amazon_redshift_v1_connector/index.html", "title": "Amazon Redshift v1 Connector"}, {"objectID": "30d6fc292216-2", "text": "Creating a Redshift account\nTo create a Redshift user account, contact your Redshift Database Administrator (DBA) and have them follow these directions:By default, only the master user that you created when you launched the cluster has access to the initial database in the cluster. To grant other users access, you must create one or more user accounts. Database user accounts are global across all the databases in a cluster; they do not belong to individual databases.Use the CREATE USER command to create a new database user. When you create a new user, you specify the name of the new user and a password. A password is required. It must have between 8 and 64 characters, and it must include at least one uppercase letter, one lowercase letter, and one numeral.For example, to create a user named GUEST with password ABCd4321, issue the following command:create user guest password 'ABCd4321'; \nWhitelisting IP addresses\nBefore you can connect to a Redshift database, you must also whitelist a number of IP addresses on your database server on the\u00a0port\u00a0you want to connect to. For the full list of IP addresses, see Whitelisting IP Addresses for Connectors.\nConnecting to Your\u00a0Redshift Database\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the\u00a0Redshift\u00a0Connector page. The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your database. The following table describes what is needed for each field:", "source": "../../raw_kb/article/amazon_redshift_v1_connector/index.html", "title": "Amazon Redshift v1 Connector"}, {"objectID": "30d6fc292216-3", "text": "FieldDescriptionJDBC DriverSelect the JDBC driver you want to use to connect.\u00a0HostEnter the host string to connect to the SQL database. For example:\u00a0//examplecluster.cpdbcwb2mzr4.us-east-2.redshift.amazonaws.comDatabaseEnter the name of the SQL database.UsernameEnter your\u00a0Redshift username.PasswordEnter your\u00a0Redshift password.Database PortEnter the port number for the database.Certificate FormatSelect\u00a0the certificate format. If you do not want to include a certificate, select\u00a0No Certificate. If you select\u00a0Certificate\u00a0String, you must paste the text for your certificate in the\u00a0Certificate field. If you select\u00a0URL Path, you must enter the URL where your certificate is located in the\u00a0Certificate field. \u00a0\u00a0CertificatePaste the text for your CA certificate or enter the URL where your certificate is located.\u00a0This is optional. If you do not want to include a certificate, select No Certificate in the Certificate\u00a0Format menu.\nOnce you have entered valid\u00a0Redshift credentials, you can use the same account any time you go to create a new\u00a0Redshift DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nIn this pane you create an SQL query to pull data from your database. You can also choose a specific database table and columns and specify which columns you want to appear in your report.\nMenuDescriptionQueryEnter the Structured Query Language (SQL) query to use in selecting the data you want. For example:\nselect * from EmployeeDatabase TablesSelect the database table you want to appear in your report.Table Columns\u00a0Select all\u00a0table columns you want to appear in your report.\u00a0\u00a0Query Helper\u00a0Revise this query if desired.\u00a0\nOther Panes", "source": "../../raw_kb/article/amazon_redshift_v1_connector/index.html", "title": "Amazon Redshift v1 Connector"}, {"objectID": "30d6fc292216-4", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nTroubleshooting\nIf you run into connection issues...\nVerify the connection works using pgAdmin.Confirm all of the necessary Domo IPs have been whitelisted (see Whitelisting IP addresses, above).Run a profile on the SQL query.\nFAQ\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits that I need to be aware of?\nLimits depend on your server configuration.\nWhat do I need to be aware of while writing a query?\nMake sure that all the words, table names and field names are correctly spelled. Refer to the Query Helper field for query help.", "source": "../../raw_kb/article/amazon_redshift_v1_connector/index.html", "title": "Amazon Redshift v1 Connector"}, {"objectID": "58823b21f2bd-0", "text": "TitleAmazon Redshift Writeback ConnectorArticle BodyIntro\nUse this connector to export your data from a Domo DataSet to an Amazon Redshift\u00a0database. To learn more about Redshift, visit their website at\u00a0https://aws.amazon.com/redshift/.\nYou export data to a\u00a0Redshift database\u00a0in the Data Center. This topic discusses the fields and menus that are specific to the Redshift Writeback\u00a0connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\n\n\n\n\nNote: The owner of a writeback dataset must also be an owner or co-owner of the input dataset.", "source": "../../raw_kb/article/amazon_redshift_writeback_connector/index.html", "title": "Amazon Redshift Writeback Connector"}, {"objectID": "58823b21f2bd-1", "text": "Prerequisites\nTo configure this connector, you will need the following:\nA Domo Client ID and Client Secret. To obtain these credentials, do the following:Log into your Domo developer account at https://developer.domo.com/login.\u00a0Create a new client.\u00a0Select the desired data and user application scope.Click\u00a0Create.The hostname or IP address of your Redshift server.Your Redshift\u00a0database name.Your Redshift username and password.\u00a0The port number of your Redshift server.Your AWS access key.\u00a0This is available in the AWS Console in the Security Credentials section (or under Users if you are using IAM).\u00a0Your AWS secret key.\u00a0This was provided when you created your access\u00a0key. You can generate a new one in the AWS Console.\nIf you don't know any of your SQL credentials, contact your SQL Database Administrator.\nYou may also paste the text for a\u00a0Redshift CA certificate (or you may input the URL where the certificate is located), but this is optional.\nConfiguring the Connection\nThis section enumerates the options in the Credentials and Details panes in the Redshift Writeback Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Domo\u00a0developer\u00a0account as well as the table in your Redshift database where you want your data to be copied to. The following table describes what is needed for each field:", "source": "../../raw_kb/article/amazon_redshift_writeback_connector/index.html", "title": "Amazon Redshift Writeback Connector"}, {"objectID": "58823b21f2bd-2", "text": "FieldDescriptionDomo Client IDEnter your Domo client ID.Domo Client SecretEnter your Domo client secret.HostEnter your Redshift database\u00a0hostname.Database\u00a0Enter the name of your Redshift database.UsernameEnter your Redshift username.PasswordEnter your Redshift password.\u00a0Database PortEnter your Redshift database\u00a0port number.Certificate FormatSelect a certificate format. This must match what you enter in the Certificate field. If you are not aware of a special certificate, select No certificate.CertificatePaste the text for your CA certificate, or enter the URL where your certificate is located. By default no certificate is required.Access KeyEnter your AWS access key.Secret KeyEnter your AWS secret key.RegionSelect the S3 region containing the bucket you want to push data to.BucketEnter the name of the AWS bucket you want to push data to.\nFor more information about obtaining these credentials, see \"Prerequisites,\" above.\u00a0\nOnce you have entered valid credentials, you can use the same account any time you go to set up a new Domo-Redshift connection. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a number of fields for specifying your data and indicating where it's going.", "source": "../../raw_kb/article/amazon_redshift_writeback_connector/index.html", "title": "Amazon Redshift Writeback Connector"}, {"objectID": "58823b21f2bd-3", "text": "This pane contains a number of fields for specifying your data and indicating where it's going.\nMenuDescriptionDataSet IDEnter the DataSet\u00a0ID (GUID) for the DataSet you want to copy to Redshift. You can find the ID by opening the details view for the DataSet in the Data Center and looking at the portion of the URL following datasources/.\u00a0For example, in the URL\u00a0https://mycompany.domo.com/datasources/845305d8-da3d-4107-a9d6-13ef3f86d4a4/details/overview, the DataSet ID is\u00a0845305d8-da3d-4107-a9d6-13ef3f86d4a4.\u00a0Table Name SourceSelect how you want to name the table where data will be copied.\u00a0DataSet\u00a0ID. The table name will be the number you entered for\u00a0DataSet ID.DataSet\u00a0Name. The table name will be the same as that of the input DataSet.Custom Name. You will give the table a custom name in the\u00a0Custom Table Name\u00a0field.Custom Table NameEnter the name of the table in your Redshift database where you want your DataSet data to be copied.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_redshift_writeback_connector/index.html", "title": "Amazon Redshift Writeback Connector"}, {"objectID": "56ed1623fef7-0", "text": "Title\n\nAmazon S3 Advanced Connector\n\nArticle Body", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ed1623fef7-1", "text": "Intro\nAmazon S3 is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nThe Amazon S3 and Amazon S3 Advanced connectors are almost the same. The only difference is in how they handle multiple files\u00a0that begin with the prefix string you provide in the Details section of the connector. The standard Amazon S3 connector will only import the latest modified file. The Amazon S3 Advanced connector will import all files with the provided prefix, assuming they all have the same schema. However, the Amazon S3 Advanced connector will only get files modified since the last run or new files after the first run. For information about the standard version of the connector, see\u00a0Amazon S3 Connector.\nThe Amazon S3 Advanced connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking\u00a0File\u00a0in the toolbar at the top of the window.\nYou connect to your Amazon S3 account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 Advanced connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is an excellent choice for retrieving flat files when APIs are not an option.Primary MetricsN/APrimary Company RolesData specialistsMarketing rolesFinance rolesAnyone who has data stored in S3Average Implementation TimeLess than an hour if you have the correct file types in S3.Ease of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ed1623fef7-2", "text": "Best Practices\nUnderstanding the data stored in S3 and its relation to other S3 databases will be a huge asset in using this connector.\nPrerequisites\nTo connect to your Amazon S3 account and create a DataSet, you must have the following:\nYour AWS access key. You can find this in the Security Credentials section of the AWS Console. Alternatively, if you are using IAM, you can find it under Users.Your AWS secret key, which was provided when you created your access key. You can generate a new secret key in the AWS Console.The name of the AWS S3 Bucket you want to retrieve files from.\nCreating a User with the Proper Permissions\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.\nTo configure your user in the IAM Amazon Console,\nAdd a new user, setting options as follows:In the Details pane, check the box for Programmatic Access under Select AWS access type.  \u00a0In the Permissions pane, select Attach existing policies directly, then check the box for either AmazonS3FullAccess or AmazonS3ReadOnlyAccess.\t\tCustomer-managed policies do not work.  In the Review pane, click Create User.After you create your user, copy the access and secret keys to use in the Credentials pane in Domo.\nConnecting to Your Amazon S3 Account\nThis section enumerates the options in the\u00a0 Credentials \u00a0and\u00a0 Details \u00a0panes in the Amazon S3 Advanced Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Amazon S3 account. The following table describes what is needed for each field:", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ed1623fef7-3", "text": "FieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see \"Prerequisites,\" above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see \"Prerequisites,\" above.BucketEnter the Amazon S3 Bucket you want to pull files from.RegionSelect the desired Amazon S3 Bucket region.\nOnce you have entered valid Amazon S3 credentials, you can use the same account any time you go to create a new Amazon S3 Advanced DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains various menus for locating and configuring the file you want to pull into Domo.", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ed1623fef7-4", "text": "MenuDescriptionFile Type to ImportSelect the type of file you want to import into Domo, either CSV, JSON, or XML.How Would You Like to Choose Your Filename?Select how you want Domo to identify your file. Options are as follows:Complete FilenameYou enter the complete path to the file.Partial FilenameYou enter a partial file name and specify whether the file to retrieve starts with or contains the name you have entered.FilesDiscoveryA list of files in the selected bucket appears and you choose the one you want. This option is available only when your selected S3 bucket contains fewer than 100\u00a0files.Prefix (Optional)Enter a prefix to filter results by. A prefix limits results to only those keys that begin with the given prefix. Note that\u00a0this advanced version of the S3\u00a0connector imports all files with the provided prefix, assuming they all have the same schema.Enter Complete FilepathEnter the complete path for the desired file.File NameEnter the name of the file you want to import, with or without the file path. For example: folder_name/file_nameFile Name Match TypeSpecify whether the file", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ed1623fef7-5", "text": "folder_name/file_nameFile Name Match TypeSpecify whether the file you want to retrieve starts with or contains the text you entered under File Name.File Compression TypeSelect the compression type for the file you want to retrieve. If the file is not compressed, select None.Are Headers Present in CSV File?Select Yes if the CSV file you are importing contains headers; otherwise select No.List of FilesSelect the file you want to retrieve.Select the Delimiting CharacterSelect the delimiter used in the CSV file you want to retrieve. If your delimiter is not listed, select Other.Quote CharacterSelect a quote character to parse the CSV file you want to retrieve. A double quote (\") is the CSV standard.Escape CharacterSelect an escape character to parse the CSV file you want to retrieve. A backslash (\\) is the CSV standard.Enable Parsing for Large JSON Files?Specify whether parsing is enabled for large JSON files.Does Your JSON Text Require a Line Reader?Specify whether the text in your JSON file includes multiple lines that should be read.Should the Backslash be Escaped?Specify whether the text in your JSON file contains backslash", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ed1623fef7-6", "text": "whether the text in your JSON file contains backslash characters that should be escaped.Enter Your Data TagEnter the tag for the data in your JSON or XML file.Enter Fields to ExcludeProvide a comma-separated list of fields to exclude in your JSON/XML import.Enter Your Header TagEnter the tag for the header in your JSON or XML file.Do You Require Attributes in Data?Specify whether you require attributes values as part of your XML data.Enter XPath ExpressionEnter your XPath expression.Is Password Protected?Select Yes if the file you are retrieving is password-protected; otherwise select No.Header Start RowEnter the number of the header row in the Excel file you are retrieving.Data Start RowEnter the number of the first data row in the Excel file you are retrieving.FooterEnter the number of the footer row in the Excel file you are retrieving.Blank RowsSelect the action that should be taken if blank rows are encountered. If you select Skip blank rows, any blank rows are skipped; if you select Stop at the first blank column, the data import stops when a column with a blank row", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ed1623fef7-7", "text": "data import stops when a column with a blank row is encountered.Empty Column HeadersSpecify what should happen if empty column headers are encountered. If you choose\u00a0Add blank columns, column names are automatically generated for the new columns. If you select Stop at the first blank column, the data import stops when an empty header is encountered.", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ed1623fef7-8", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nTroubleshooting\nEnsure that the file is present in the S3 bucket and that the correct file type is specified in the connector settings.NULL columns must be removed before the connector can successfully retrieve data.If you run into a \"Failed to Import Successfully\" error when trying to import a CSV file, you can often get around this by changing the option in the\u00a0Quote Character\u00a0menu to\u00a0Null Character.\nFAQ\nHow frequently will my data update?\nAs often as needed.\nCan I use the same Amazon S3 account to create multiple DataSets?\nYes.\nAre there any API limits that I need to be aware of?\nYou may encounter a limit of 100 Amazon S3 buckets per account.\nWhy is the connector unable to pull the data?\nEnsure that the file is present in the S3 bucket and specify the correct file type in the connector settings. Also, remove null columns to allow the connector to retrieve the data.\nWhat's the difference between the standard Amazon S3 connector and the advanced version?\nThe Amazon S3 connector and the Amazon S3 Advanced connector both use the same SDK. They differ in how they handle multiple files.If your Amazon S3 bucket contains multiple files that begin with the prefix string you provide in the Details section of the connector, the Amazon S3 connector will only import the latest modified file. The Amazon S3 Advanced connector will import all files with the provided prefix, assuming they all have the same schema.", "source": "../../raw_kb/article/amazon_s3_advanced_connector/index.html", "title": "Amazon S3 Advanced Connector"}, {"objectID": "56ff177fbc19-0", "text": "TitleAmazon S3 Advanced Multipart ConnectorArticle BodyIntro\nAmazon S3 Advance Multipart Connector is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nThe Amazon S3 and Amazon S3 Advanced Multipart connectors are almost the same. The only difference is in how they handle multiple files that begin with the prefix string you provide in the\u00a0Details\u00a0section of the connector. The standard Amazon S3 connector will only import the latest modified file. The Amazon S3 Advanced Multipart connector will import all files with the provided prefix, assuming they all have the same schema. However, the Amazon S3 Advanced Multipart connector will only get files modified since the last run or new files after the first run. For information about the standard version of the connector, see Amazon S3 Advanced Connector \u2013 Domo\nThe Amazon S3 Advanced connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking\u00a0File\u00a0in the toolbar at the top of the window.\nYou connect to your Amazon S3 account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 Advanced connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "56ff177fbc19-1", "text": "Primary Use CasesThis connector is an excellent choice for retrieving flat files when APIs are not an option.Primary MetricsN/APrimary Company RolesData specialistsMarketing rolesFinance rolesAnyone who has data stored in S3Average Implementation TimeLess than an hour if you have the correct file types in S3.Ease of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices\nUnderstanding the data stored in S3 and its relation to other S3 databases will be a huge asset in using this connector.\nPrerequisites\nTo connect to your Amazon S3 account and create a DataSet, you must have the following:\nYour AWS access key. You can find this in the\u00a0Security Credentials\u00a0section of the AWS Console. Alternatively, if you are using IAM, you can find it under\u00a0Users.Your AWS secret key was provided when you created your access key. You can generate a new secret key in the AWS Console.The name of the AWS S3 Bucket you want to retrieve files from.\nCreating a User with the Proper Permissions\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.\nTo configure your user in the IAM Amazon Console,\nAdd a new user, setting options as follows:In the\u00a0Details\u00a0pane, check the box for\u00a0Programmatic Access\u00a0under\u00a0Select AWS access type.\u00a0In the\u00a0Permissions\u00a0pane, select\u00a0Attach existing policies directly, then check the box for either\u00a0AmazonS3FullAccess\u00a0or\u00a0AmazonS3ReadOnlyAccess.Customer-managed policies\u00a0do not work.In the\u00a0Review\u00a0pane, click\u00a0Create User.After you create your user, copy the access and secret keys to use in the\u00a0Credentials\u00a0pane in Domo.\nConnecting to Your Amazon S3 Account", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "56ff177fbc19-2", "text": "Connecting to Your Amazon S3 Account\nThis section enumerates the options in the\u00a0\u00a0Credentials and\u00a0\u00a0Details panes on the Amazon S3 Advanced Connector page. The components of the other panes on this page,\u00a0Scheduling,\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Amazon S3 account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see \"Prerequisites,\" above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see \"Prerequisites,\" above.BucketEnter the Amazon S3 Bucket you want to pull files from.RegionSelect the desired Amazon S3 Bucket region.\nOnce you have entered valid Amazon S3 Advanced Miltipart credentials, you can use the same account any time you go to create a new Amazon S3 Advanced DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nTo know more about the Credentials click on the link below:\u00a0\nUnderstanding and getting your AWS credentials - AWS General Reference (amazon.com)\nDetails Pane\nThis pane contains various menus for locating and configuring the file you want to pull into Domo.", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "56ff177fbc19-3", "text": "MenuDescriptionFile Type to ImportSelect the type of file you want to import into Domo, either CSV, JSON, or XML.How Would You Like to Choose Your Filename?Select how you want Domo to identify your file. Options are as follows:Complete FilenameYou enter the complete path to the file.Partial FilenameYou enter a partial file name and specify whether the file to retrieve starts with or contains the name you have entered.FilesDiscoveryA list of files in the selected bucket appears and you choose the one you want. This option is available\u00a0only\u00a0when your selected S3 bucket contains fewer than 100\u00a0files.Prefix (Optional)Enter a prefix to filter results by. A prefix limits results to only those keys that begin with the given prefix. Note that\u00a0this advanced version of the S3\u00a0connector imports\u00a0all\u00a0files with the provided prefix, assuming they all have the same schema.Enter Complete FilepathEnter the complete path for the desired file.File NameEnter the name of the file you want to import, with or without the file path. For example:\u00a0folder_name/file_nameFile Name Match TypeSpecify whether the file", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "56ff177fbc19-4", "text": "Name Match TypeSpecify whether the file you want to retrieve starts with or contains the text you entered under\u00a0File Name.File Compression TypeSelect the compression type for the file you want to retrieve. If the file is not compressed, select\u00a0None.Are Headers Present in CSV File?Select\u00a0Yes\u00a0if the CSV file you are importing contains headers; otherwise select\u00a0No.List of FilesSelect the file you want to retrieve.Select the Delimiting CharacterSelect the delimiter used in the CSV file you want to retrieve. If your delimiter is not listed, select\u00a0Other.Quote CharacterSelect a quote character to parse the CSV file you want to retrieve. A double quote (\") is the CSV standard.Escape CharacterSelect an escape character to parse the CSV file you want to retrieve. A backslash (\\) is the CSV standard.Enable Parsing for Large JSON Files?Specify whether parsing is enabled for large JSON files.Does Your JSON Text Require a Line Reader?Specify whether the text in your JSON file includes multiple lines that should be read.Should the Backslash be Escaped?Specify whether the text in your JSON file contains backslash characters that should", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "56ff177fbc19-5", "text": "text in your JSON file contains backslash characters that should be escaped.Enter Your Data TagEnter the tag for the data in your JSON or XML file.Enter Fields to ExcludeProvide a comma-separated list of fields to exclude in your JSON/XML import.Enter Your Header TagEnter the tag for the header in your JSON or XML file.Do You Require Attributes in Data?Specify whether you require attributes values as part of your XML data.Enter XPath ExpressionEnter your XPath expression.Is Password Protected?Select\u00a0Yes\u00a0if the file you are retrieving is password-protected; otherwise select\u00a0No.Header Start RowEnter the number of the header row in the Excel file you are retrieving.Data Start RowEnter the number of the first data row in the Excel file you are retrieving.FooterEnter the number of the footer row in the Excel file you are retrieving.Blank RowsSelect the action that should be taken if blank rows are encountered. If you select\u00a0Skip blank rows, any blank rows are skipped; if you select\u00a0Stop at the first blank column, the data import stops when a column with a blank row is", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "56ff177fbc19-6", "text": "import stops when a column with a blank row is encountered.Empty Column HeadersSpecify what should happen if empty column headers are encountered. If you choose\u00a0Add blank columns, column names are automatically generated for the new columns. If you select\u00a0Stop at the first blank column, the data import stops when an empty header is encountered.", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "56ff177fbc19-7", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nTroubleshooting\nEnsure that the file is present in the S3 bucket and that the correct file type is specified in the connector settings.NULL columns\u00a0must\u00a0be removed before the connector can successfully retrieve data.If you run into a \"Failed to Import Successfully\" error when trying to import a CSV file, you can often get around this by changing the option in the\u00a0Quote Character\u00a0menu to\u00a0No Quote Character.\nFAQ\nWhat are the Upload limits of the Amazon S3 Multipart Connector?\nAmazon S3 returns the parts information for the specified multipart upload, up to a maximum of 1,000 parts. If there are more than 1,000 parts in the multipart upload, you must send a series of list part requests to retrieve all the parts.\nPlease refer to the below link for more details:\nAmazon S3 multipart upload limits - Amazon Simple Storage Service\nFor more details about Multipart, you can refer to the link provided below:\u00a0\nUploading and copying objects using multipart upload - Amazon Simple Storage Service\nHow frequently will my data update?\nAs often as needed.\nCan I use the same Amazon S3 account to create multiple DataSets?\nYes.\nAre there any API limits that I need to be aware of?\nYou may encounter a limit of 100 Amazon S3 buckets per account.\nWhy is the connector unable to pull the data?\nEnsure that the file is present in the S3 bucket and specify the correct file type in the connector settings. Also, remove null columns to allow the connector to retrieve the data.\nWhat's the difference between the standard Amazon S3 connector and the advanced version?", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "56ff177fbc19-8", "text": "What's the difference between the standard Amazon S3 connector and the advanced version?\nThe Amazon S3 connector and the Amazon S3 Advanced connector both use the same SDK. They differ in how they handle multiple files.If your Amazon S3 bucket contains multiple files that begin with the prefix string you provide in the Details section of the connector, the Amazon S3 connector will only import the latest modified file. The Amazon S3 Advanced connector will import all files with the provided prefix, assuming they all have the same schema.", "source": "../../raw_kb/article/amazon_s3_advanced_multipart_connector/index.html", "title": "Amazon S3 Advanced Multipart Connector"}, {"objectID": "8fd92e9d4bf5-0", "text": "TitleAmazon S3 Analytics Assume Role ConnectorArticle BodyIntro\nAmazon S3 Analytics Assume Role Connector is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nThe Amazon S3 and Amazon S3 Advanced connectors are almost the same. The only difference is in how they handle multiple files\u00a0that begin with the prefix string you provide in the\u00a0Details\u00a0section of the connector.\nBy using Amazon S3 analytics\u00a0Storage Class Analysis\u00a0you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. For more information about storage classes, see\u00a0Using Amazon S3 storage classes.\nYou connect to your Amazon S3 Analytics Connector account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is an excellent choice for retrieving flat files when APIs are not an option.Primary MetricsNAPrimary Company RolesData specialistsMarketing rolesFinance rolesAnyone who has data stored in S3Average Implementation TimeLess than an hour if you have the correct file types in S3.Ease of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices\nUnderstanding the data stored in S3 Analytics Connector and its relation to other S3 databases will be a huge asset in using this connector.\nPrerequisites", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-1", "text": "Prerequisites\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.\nTo configure your user in the IAM Amazon Console,\nAdd a new user, setting options as follows:In the\u00a0Details\u00a0pane, check the box for\u00a0Programmatic Access\u00a0under\u00a0Select AWS access type.\u00a0In the\u00a0Permissions\u00a0pane, select\u00a0Attach existing policies directly, then check the box for either\u00a0AmazonS3FullAccess\u00a0or\u00a0AmazonS3ReadOnlyAccess.Customer-managed policies\u00a0do not work.In the\u00a0Review\u00a0pane, click\u00a0Create User.After you create your user, copy the access and secret keys to use in the\u00a0Credentials\u00a0pane in Domo.\nConnecting to Your\u00a0Amazon S3 Analytics Assume Connector\u00a0Account\nThis section enumerates the options in the Credentials and Details panes in the [insert Connector name here] Connector page. The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\n\u00a0\nCreating an IAM role (console)\nYou can use the AWS Management Console to create a role that an IAM user can assume. For example, assume that your organization has multiple AWS accounts to isolate a development environment from a production environment. For high-level information about creating a role that allows users in the development account to access resources in the production account, see\u00a0Example scenario using separate development and production accounts.", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-2", "text": "To create a role (console)\nSign in to the AWS Management Console and open the IAM console at\u00a0https://console.aws.amazon.com/iam/.In the navigation pane of the console, choose\u00a0Roles\u00a0and then choose\u00a0Create role.Choose\u00a0AWS account\u00a0role type.To create a role for your account, choose\u00a0This account. To create a role for another account, choose\u00a0Another AWS account\u00a0and enter the\u00a0Account ID\u00a0to which you want to grant access to your resources.\nThe administrator of the specified account can grant permission to assume this role to any IAM user in that account. To do this, the administrator attaches a policy to the user or a group that grants permission for the\u00a0sts:AssumeRole\u00a0action. That policy must specify the role's ARN as the\u00a0Resource.If you are granting permissions to users from an account that you do not control, and the users will assume this role programmatically, select\u00a0Require external ID. The external ID can be any word or number that is agreed upon between you and the administrator of the third-party account. This option automatically adds a condition to the trust policy that allows the user to assume the role only if the request includes the correct\u00a0sts:ExternalID. For more information, see\u00a0How to use an external ID when granting access to your AWS resources to a third party.\n\n\nImportant\n\n\nChoosing this option restricts access to the role only through the AWS CLI, Tools for Windows PowerShell, or the AWS API. This is because you cannot use the AWS console to switch to a role that has an\u00a0externalId\u00a0condition in its trust policy. However, you can create this kind of access programmatically by writing a script or an application using the relevant SDK. For more information and a sample script, see\u00a0How to Enable Cross-Account Access to the AWS Management Console\u00a0in the AWS Security Blog.", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-3", "text": "If you want to restrict the role to users who sign in with multi-factor authentication (MFA), select\u00a0Require MFA. This adds a condition to the role's trust policy that checks for an MFA sign-in. A user who wants to assume the role must sign in with a temporary one-time password from a configured MFA device. Users without MFA authentication cannot assume the role. For more information about MFA, see\u00a0Using multi-factor authentication (MFA) in AWSChoose\u00a0Next.IAM includes a list of the AWS managed and customer managed policies in your account. Select the policy to use for the permissions policy or choose\u00a0Create policy\u00a0to open a new browser tab and create a new policy from scratch. For more information, see\u00a0Creating IAM policies. After you create the policy, close that tab and return to your original tab. Select the check box next to the permissions policies that you want anyone who assumes the role to have. If you prefer, you can select no policies at this time, and then attach policies to the role later. By default, a role has no permissions.(Optional) Set a\u00a0permissions boundary. This is an advanced feature.", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-4", "text": "Open the\u00a0Set permissions boundary\u00a0section and choose\u00a0Use a permissions boundary to control the maximum role permissions. Select the policy to use for the permissions boundary.Choose\u00a0Next.For\u00a0Role name, enter a name for your role. Role names must be unique within your AWS account. They are not distinguished by case. For example, you cannot create roles named both\u00a0PRODROLE\u00a0and\u00a0prodrole. Because other AWS resources might reference the role, you cannot edit the name of the role after it has been created.(Optional) For\u00a0Description, enter a description for the new role.Choose\u00a0Edit\u00a0in the\u00a0Step 1: Select trusted entities\u00a0or\u00a0Step 2: Add permissions\u00a0sections to edit the use cases and permissions for the role.(Optional) Add metadata to the role by attaching tags as key\u2013value pairs. For more information about using tags in IAM, see\u00a0Tagging IAM resources.Review the role and then choose\u00a0Create role.", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-5", "text": "Credentials Pane\nThe Domo\u00a0Amazon S3 Analytics\u00a0Connector uses OAuth to connect, so there is no need to enter credentials within Domo. Click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if you have existing\u00a0Amazon S3 Analytics Connector\u00a0accounts in Domo to open the\u00a0Amazon S3 Analytics Connector\u00a0OAuth screen where you can enter your\u00a0Amazon S3 Analytics\u00a0username and password. Once you have entered valid\u00a0credentials, you can use the same account any time you go to create a new\u00a0Amazon S3 Analytics\u00a0DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\n\n\n\n\n\nNote:\u00a0If you are already logged into\u00a0Amazon S3 Analytics Assume Connector\u00a0when you connect in Domo, you are authenticated automatically when you click\u00a0Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of\u00a0Amazon S3 Analytics Connector.\n\n\n\nFieldDescriptionRole ARNEnter your Amazon Resource Name (ARN) of the role to assume.Session NameEnter the identifier for the assumed role session.External ID\u00a0\u00a0Generate the ExternalID by clicking the generate button and paste it into the trust relationship document of IAM Role.\u00a0\u00a0Region\u00a0Enter the amazon S3 Region.", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-6", "text": "Details Pane\nThis pane contains various menus for locating and configuring the file you want to pull into Domo.\nMenuDescriptionS3 Bucket RegionSelect the S3 Bucket Region where your file is located.", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-7", "text": "File Type to ImportSelect the type of file you want to import into Domo, either CSV, JSON, or XML.How Would You Like to Choose Your Filename?Select how you want Domo to identify your file. Options are as follows:Complete FilenameYou enter the complete path to the file.Partial FilenameYou enter a partial file name and specify whether the file to retrieve starts with or contains the name you have entered.FilesDiscoveryA list of files in the selected bucket appears and you choose the one you want. This option is available\u00a0only\u00a0when your selected S3 bucket contains fewer than 100\u00a0files.Prefix (Optional)Enter a prefix to filter results by. A prefix limits results to only those keys that begin with the given prefix. Note that with this standard version of the connector, only the latest modified file will be imported. The advanced version of the connector retrieves all files with the same prefix, provided they have the same schema. For more information, see\u00a0Amazon S3 Advanced Connector.Enter Complete FilepathEnter the complete path for the desired file.File NameEnter the name of the", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-8", "text": "for the desired file.File NameEnter the name of the file you want to import, with or without the file path. For example:\u00a0folder_name/file_nameFile Name Match TypeSpecify whether the file you want to retrieve starts with or contains the text you entered under\u00a0File Name.File Compression TypeSelect the compression type for the file you want to retrieve. If the file is not compressed, select\u00a0None.Are Headers Present in CSV File?Select\u00a0Yes\u00a0if the CSV file you are importing contains headers; otherwise select\u00a0No.List of FilesSelect the file you want to retrieve.Select the Delimiting CharacterSelect the delimiter used in the CSV file you want to retrieve. If your delimiter is not listed, select\u00a0Other.Quote CharacterSelect a quote character to parse the CSV file you want to retrieve. A double quote (\") is the CSV standard.Escape CharacterSelect an escape character to parse the CSV file you want to retrieve. A backslash (\\) is the CSV standard.Enable Parsing for Large JSON Files?Specify whether parsing is enabled for large JSON files.Does Your JSON Text Require a Line Reader?Specify whether the text in your", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-9", "text": "Require a Line Reader?Specify whether the text in your JSON file includes multiple lines that should be read.Should the Backslash be Escaped?Specify whether the text in your JSON file contains backslash characters that should be escaped.Enter Your Data TagEnter the tag for the data in your JSON or XML file.Enter Fields to ExcludeProvide a comma-separated list of fields to exclude in your JSON/XML import.Enter Your Header TagEnter the tag for the header in your JSON or XML file.Do You Require Attributes in Data?Specify whether you require attributes values as part of your XML data.Enter XPath ExpressionEnter your XPath expression.Is Password Protected?Select\u00a0Yes\u00a0if the file you are retrieving is password-protected; otherwise select\u00a0No.Header Start RowEnter the number of the header row in the Excel file you are retrieving.Data Start RowEnter the number of the first data row in the Excel file you are retrieving.FooterEnter the number of the footer row in the Excel file you are retrieving.Blank RowsSelect the action that should be taken if blank rows are encountered. If you select\u00a0Skip blank rows, any", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-10", "text": "encountered. If you select\u00a0Skip blank rows, any blank rows are skipped; if you select\u00a0Stop at the first blank column, the data import stops when a column with a blank row is encountered.Empty Column HeadersSpecify what should happen if empty column headers are encountered. If you choose\u00a0Add blank columns, column names are automatically generated for the new columns. If you select\u00a0Stop at the first blank column, the data import stops when an empty header is encountered.", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "8fd92e9d4bf5-11", "text": "Troubleshooting\nEnsure that the file is present in the S3 bucket and that the correct file type is specified in the connector settings.NULL columns\u00a0must\u00a0be removed before the connector can successfully retrieve data.If you run into a \"Failed to Import Successfully\" error when trying to import a CSV file, you can often get around this by changing the option in the\u00a0Quote Character\u00a0menu to\u00a0No Quote Character.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_s3_analytics_assume_role_connector/index.html", "title": "Amazon S3 Analytics Assume Role Connector"}, {"objectID": "ea4409ece4de-0", "text": "TitleAmazon S3 Analytics ConnectorArticle BodyIntro\nAmazon S3 Analytics Connector is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nThe Amazon S3 and Amazon S3 Advanced connectors are almost the same. The only difference is in how they handle multiple files\u00a0that begin with the prefix string you provide in the\u00a0Details\u00a0section of the connector.\nBy using Amazon S3 analytics\u00a0Storage Class Analysis\u00a0you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. For more information about storage classes, see\u00a0Using Amazon S3 storage classes.\nYou connect to your Amazon S3 Analytics Connector account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is an excellent choice for retrieving flat files when APIs are not an option.Primary MetricsNAPrimary Company RolesData specialistsMarketing rolesFinance rolesAnyone who has data stored in S3Average Implementation TimeLess than an hour if you have the correct file types in S3.Ease of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices\nUnderstanding the data stored in S3 Analytics Connector and its relation to other S3 databases will be a huge asset in using this connector.\nPrerequisites", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "ea4409ece4de-1", "text": "Prerequisites\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.\nTo configure your user in the IAM Amazon Console,\nAdd a new user, setting options as follows:In the\u00a0Details\u00a0pane, check the box for\u00a0Programmatic Access\u00a0under\u00a0Select AWS access type.\u00a0In the\u00a0Permissions\u00a0pane, select\u00a0Attach existing policies directly, then check the box for either\u00a0AmazonS3FullAccess\u00a0or\u00a0AmazonS3ReadOnlyAccess.Customer-managed policies\u00a0do not work.In the\u00a0Review\u00a0pane, click\u00a0Create User.After you create your user, copy the access and secret keys to use in the\u00a0Credentials\u00a0pane in Domo.\nConnecting to Your Amazon S3 Analytics Connector\u00a0Account\nThis section enumerates the options in the Credentials and Details panes in the [insert Connector name here] Connector page. The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThe Domo Amazon S3 Analytics\u00a0Connector uses OAuth to connect, so there is no need to enter credentials within Domo. Click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if you have existing Amazon S3 Analytics Connector accounts in Domo to open the Amazon S3 Analytics Connector OAuth screen where you can enter your Amazon S3 Analytics\u00a0username and password. Once you have entered valid\u00a0credentials, you can use the same account any time you go to create a new Amazon S3 Analytics DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "ea4409ece4de-2", "text": "Note:\u00a0If you are already logged into Amazon S3 Analytics Connector\u00a0when you connect in Domo, you are authenticated automatically when you click\u00a0Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of Amazon S3 Analytics Connector.\n\n\n\nFieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see \"Prerequisites,\" above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see \"Prerequisites,\" above.", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "ea4409ece4de-3", "text": "Details Pane\nThis pane contains various menus for locating and configuring the file you want to pull into Domo.\nMenuDescriptionS3 Bucket RegionSelect the S3 Bucket Region where your file is located.", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "ea4409ece4de-4", "text": "File Type to ImportSelect the type of file you want to import into Domo, either CSV, JSON, or XML.How Would You Like to Choose Your Filename?Select how you want Domo to identify your file. Options are as follows:Complete FilenameYou enter the complete path to the file.Partial FilenameYou enter a partial file name and specify whether the file to retrieve starts with or contains the name you have entered.FilesDiscoveryA list of files in the selected bucket appears and you choose the one you want. This option is available only when your selected S3 bucket contains fewer than 100\u00a0files.Prefix (Optional)Enter a prefix to filter results by. A prefix limits results to only those keys that begin with the given prefix. Note that with this standard version of the connector, only the latest modified file will be imported. The advanced version of the connector retrieves all files with the same prefix, provided they have the same schema. For more information, see\u00a0Amazon S3 Advanced Connector.Enter Complete FilepathEnter the complete path for the desired file.File NameEnter the name of the", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "ea4409ece4de-5", "text": "for the desired file.File NameEnter the name of the file you want to import, with or without the file path. For example: folder_name/file_nameFile Name Match TypeSpecify whether the file you want to retrieve starts with or contains the text you entered under File Name.File Compression TypeSelect the compression type for the file you want to retrieve. If the file is not compressed, select None.Are Headers Present in CSV File?Select Yes if the CSV file you are importing contains headers; otherwise select No.List of FilesSelect the file you want to retrieve.Select the Delimiting CharacterSelect the delimiter used in the CSV file you want to retrieve. If your delimiter is not listed, select Other.Quote CharacterSelect a quote character to parse the CSV file you want to retrieve. A double quote (\") is the CSV standard.Escape CharacterSelect an escape character to parse the CSV file you want to retrieve. A backslash (\\) is the CSV standard.Enable Parsing for Large JSON Files?Specify whether parsing is enabled for large JSON files.Does Your JSON Text Require a Line Reader?Specify whether the text in your", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "ea4409ece4de-6", "text": "Require a Line Reader?Specify whether the text in your JSON file includes multiple lines that should be read.Should the Backslash be Escaped?Specify whether the text in your JSON file contains backslash characters that should be escaped.Enter Your Data TagEnter the tag for the data in your JSON or XML file.Enter Fields to ExcludeProvide a comma-separated list of fields to exclude in your JSON/XML import.Enter Your Header TagEnter the tag for the header in your JSON or XML file.Do You Require Attributes in Data?Specify whether you require attributes values as part of your XML data.Enter XPath ExpressionEnter your XPath expression.Is Password Protected?Select Yes if the file you are retrieving is password-protected; otherwise select No.Header Start RowEnter the number of the header row in the Excel file you are retrieving.Data Start RowEnter the number of the first data row in the Excel file you are retrieving.FooterEnter the number of the footer row in the Excel file you are retrieving.Blank RowsSelect the action that should be taken if blank rows are encountered. If you select Skip blank rows, any", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "ea4409ece4de-7", "text": "encountered. If you select Skip blank rows, any blank rows are skipped; if you select Stop at the first blank column, the data import stops when a column with a blank row is encountered.Empty Column HeadersSpecify what should happen if empty column headers are encountered. If you choose\u00a0Add blank columns, column names are automatically generated for the new columns. If you select Stop at the first blank column, the data import stops when an empty header is encountered.", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "ea4409ece4de-8", "text": "Troubleshooting\nEnsure that the file is present in the S3 bucket and that the correct file type is specified in the connector settings.NULL columns must be removed before the connector can successfully retrieve data.If you run into a \"Failed to Import Successfully\" error when trying to import a CSV file, you can often get around this by changing the option in the\u00a0Quote Character\u00a0menu to\u00a0No Quote Character.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_s3_analytics_connector/index.html", "title": "Amazon S3 Analytics Connector"}, {"objectID": "d5ddcf4abbfd-0", "text": "TitleAmazon S3 AssumeRole Advanced ConnectorArticle BodyIntro\nAmazon S3 is built to store and retrieve any amount of data from anywhere. Like Amazon S3, Domo is built to scale with your business. Our customers collectively upload new data into their Domo environments millions of times each week. Have datasets that exceed 50 billion rows? No problem, Domo is built to handle huge amounts of data with speed. Domo\u2019s S3 connector will allow you to leverage all of your S3 data anytime, anywhere.Domo connects directly to S3 and delivers the information you need in real-time visualizations that make analysis easier. Plus, you can see your S3 data alongside metrics from any other system, all in a single platform, and get instant notifications when your metrics hit thresholds that you determine.If your Amazon S3 bucket contains multiple files that begin with the prefix string you provide in the Details section of the connector, the Amazon S3 AssumeRole Advanced connector will import all files with the provided prefix, assuming they all have the same schema. Use Domo's\u00a0Amazon S3 AssumeRole Advanced Connector to connect your S3 bucket data with the Amazon S3 AssumeRole Advanced integration that imports all files with the same prefix.\u00a0To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html).\nThe Amazon S3 AssumeRole Advanced connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking\u00a0File\u00a0in the toolbar at the top of the window.", "source": "../../raw_kb/article/amazon_s3_assumerole_advanced_connector/index.html", "title": "Amazon S3 AssumeRole Advanced Connector"}, {"objectID": "d5ddcf4abbfd-1", "text": "You connect to your Amazon S3 account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 AssumeRole Advanced connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding\u00a0a DataSet Using a Data Connector.\nBest Practices\nUnderstanding the data stored in S3 and its relation to other S3 databases will be a huge asset in using this connector.\nPrerequisites\nTo connect to your Amazon S3 account and create a DataSet, you must have the following:\nThe Amazon Resource Name (ARN) of the role to assume.The identifier for the assumed role session. You will need to set up a trust policy. This is described in continuation.The unique identifier used by third parties when assuming roles in their customers' accounts.The name of the Amazon S3 bucket you want to pull data from.Your\u00a0Amazon S3 Region.\nTrust policy configuration\nThe trust policy for the role session identifier should look as follows:\n{\"Effect\": \"Allow\",\"Principal\": {\"AWS\": \"arn:aws:iam::*accountId*:root\"},\"Action\": \"sts:AssumeRole\",\"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"*customer externalID*\"}}}\nAccount IDs for Domo environments are as follows:\nUS: 339405024189AU: 010251424122EMEA (IE): 687132894031JP: 622384692065CA: 710710207408\nFAQs\nQ:\u00a0What is ARN? Can it\u00a0be a user or a role?A: ARN is Amazon Resource Name (ARN). It must be a role.", "source": "../../raw_kb/article/amazon_s3_assumerole_advanced_connector/index.html", "title": "Amazon S3 AssumeRole Advanced Connector"}, {"objectID": "d5ddcf4abbfd-2", "text": "Q: What is the Role Session Name? Do I\u00a0need to establish it\u00a0on the trust policy or within the\u00a0AWS environment first?A: The Role Session Name is\u00a0the identifier for the assumed role session. It\u00a0can be any name you choose.\nQ: Can I get a detailed example of a principal trust policy and connector config?A: Below is what the trust policy should look like for a customer in us-east-1.\nNote: You\u00a0need to replace the EXTERNAL_ID with the ID generated by DOMO on the Connector Credentials section UI.\n{\"Effect\": \"Allow\",\"Principal\":\n{ \"AWS\": \"arn:aws:iam::339405024189:root\" }\n,\"Action\": \"sts:AssumeRole\",\"Condition\": {\"StringEquals\":\n{ \"sts:ExternalId\": \"EXTERNAL_ID\" }\n}}\nDescription for\u00a0the fields in the connector configuration:\nRole ARNThis is the ARN of the role that the customer created and added the trust policy to.\nRole Session NameThis can be left as the default - \"Domo\".\nExternal IDThis is the external ID that DOMO generated in the credentials section pane and put into their trust policy for the role.\nBucketThis is the S3 bucket the customer wants to get data out of.\nRegionThis is the AWS region in which their S3 bucket resides.\nConnecting to an Amazon S3 Bucket\nThis section enumerates the options in the\u00a0\u00a0Credentials\u00a0and\u00a0Details panes in the Amazon S3 AssumedRole\u00a0Advanced Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane", "source": "../../raw_kb/article/amazon_s3_assumerole_advanced_connector/index.html", "title": "Amazon S3 AssumeRole Advanced Connector"}, {"objectID": "d5ddcf4abbfd-3", "text": "Credentials Pane\nThis pane contains fields for entering credentials to connect to an Amazon S3 bucket. The following table describes what is needed for each field:\nFieldDescriptionRole ARNEnter the Amazon Resource Name (ARN) of the role you want to assume.Role Session NameEnter the identifier for the assumed role session.External IDEnter the unique identifier used by third parties when assuming roles in their customers' accounts.BucketEnter the Amazon S3 Bucket you want to pull files from.RegionSelect the S3 Bucket Region where your file is located.\nOnce you have entered valid Amazon S3 credentials, you can use the same account any time you go to create a new Amazon S3 AssumedRole\u00a0Advanced DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/amazon_s3_assumerole_advanced_connector/index.html", "title": "Amazon S3 AssumeRole Advanced Connector"}, {"objectID": "d5ddcf4abbfd-4", "text": "MenuDescriptionWhat File Type would you like to import?Select the file type that you would like to parse and import,\u00a0either CSV, JSON, TSV, TXT, XML, XLS, or XLSX.PrefixEnter a prefix to filter results by. A prefix limits the results to only those keys that begin with the specified prefix.File NameEnter the name of the Amazon S3 Object(file) that you would like to import.File Name Match TypeSpecify whether the file you want to retrieve starts with or contains the text you entered under\u00a0File Name.File Compression TypeSelect the compression type of your file, either Gzip, zip, or none.Subfile NameEnter the name or a portion of the name of the subfiles that you would like to import.Add Filename ColumnSpecify if the _BATCH_FILE_NAME_\u00a0column should be added to the final output or not.Select the Delimiting CharacterSelect the delimiting character used in your file. If your delimiter is not listed select 'Other.'Specify your DelimiterEnter the character used to delimit your character separated values (CSV) text.Quote CharacterSelect the desired quote character for parsing CSV files.", "source": "../../raw_kb/article/amazon_s3_assumerole_advanced_connector/index.html", "title": "Amazon S3 AssumeRole Advanced Connector"}, {"objectID": "d5ddcf4abbfd-5", "text": "CharacterSelect the desired quote character for parsing CSV files. Double quote is the default quote character for CSV standard.Custom\u00a0Quote CharacterEnter the desired CSV Quote character.Escape CharacterSelect the desired escape character for parsing CSV files.Custom Escape CharacterEnter the desired CSV escape character.Are Headers present in CSV file?Select YES if the file contains headers, else select NO.Date Columns and FormatsEnter the desired date column names and their respective formats as specified below, in the same order they exist in the file.Example: columnName1:dateFormat1,columnName2:dateFormat2columnName1:dd-MM-yyyy,columnName2:MM-dd-yyyyEnable parsing for large JSON files?Select Yes to enable parsing large JSON files.Does your JSON\u00a0text require a line reader?Select Yes if your JSON text includes multiple lines that should be read.Should the backslash be escaped?Select Yes if your JSON text has backslash characters that need to be escaped.Enter your data tagEnter the tag for the data in your file.Enter your sub list to flattenEnter the comma-separated lists that you would like to flatten out in your data.Enter fields to excludeProvide a comma-separated list of fields you want", "source": "../../raw_kb/article/amazon_s3_assumerole_advanced_connector/index.html", "title": "Amazon S3 AssumeRole Advanced Connector"}, {"objectID": "d5ddcf4abbfd-6", "text": "to excludeProvide a comma-separated list of fields you want to exclude from the import.Enter your header tagEnter the tag for the header in your JSON text.Header Start RowEnter the header start row number in the file.Data Start RowEnter the data start row number in the file.Footer Rows to SkipEnter the number of rows at the end of the file to skip. For example, to skip the last two rows you would enter 2.Sheet NameEnter the sheet name you want to retrieve from the specified spreadsheet be sure to check sheet name for accidental spaces, first sheet of the workbook will be used if the field is left blank.Enter XPath ExpressionEnter your XPath expression.Do you require Attributes in Data?Select 'Yes' if you require attributes values as a part of data.", "source": "../../raw_kb/article/amazon_s3_assumerole_advanced_connector/index.html", "title": "Amazon S3 AssumeRole Advanced Connector"}, {"objectID": "d5ddcf4abbfd-7", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_s3_assumerole_advanced_connector/index.html", "title": "Amazon S3 AssumeRole Advanced Connector"}, {"objectID": "2a47df2b724b-0", "text": "TitleAmazon S3 AssumeRole ConnectorArticle BodyIntro\nAmazon S3 is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Domo's AmazonS3 AssumeRole Connector allows you to pull data from S3 bucket using\u00a0AssumeRole\u00a0authentication. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nThe Amazon S3 AssumeRole connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking\u00a0File\u00a0in the toolbar at the top of the window.\nYou connect to your Amazon S3 account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is an excellent choice for retrieving flat files when APIs are not an option.Primary MetricsN/APrimary Company RolesData specialistsMarketing rolesFinance rolesAnyone who has data stored in S3Average Implementation TimeLess than an hour if you have the correct file types in S3.Ease of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices\nUnderstanding the data stored in S3 and its relation to other S3 databases will be a huge asset in using this connector.\nPrerequisites\nTo connect to your Amazon S3 account and create a DataSet, you must have the following:", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-1", "text": "To connect to your Amazon S3 account and create a DataSet, you must have the following:\nThe Amazon Resource Name (ARN) of the role to assume.The identifier for the assumed role session. You will need to set up a trust policy. This is described in continuation.The unique identifier used by third parties when assuming roles in their customers' accounts.The name of the Amazon S3 bucket you want to pull data from.\nTrust policy configuration\nThe trust policy for the role session identifier should look as follows:\n{\"Effect\": \"Allow\",\"Principal\": {\"AWS\": \"arn:aws:iam::*accountId*:root\"},\"Action\": \"sts:AssumeRole\",\"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"*customer externalID*\"}}}\nAccount IDs for Domo environments are as follows:\nUS: 339405024189AU: 010251424122EMEA: 687132894031JP: 622384692065CA: 710710207408\nSo\u00a0if you wanted to grant access to Australia-based user \"myIAMuser123,\" who has an account ID of \"46822464880681,\"\u00a0the trust policy would look like this:\n{\"Effect\": \"Allow\",\"Principal\": {\"AWS\": \" arn:aws:iam::46822464880681:user/myIAMuser123 \"},\"Action\": \"sts:AssumeRole\",\"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"010251424122\"}}}\nConnecting to an Amazon S3 Bucket\nThis section enumerates the options in the\u00a0 Credentials and\u00a0Details \u00a0panes in the Amazon S3 AssumedRole Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-2", "text": "Credentials Pane\nThis pane contains fields for entering credentials to connect to an Amazon S3 bucket. The following table describes what is needed for each field: \u00a0\nFieldDescriptionRole ARNEnter the Amazon Resource Name (ARN) of the role you want to assume.Role Session NameEnter the identifier for the assumed role session.External IDEnter the unique identifier used by third parties when assuming roles in their customers' accounts.BucketEnter the Amazon S3 Bucket you want to pull files from.RegionSelect the S3 Bucket Region where your file is located.\nOnce you have entered valid Amazon S3 credentials, you can use the same account any time you go to create a new Amazon S3 AssumedRole DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains various menus for locating and configuring the file you want to pull into Domo.", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-3", "text": "MenuDescriptionFile Type to ImportSelect the type of file you want to import into Domo, either CSV, JSON, TSV, TXT, XML, XLS, or XLSX.Does Your Filename Have an Extension?Select\u00a0Yes\u00a0if the filename ends with the supported file type extension; otherwise select\u00a0No.How Would You Like to Choose Your Filename?Select how you want Domo to identify your file. Options are as follows:Complete FilenameYou enter the complete path to the file.Partial FilenameYou enter a partial file name and specify whether the file to retrieve starts with or contains the name you have entered.FilesDiscoveryA list of files in the selected bucket appears and you choose the one you want. This option is available only when your selected S3 bucket contains fewer than 100 files.Enter Complete FilepathEnter the complete path for the desired file.List of Files from Amazon S3 BucketSelect the file you want to retrieve.Prefix (Optional)Enter a prefix to filter results by. A prefix limits results to only those keys that begin with the given prefix.File NameEnter the name of the file you want to import, with", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-4", "text": "the name of the file you want to import, with or without the file path. For example: folder_name/file_nameFile Name Match TypeSpecify whether the file you want to retrieve starts with or contains the text you entered under File Name.File Compression TypeSelect the compression type for the file you want to retrieve. If the file is not compressed, select None.Select a File from ZIP ArchiveSelect the ZIP file you want to import.Add FileName ColumnSelect\u00a0Yes\u00a0if you want a \"_BATCH_FILE_NAME_\" column to be added to your output data; otherwise select\u00a0No.Select the Delimiting CharacterSelect the delimiter used in the CSV file you want to retrieve. If your delimiter is not listed, select Other.Specify Your Delimiter (Conditional)Enter the character used to delimit your CSV text.Quote CharacterSelect a quote character to parse the CSV file you want to retrieve. A double quote (\") is the CSV standard. If your desired quote character is not in the list, select\u00a0Other.Custom Quote Character (Conditional)Enter the desired quote character.Escape CharacterSelect an escape character to parse the CSV file you want to retrieve.", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-5", "text": "character to parse the CSV file you want to retrieve. A backslash (\\) is the CSV standard.\u00a0If your desired escape\u00a0character is not in the list, select\u00a0Other.Custom Escape Character (Conditional)Enter the desired escape character.\u00a0Are Headers Present in CSV File?Select Yes if the CSV file you are importing contains headers; otherwise select No.Enter Your Header Tag (Conditional)Enter the tag for the header in your JSON\u00a0file.Does Your JSON Text Require a Line Reader?Specify whether the text in your JSON file includes multiple lines that should be read.Should the Backslash be Escaped?Specify whether the text in your JSON file contains backslash characters that should be escaped.Enter your Sublist to Flatten (Optional)Enter the comma-separated sublists you want to flatten in your data.Enter XPath Expression (Optional)Enter your XPath expression.Do You Require Attributes in Data?Specify whether you require attributes values as part of your XML data.Enter Your Data Tag (Optional)Enter the tag for the data in your JSON or XML file.Enter Fields to Exclude (Optional)Provide a comma-separated list of fields to exclude in your JSON/XML import.Sheet Name", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-6", "text": "of fields to exclude in your JSON/XML import.Sheet Name (Optional)Enter the sheet name you want to import from the specified Excel spreadsheet. Be sure to check the name for accidental spaces. If this field is left blank, the first sheet of the workbook will be used.File Password (Conditional)If your Excel file is password-protected, enter the file password.LayoutSelect the layout of your Excel file. The following options are available:Column as Header. Indicates that your Excel data uses a column-based layout (i.e. each column has a header).Row as Header. Indicates that your Excel data uses a row-based layout (i.e. each row has a header).Cross Tab. Indicates that your Excel data uses a cross-tab (pivot) layout.Raw. Indicates that your Excel data is raw (unformatted).Advanced. Use this option if you want to configure advanced layout options such as header and data start rows, footer rows to skip, date format, etc.\u00a0Mode\u00a0Select how the cell range of your Excel data will be determined. If you select\u00a0Auto, the range is determined automatically. If you", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-7", "text": "the range is determined automatically. If you select\u00a0Manual, you will be asked to enter the column header and data ranges manually.Enable Parsing for Large JSON Files?Specify whether parsing is enabled for large JSON files.Column Header RangeEnter the range for the column headers in your Excel sheet. For example:\u00a0A2:F8Row Header RangeEnter the range for the row\u00a0headers in your Excel sheet. For example:\u00a0A2:A8Data RangeEnter the range for the data (non-header) cells in your Excel sheet. For example:\u00a0A3:F20\u00a0Header Start Row\u00a0(Advanced only) (optional)Enter the header row number. If you do not enter a number, the first row is considered the header row.Data Start Row\u00a0(Advanced only)\u00a0(optional)Enter the starting data row number. If you do not enter a number, the first row after the header row is considered the starting row.Footer Rows to Skip\u00a0(Advanced only)\u00a0(optional)If there are one or more rows at the bottom of the file you do not want to import, enter the number of rows to skip.Date Format (Advanced only)\u00a0(optional)Select the date", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-8", "text": "to skip.Date Format (Advanced only)\u00a0(optional)Select the date format used in the Excel data. If you want dates to be represented as text, select\u00a0Show Dates as Strings.Blank Rows\u00a0(Advanced only)\u00a0(optional)Select the desired outcome if blank rows are encountered in the Excel file. If you select\u00a0Skip blank rows, the file is processed as normal without the blank rows. If you select\u00a0Stop at the first blank row, everything is pulled up until the first blank row.Empty Column Headers (Advanced only)\u00a0(optional)Select the desired outcome if empty column headers are encountered in the Excel. If you select\u00a0Add\u00a0blank columns, new columns are generated with default names applied. If you select\u00a0Stop at the first blank column, everything is pulled up until the first column with an empty header.", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "2a47df2b724b-9", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nTroubleshooting\nEnsure that the file is present in the S3 bucket and that the correct file type is specified in the connector settings.NULL columns must be removed before the connector can successfully retrieve data.An unescaped character in the data file could cause the \"Failed to execute import successfully\" error. If your dataset has this error, you should check that the Quote Character in the Details section is correct.", "source": "../../raw_kb/article/amazon_s3_assumerole_connector/index.html", "title": "Amazon S3 AssumeRole Connector"}, {"objectID": "e452cbaea496-0", "text": "TitleAmazon S3 AssumeRole Writeback ConnectorArticle BodyIntro\nAmazon S3 is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Use Domo\u2019s Amazon S3 AssumeRole Writeback connector to export your data from a Domo DataSet to an Amazon S3 bucket using an assumed role for the session.\u00a0To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nYou export data to an S3 bucket\u00a0in the Data Center. This topic discusses the fields and menus that are specific to the Domo\u00a0to S3 AssumeRole\u00a0Writeback\u00a0connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\n\n\n\n\nNote: The owner of a writeback dataset must also be an owner or co-owner of the input dataset.", "source": "../../raw_kb/article/amazon_s3_assumerole_writeback_connector/index.html", "title": "Amazon S3 AssumeRole Writeback Connector"}, {"objectID": "e452cbaea496-1", "text": "Prerequisites\nTo connect to your Amazon S3 account and create a DataSet, you must have the following:\nA Domo Client ID and Client Secret.The Amazon Resource Name (ARN) of the role to assume.The identifier for the assumed role session. You will need to set up a trust policy. This is described in continuation.The unique identifier used by third parties when assuming roles in their customers' accounts.The name of the Amazon S3 bucket you want to pull data from.\nTrust policy configuration\nThe trust policy for the role session identifier should look as follows:\n{\"Effect\": \"Allow\",\"Principal\": {\"AWS\": \"arn:aws:iam::*accountId*:root\"},\"Action\": \"sts:AssumeRole\",\"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"*customer externalID*\"}}}\nAccount IDs for Domo environments are as follows:\nUS: 339405024189AU: 010251424122EMEA (IE): 687132894031JP: 622384692065CA: 710710207408\nSo\u00a0if you wanted to grant access to Australia-based user \"myIAMuser123,\" who has an account ID of \"46822464880681,\"\u00a0the trust policy would look like this:\n{\"Effect\": \"Allow\",\"Principal\": {\"AWS\": \" arn:aws:iam::46822464880681:user/myIAMuser123 \"},\"Action\": \"sts:AssumeRole\",\"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"010251424122\"}}}\nTo obtain the Domo Client ID and Client Secret:\nLog into your\u00a0Domo developer account.In the top right corner, click\u00a0My Accounts\u00a0>>\u00a0New Client.Enter the name and description for your application.Provide the application scope by selecting the checkboxes for\u00a0Data\u00a0and\u00a0User.Click\u00a0Create.", "source": "../../raw_kb/article/amazon_s3_assumerole_writeback_connector/index.html", "title": "Amazon S3 AssumeRole Writeback Connector"}, {"objectID": "e452cbaea496-2", "text": "Once you have created a client, you can manage the client by clicking on\u00a0Manage Client. Your\u00a0Client Secret\u00a0will appear in the\u00a0Manage Client\u00a0section.", "source": "../../raw_kb/article/amazon_s3_assumerole_writeback_connector/index.html", "title": "Amazon S3 AssumeRole Writeback Connector"}, {"objectID": "e452cbaea496-3", "text": "Important: You will need the WRITE (s3:PutObject) permission on Amazon S3.", "source": "../../raw_kb/article/amazon_s3_assumerole_writeback_connector/index.html", "title": "Amazon S3 AssumeRole Writeback Connector"}, {"objectID": "e452cbaea496-4", "text": "Configuring the Connection\nThis section enumerates the options in the Credentials and Details panes in the Amazon S3 AssumeRole Writeback Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Domo\u00a0developer\u00a0account as well as your S3 bucket. The following table describes what is needed for each field:\nFieldDescriptionDomo Client IDEnter your Domo client ID. See Prerequisites for details.Domo Client SecretEnter your Domo client secret.\u00a0See Prerequisites for details.Role Session NameEnter the identifier for the assumed role session.Role ARNEnter the Amazon Resource Name (ARN) of the role you want to assume.External IDEnter the unique identifier used by third parties when assuming roles in their customers' accounts.BucketEnter the Amazon S3 Bucket you want to pull files from.RegionSelect the S3 Bucket Region where your file is located.\nOnce you have entered valid credentials, you can use the same account any time you go to create a new Domo-S3 connection. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a number of fields for specifying your data and indicating where it's going.", "source": "../../raw_kb/article/amazon_s3_assumerole_writeback_connector/index.html", "title": "Amazon S3 AssumeRole Writeback Connector"}, {"objectID": "e452cbaea496-5", "text": "This pane contains a number of fields for specifying your data and indicating where it's going.\nMenuDescriptionHow Would You Like to Select Your Folder Path?Specify whether you would like to search for your folder path or enter it manually.Select Folder PathSelect the folder path that you would like to use.PathEnter the folder path where you would like the file to be saved in your Amazon S3 bucket.OverwriteSelect this check box if you want to overwrite\u00a0the existing file in your\u00a0Amazon S3 bucket with selected filename.\u00a0How Would You Like to Select Your Input DataSet?Specify whether you would like to search for your DataSet ID or enter it manually.DataSet IDSelect the DataSet ID that you would like to use.\u00a0Input DataSet\u00a0IDEnter the DataSet\u00a0ID (GUID) for the DataSet you want to copy to the S3 Bucket.\nYou can find the DataSet ID by opening the details view for the DataSet in the Data Center and looking at the portion of the URL following\u00a0datasources/.\u00a0Example:\u00a0In the URL\u00a0https://mycompany.domo.com/datasourc...tails/overview, the DataSet ID is\u00a0845305d8-da3d-4107-a9d6-13ef3f86d4a4.Select FilenameSelect whether you want to reference your DataSet\u00a0using the\u00a0Input DataSet ID, Input DataSet Name,\u00a0or an output file name.\u00a0\nIf you choose\u00a0Use Input Dataset Name\u00a0or\u00a0Enter File Name\u00a0and your dataset name contains the macro <current_date> or <current_date-1> without spaces, then <current_date> will be replaced by today's date and <current_date-1> will be replaced by yesterday's date or -31 for 31 days ago in format yyyy-MM-dd.FilenameEnter the output file name.", "source": "../../raw_kb/article/amazon_s3_assumerole_writeback_connector/index.html", "title": "Amazon S3 AssumeRole Writeback Connector"}, {"objectID": "e452cbaea496-6", "text": "If you wish to add current date to the file name please use the macro <current_date> or <current_date-1>. Example: If fileName is\u00a0domo_<current_date>,\u00a0the output file generated in s3 bucket would be domo_(todays date in format yyyy-MM-dd) like domo_2019-07-16.Only Writeback\u00a0New DataCheck this box to only pull your last execution data from the input dataset and export it to S3 Object.File ExtensionSelect the extension that should be used as the file extension for your output file.DelimiterSelect the character that should be used as a delimiter for your output file.CompressionSelect the compression type that should be used for your output file.Encryption MethodSelect the encryption type that should be used for your output file.Encryption KeyEnter the encryption key.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_s3_assumerole_writeback_connector/index.html", "title": "Amazon S3 AssumeRole Writeback Connector"}, {"objectID": "c7b6f5cc0134-0", "text": "Title\n\nAmazon S3 Connector\n\nArticle Body", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "c7b6f5cc0134-1", "text": "Intro\nAmazon S3 is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nThe Amazon S3 and Amazon S3 Advanced connectors are almost the same. The only difference is in how they handle multiple files\u00a0that begin with the prefix string you provide in the\u00a0Details section of the connector.\u00a0The standard Amazon S3 connector will only import the latest modified file. The Amazon S3 Advanced connector will import all files with the provided prefix, assuming they all have the same schema. However, the Amazon S3 Advanced connector will only get files modified since the last run or new files after the first run. For more information about the advanced version of the connector, see\u00a0Amazon S3 Advanced Connector.\nThe Amazon S3 connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking\u00a0File\u00a0in the toolbar at the top of the window.\nYou connect to your Amazon S3 account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is an excellent choice for retrieving flat files when APIs are not an option.Primary MetricsN/APrimary Company RolesData specialistsMarketing rolesFinance rolesAnyone who has data stored in S3Average Implementation TimeLess than an hour if you have the correct file types in S3.Ease of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "c7b6f5cc0134-2", "text": "Best Practices\nUnderstanding the data stored in S3 and its relation to other S3 databases will be a huge asset in using this connector.\nPrerequisites\nTo connect to your Amazon S3 account and create a DataSet, you must have the following:\nYour AWS access key. You can find this in the Security Credentials section of the AWS Console. Alternatively, if you are using IAM, you can find it under Users.Your AWS secret key, which was provided when you created your access key. You can generate a new secret key in the AWS Console.The name of the AWS S3 Bucket you want to retrieve files from.\nCreating a User with the Proper Permissions\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.\nTo configure your user in the IAM Amazon Console,\nAdd a new user, setting options as follows:In the Details pane, check the box for Programmatic Access under Select AWS access type.  \u00a0In the Permissions pane, select Attach existing policies directly, then check the box for either AmazonS3FullAccess or AmazonS3ReadOnlyAccess.\t\tCustomer-managed policies do not work.  In the Review pane, click Create User.After you create your user, copy the access and secret keys to use in the Credentials pane in Domo.\nConnecting to Your Amazon S3 Account\nThis section enumerates the options in the\u00a0 Credentials \u00a0and\u00a0 Details \u00a0panes in the Amazon S3 Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Amazon S3 account. The following table describes what is needed for each field:", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "c7b6f5cc0134-3", "text": "FieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see \"Prerequisites,\" above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see \"Prerequisites,\" above.BucketEnter the Amazon S3 Bucket you want to pull files from.S3 Bucket RegionSelect the S3 Bucket Region where your file is located.\nOnce you have entered valid Amazon S3 credentials, you can use the same account any time you go to create a new Amazon S3 DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains various menus for locating and configuring the file you want to pull into Domo.", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "c7b6f5cc0134-4", "text": "MenuDescriptionWhat file type would you like to import?Select the file type you want to import into Domo: CSV, JSON, XML, Excel, TSV, or TXT.How Would You Like to Choose Your Filename?Select how you want Domo to identify your file. Options are as follows:Complete FilenameYou enter the complete path to the file.Partial FilenameYou enter a partial file name and specify whether the file to retrieve starts with or contains the name you have entered.FilesDiscoveryA list of files in the selected bucket appears and you choose the one you want. This option is available only when your selected S3 bucket contains fewer than 100\u00a0files.Prefix (Optional)Enter a prefix to filter results by. A prefix limits results to only those keys that begin with the given prefix. Note that with this standard version of the connector, only the latest modified file will be imported. The advanced version of the connector retrieves all files with the same prefix, provided they have the same schema. For more information, see\u00a0Amazon S3 Advanced Connector.S3 Path HelperEnter a prefix value to populate", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "c7b6f5cc0134-5", "text": "Advanced Connector.S3 Path HelperEnter a prefix value to populate S3 Path Helper.Enter Complete FilepathEnter the complete path for the desired file.File NameEnter the name of the file you want to import, with or without the file path. For example: folder_name/file_nameFile Name Match TypeSpecify whether the file you want to retrieve starts with or contains the text you entered under File Name.File Compression TypeSelect the compression type for the file you want to retrieve. If the file is not compressed, select None.Are Headers Present in CSV File?Select Yes if the CSV file you are importing contains headers; otherwise select No.List of FilesSelect the file you want to retrieve.Select the Delimiting CharacterSelect the delimiter used in the CSV file you want to retrieve. If your delimiter is not listed, select Other.Quote CharacterSelect a quote character to parse the CSV file you want to retrieve. A double quote (\") is the CSV standard.Escape CharacterSelect an escape character to parse the CSV file you want to retrieve. A backslash (\\) is the CSV standard.Add Filename ColumnSpecify if the\u00a0_BATCH_FILE_NAME_ column", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "c7b6f5cc0134-6", "text": "CSV standard.Add Filename ColumnSpecify if the\u00a0_BATCH_FILE_NAME_ column should be added to the final output or not.File EncodingSelect the appropriate file encoding. By default, the encoding is set to UTF-8.Use Previous Schema(Append Mode)Select the checkbox to use the previous schema in the subsequent runs of the dataset (applicable only in Append mode). If it is not selected, the previous schema will not be used.Enable Parsing for Large JSON Files?Specify whether parsing is enabled for large JSON files.Does Your JSON Text Require a Line Reader?Specify whether the text in your JSON file includes multiple lines that should be read.Should the Backslash be Escaped?Specify whether the text in your JSON file contains backslash characters that should be escaped.Enter Your Data TagEnter the tag for the data in your JSON or XML file.Enter Fields to ExcludeProvide a comma-separated list of fields to exclude in your JSON/XML import.Enter Your Header TagEnter the tag for the header in your JSON or XML file.Do You Require Attributes in Data?Specify whether you require attributes values as part of your XML data.Enter XPath ExpressionEnter your", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "c7b6f5cc0134-7", "text": "as part of your XML data.Enter XPath ExpressionEnter your XPath expression.Is Password Protected?Select Yes if the file you are retrieving is password-protected; otherwise select No.Header Start RowEnter the number of the header row in the Excel file you are retrieving.Data Start RowEnter the number of the first data row in the Excel file you are retrieving.FooterEnter the number of the footer row in the Excel file you are retrieving.Blank RowsSelect the action that should be taken if blank rows are encountered. If you select Skip blank rows, any blank rows are skipped; if you select Stop at the first blank column, the data import stops when a column with a blank row is encountered.Empty Column HeadersSpecify what should happen if empty column headers are encountered. If you choose\u00a0Add blank columns, column names are automatically generated for the new columns. If you select Stop at the first blank column, the data import stops when an empty header is encountered.", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "c7b6f5cc0134-8", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nTroubleshooting\nEnsure that the file is present in the S3 bucket and that the correct file type is specified in the connector settings.NULL columns must be removed before the connector can successfully retrieve data.If you run into a \"Failed to Import Successfully\" error when trying to import a CSV file, you can often get around this by changing the option in the\u00a0Quote Character\u00a0menu to\u00a0No Quote Character.\nFAQ\nHow frequently will my data update?\nAs often as needed.\nCan I use the same Amazon S3 account for multiple DataSets?\nYes.\nAre there any API limits that I need to be aware of?\nYou may encounter a limit of 100 Amazon S3 buckets per account.\nWhy is the connector unable to pull the data?\nEnsure that the file is present in the S3 bucket and specify the correct file type in the connector settings. Also, remove null columns to allow the connector to retrieve the data.\nWhat's the difference between the standard Amazon S3 connector and the advanced version?\nThe Amazon S3 connector and the Amazon S3 Advanced connector both use the same SDK. They differ in how they handle multiple files.If your Amazon S3 bucket contains multiple files that begin with the prefix string you provide in the Details section of the connector, the Amazon S3 connector will only import the latest modified file. The Amazon S3 Advanced connector will import all files with the provided prefix, assuming they all have the same schema. For more information about the advanced version, see\u00a0Amazon S3 Advanced Connector.", "source": "../../raw_kb/article/amazon_s3_connector/index.html", "title": "Amazon S3 Connector"}, {"objectID": "6a6601b923e0-0", "text": "TitleAmazon S3 Partition ConnectorArticle BodyIntro\nAmazon S3 is built to store and retrieve any amount of data from anywhere. Like Amazon S3, Domo is built to scale with your business. Our customers collectively upload new data into their Domo environments millions of times each week. Have datasets that exceed 50 billion rows? No problem, Domo is built to handle huge amounts of data with speed. Domo\u2019s S3 connector will allow you to leverage all of your S3 data anytime, anywhere.Domo connects directly to S3 and delivers the information you need in real-time visualizations that make analysis easier. Plus, you can see your S3 data alongside metrics from any other system, all in a single platform, and get instant notifications when your metrics hit thresholds that you determine.The Amazon S3 Partition connector allows you to provide the folder/child-folder path you want to fetch the file(s) from. This limits the search results to only the mentioned folders. If you leave that field empty, the partition connector will search for the specified file(s) in the entire S3 bucket. If you provide only parent folder, it will search for all files under all child folders of that parent folder. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nThe Amazon S3 Partition connector is a \"File\" connector, meaning it retrieves files and outputs them to Domo. In the Data Center, you can access the connector page for this and other File connectors by clicking\u00a0File\u00a0in the toolbar at the top of the window.\nYou connect to your Amazon S3 account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 Partition Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites", "source": "../../raw_kb/article/amazon_s3_partition_connector/index.html", "title": "Amazon S3 Partition Connector"}, {"objectID": "6a6601b923e0-1", "text": "Prerequisites\nTo connect to your Amazon S3 account and create a DataSet, you must have the following:\nYour AWS access key. You can find this in the\u00a0Security Credentials\u00a0section of the AWS Console. Alternatively, if you are using IAM, you can find it under\u00a0Users.Your AWS secret key, which was provided when you created your access key. You can generate a new secret key in the AWS Console.The name of the AWS S3 Bucket you want to retrieve files from.The region where your S3 bucket is located.\nCreating a User with the Proper Permissions\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.\nTo configure your user in the IAM Amazon Console,\nAdd a new user, setting options as follows:In the\u00a0Details\u00a0pane, check the box for\u00a0Programmatic Access\u00a0under\u00a0Select AWS access type.\u00a0In the\u00a0Permissions\u00a0pane, select\u00a0Attach existing policies directly, then check the box for either\u00a0AmazonS3FullAccess\u00a0or\u00a0AmazonS3ReadOnlyAccess.Customer-managed policies\u00a0do not work.In the\u00a0Review\u00a0pane, click\u00a0Create User.After you create your user, copy the access and secret keys to use in the\u00a0Credentials\u00a0pane in Domo.\nConnecting to Your Amazon S3\u00a0Account\nThis section enumerates the options in the Credentials and Details panes in the Amazon S3 Partition Connector page. The components of the other panes in this page, Scheduling and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Amazon S3 Partition\u00a0account. The following table describes what is needed for each field:", "source": "../../raw_kb/article/amazon_s3_partition_connector/index.html", "title": "Amazon S3 Partition Connector"}, {"objectID": "6a6601b923e0-2", "text": "FieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see\u00a0Prerequisites\u00a0above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see\u00a0Prerequisites\u00a0above.BucketEnter the Amazon S3 Bucket you want to pull files from.RegionSelect the desired Amazon S3 Bucket region.\nOnce you have entered valid\u00a0Amazon S3\u00a0credentials, you can use the same account any time you go to create a new\u00a0Amazon S3 Partition\u00a0DataSet.\u00a0You can manage Connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains various menus for locating and configuring the file you want to pull into Domo.\nMenuDescriptionWhat file type would you like to import?Select the file type that you would like to parse and import.PrefixEnter the folder/child-folder path you want to fetch the file(s) from. This limits the search results to only the mentioned folders. If you leave this field empty, the connector will search for the specified file(s) in the entire S3 bucket. If you provide only parent folder, the connector will search for all files under all child folders of that parent folder.\nExample: Parent Folder/Child folder1- The connector will search and return all files from the Parent Folder/Child Folder1 hierarchy.File NameEnter the name of Amazon S3 Object (file) that you would like to import.File Name Match TypeSpecify whether the file to be retrieved starts with or contains the name you have entered in\u00a0File Name.Past DaysEnter number of past days for which you want to get data. Value can be X, where X is a positive integer. For example: 30.Date FormatSelect the required date format. By default\u00a0yyyy-MM-dd\u00a0will be used.Custom Date FormatEnter the custom date format.\nOther Panes", "source": "../../raw_kb/article/amazon_s3_partition_connector/index.html", "title": "Amazon S3 Partition Connector"}, {"objectID": "6a6601b923e0-3", "text": "Other Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see Adding a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this Connector?\nYou need your AWS access key and secret key, the name of your AWS S3 bucket, and the region where your S3 bucket is located.\nHow frequently will my data update?", "source": "../../raw_kb/article/amazon_s3_partition_connector/index.html", "title": "Amazon S3 Partition Connector"}, {"objectID": "6a6601b923e0-4", "text": "As often as needed.\n\n\n\nCan I use the same Amazon S3 account to create multiple datasets?\nYes\n\n\n\n\nAre there any API limits that I need to be aware of?\u00a0\n\nYou may encounter a limit of 100 Amazon S3 buckets per account.\n\n\n\n\nWhat's the difference between the Amazon S3 and Amazon S3 Partition connectors?\u00a0\n\nIf your Amazon S3 bucket contains multiple files that begin with the prefix string you provide in the Details section of the connector, the Amazon S3 connector will only import the latest modified file. The Amazon S3 Partition connector allows you to provide the folder/child-folder path you want to fetch the file(s) from. This limits the search results to only the mentioned folders. If you leave that field empty, the partition connector will search for the specified file(s) in the entire S3 bucket. If you provide only parent folder, it will search for all files under all child folders of that parent folder.", "source": "../../raw_kb/article/amazon_s3_partition_connector/index.html", "title": "Amazon S3 Partition Connector"}, {"objectID": "3dace34aa357-0", "text": "TitleAmazon S3 Unload Writeback ConnectorArticle BodyIntro\nAmazon S3 is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web.\u00a0With the Amazon S3 Unload Writeback connector, you can export your data from a Domo DataSet to an Amazon S3 Bucket.\u00a0To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nYou export data to S3 in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3 Unload Writeback connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\n\n\n\n\nNote: The owner of a writeback dataset must also be an owner or co-owner of the input dataset.\n\n\n\n\nPrerequisites\nTo configure this connector, you will need the following:\nA Domo Client ID and Client Secret.\u00a0Your AWS access key. You can find this in the\u00a0Security Credentials\u00a0section of the AWS Console. Alternatively, if you are using IAM, you can find it under\u00a0Users.Your AWS secret key, which was provided when you created your access key. You can generate a new secret key in the AWS Console.The name of the AWS S3 Bucket you want to push data to.\nTo obtain your Domo Client ID and Client Secret, do the following:\nLog into the\u00a0Domo developer account.In the top right corner under\u00a0My Account\u00a0click\u00a0New Client.Enter the application name and description.Provide the application scope by selecting the checkboxes for\u00a0Data\u00a0and\u00a0User.Click\u00a0Create.Once you have created a client, you can manage the client by clicking on\u00a0Manage Client.\nYour\u00a0Client Secret\u00a0will appear in the\u00a0Manage Client\u00a0section.", "source": "../../raw_kb/article/amazon_s3_unload_writeback_connector/index.html", "title": "Amazon S3 Unload Writeback Connector"}, {"objectID": "3dace34aa357-1", "text": "Important: You will need the WRITE (s3:PutObject) permission on Amazon S3.", "source": "../../raw_kb/article/amazon_s3_unload_writeback_connector/index.html", "title": "Amazon S3 Unload Writeback Connector"}, {"objectID": "3dace34aa357-2", "text": "Configuring the Connection\nThis section enumerates the options in the Credentials and Details panes in the Amazon S3 Unload Writeback Connector page. The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Domo\u00a0developer\u00a0account as well as your S3 bucket. The following table describes what is needed for each field: \u00a0\nFieldDescriptionDomo Client IDEnter your Domo client ID.Domo Client SecretEnter your Domo client secret.Access KeyEnter your AWS access key. You can find this in the\u00a0Security Credentials\u00a0section of the AWS Console. Alternatively, if you are using IAM, you can find it under\u00a0Users.Secret KeyEnter your AWS secret key that was provided when you created your access key. You can generate a new secret key in the AWS Console.BucketEnter the name of the bucket you want to push Domo data to.RegionSelect the Amazon S3 region where your bucket is located.\nOnce you have entered valid credentials, you can use the same account any time you go to set up a new Domo-S3 connection. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a number of fields for specifying your data and indicating where it's going.\nMenuDescriptionInput DataSet IDEnter your Domo dataset ID (GUID) located in the dataset URL. You can find the ID by opening the details view for the DataSet in the Data Center and looking at the portion of the URL following\u00a0datasources/.", "source": "../../raw_kb/article/amazon_s3_unload_writeback_connector/index.html", "title": "Amazon S3 Unload Writeback Connector"}, {"objectID": "3dace34aa357-3", "text": "Example: https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settingsPathEnter the path where you would like the file to be saved in your Amazon S3 bucket.Primary KeyEnter the primary key of the datafusion dataset.Specific DayEnter an integer that specifies the day you want to get the data for. 0 pulls today's data, 1 pulls yesterday's data, and 7 pulls the data from 7 days ago each time the connector runs. Only a single day of data is returned for each connector run.Select the Data Type for Date Column(s) to be parsedDate TimeDescriptionParse as is (Do not convert)Metrics columns say- 'started' & 'finished' will be shown as is received in the response.Parse and convert into Date TimeMetrics columns say- 'started' & 'finished' will be parsed into Date Time.", "source": "../../raw_kb/article/amazon_s3_unload_writeback_connector/index.html", "title": "Amazon S3 Unload Writeback Connector"}, {"objectID": "3dace34aa357-4", "text": "Note: This is just for the dataset metrics to be displayed; there will be no impact on the data to be unloaded.\n\n\n\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_s3_unload_writeback_connector/index.html", "title": "Amazon S3 Unload Writeback Connector"}, {"objectID": "f6927590875c-0", "text": "TitleAmazon S3 User-Specific Permissions ConnectorArticle BodyIntro\nAmazon S3 is an online file storage web service\u00a0that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nYou set up your Amazon S3\u00a0connection\u00a0in the Data Center. This topic discusses the fields and menus that are specific to the Amazon S3\u00a0User-Specific Permissions Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Amazon S3 account and create a DataSet, you must have the following:\nYour AWS access key. You can find this in the\u00a0Security Credentials\u00a0section of the AWS Console. Alternatively, if you are using IAM, you can find it under\u00a0Users.Your AWS secret key, which was provided when you created your access key. You can generate a new secret key in the AWS Console.The name of the AWS S3 Bucket you want to retrieve files from.The prefix path for the file you want to pull data from.\u00a0Domo initially populates all the S3 Objects present from the prefix path (in the Credentials section), then\u00a0filters these results based on the Input Format, Partial File Name and File Name Match (from the Details section). The Prefix can be /path/directory/subdir/subdir/subdir/.\nConnecting to Your Amazon S3 Account\nThis section enumerates the options in the Credentials and Details panes in the Amazon S3 Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane", "source": "../../raw_kb/article/amazon_s3_userspecific_permissions_connector/index.html", "title": "Amazon S3 User-Specific Permissions Connector"}, {"objectID": "f6927590875c-1", "text": "Credentials Pane\nThis pane contains fields for entering credentials to connect to your Amazon S3 account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see \"Prerequisites,\" above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see \"Prerequisites,\" above.BucketEnter the Amazon S3 Bucket you want to pull files from.PrefixEnter the prefix path for the file you want to retrieve. For more information about the prefix path, see \"Prerequisites,\" above.\nOnce you have entered valid Amazon S3 credentials, you can use the same account any time you go to create a new Amazon S3\u00a0User-Specific Permissions DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/amazon_s3_userspecific_permissions_connector/index.html", "title": "Amazon S3 User-Specific Permissions Connector"}, {"objectID": "f6927590875c-2", "text": "MenuDescriptionFile Type to ImportSelect the type of file you want to import into Domo.\u00a0Available file types include\u00a0CSV, JSON, XML, XLS, TSV, and TXT.How Would You Like to Choose Your Filename?Select how you want Domo to identify your file. Options are as follows:Complete FilenameYou enter the complete path to the file.Partial FilenameYou enter a partial file name and specify whether the file to retrieve starts with or contains the name you have entered.FilesDiscoveryA list of files in the selected bucket appears and you choose the one you want. This option is available\u00a0only\u00a0when your selected S3 bucket contains fewer than 100\u00a0files.Partial FilenameEnter the name of the Amazon S3 file you want to import.  Example: folder_name/file_name.   If more than one file begins with or contains this string, the connector will choose the last modified file to import.File Name Match TypeSpecify whether the filename you want to retrieve starts with or contains the text you entered into the\u00a0Partial Filename\u00a0field.File Compression TypeSelect the compression type for the file you want to retrieve.", "source": "../../raw_kb/article/amazon_s3_userspecific_permissions_connector/index.html", "title": "Amazon S3 User-Specific Permissions Connector"}, {"objectID": "f6927590875c-3", "text": "the compression type for the file you want to retrieve. If the file is not compressed, select\u00a0None.ZIP File EncodingSelect the desired ZIP file encoding type.How Would You Like to Choose Your File Inside a ZIP?Select whether you want to enter the name of your ZIP file or have it be discovered.Enter File Name Inside ZIPEnter the file name inside the ZIP file you want to retrieve.Are Headers Present in CSV File?Select\u00a0Yes\u00a0if the CSV/TSV file you are importing contains headers; otherwise select\u00a0No.Select the Delimiting CharacterSelect the delimiter used in the CSV/TSV file you want to retrieve. If your delimiter is not listed, select\u00a0Other.Specify Your DelimiterEnter the custom delimiter used in the CSV/TSV file you want to retrieve.Quote CharacterSelect a quote character to parse the CSV/TSV file you want to retrieve. A double quote (\") is the CSV standard.Escape CharacterSelect an escape character to parse the CSV/TSV file you want to retrieve. A backslash (\\) is the CSV standard.Custom Escape CharacterEnter the custom escape character used in the", "source": "../../raw_kb/article/amazon_s3_userspecific_permissions_connector/index.html", "title": "Amazon S3 User-Specific Permissions Connector"}, {"objectID": "f6927590875c-4", "text": "Escape CharacterEnter the custom escape character used in the CSV/TSV file you want to retrieve.Add Filename Column?Choose Yes to add a column to your data that specifies the name of the file imported. This column is named \"BATCH_FILE_NAME_.\"File EncodingSelect the desired file encoding. The default is UTF-8.Enable Parsing for Large JSON Files?Specify whether parsing is enabled for large JSON files.Enter Your Sublist to Flatten (Optional)Enter the comma-separated lists you want to\u00a0flatten out in your data.Does Your JSON Text Require a Line Reader?Specify whether the text in your JSON file includes multiple lines that should be read.Should the Backslash be Escaped?Specify whether the text in your JSON file contains backslash characters that should be escaped.Do You Require Attributes in Data?Select Yes\u00a0if you require attributes values as a part of the data.Enter Your Data Tag (Optional)Enter the tag for the data in your JSON or XML file.Enter Fields to Exclude (Optional)Provide a comma-separated list of fields to exclude in your JSON/XML import.Enter Your Header Tag (Optional)Enter the tag for the header in your JSON", "source": "../../raw_kb/article/amazon_s3_userspecific_permissions_connector/index.html", "title": "Amazon S3 User-Specific Permissions Connector"}, {"objectID": "f6927590875c-5", "text": "(Optional)Enter the tag for the header in your JSON or XML file.Is Password Protected?Select\u00a0Yes\u00a0if the file you are retrieving is password-protected; otherwise select\u00a0No.Header Start RowEnter the number of the header row in the Excel file you are retrieving.Data Start RowEnter the number of the first data row in the Excel file you are retrieving.FooterEnter the number of the footer row in the Excel file you are retrieving.Blank RowsSelect the action that should be taken if blank rows are encountered. If you select\u00a0Skip blank rows, any blank rows are skipped; if you select\u00a0Stop at the first blank column, the data import stops when a column with a blank row is encountered.Empty Column HeadersSpecify what should happen if empty column headers are encountered. If you choose\u00a0Add blank columns, column names are automatically generated for the new columns. If you select\u00a0Stop at the first blank column, the data import stops when an empty header is encountered.", "source": "../../raw_kb/article/amazon_s3_userspecific_permissions_connector/index.html", "title": "Amazon S3 User-Specific Permissions Connector"}, {"objectID": "f6927590875c-6", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\nWhy is the Connector not pulling data?\nEnsure that the file is present in the S3 bucket, and specify the correct file type in the Connector settings. Also, remove NULL columns to allow the Connector to retrieve the data.\nHow can I import files from subdirectories?\nDomo initially populates all the S3 Objects present from the prefix path (in the Credentials section), then filters these results based on the Input Format, Partial File Name and File Name Match (from the Details section). The Prefix can be /path/directory/subdir/subdir/subdir/.\nHow frequently will my data update?\nAs often as needed.\nCan I use the same Amazon S3 account for multiple DataSets?\nYes.\nI don't see the files I expect in my DataSet? What's going on here?\nThis Connector pulls in only one file. If you select Partial File Name, and there are multiple files in your prefix path that match, the connector imports only the most recently modified file.\nI want to import multiple files into one DataSet from my Amazon S3 bucket. How do I do this?\nThis connector only pulls in one file. If you want multiple Amazon S3 files in one Domo DataSet, use the Amazon S3 Advanced Connector (documentation still in progress).", "source": "../../raw_kb/article/amazon_s3_userspecific_permissions_connector/index.html", "title": "Amazon S3 User-Specific Permissions Connector"}, {"objectID": "5dc3e7c9c7d4-0", "text": "TitleAmazon S3 Writeback ConnectorArticle BodyIntro\nAmazon S3 is an online file storage web service offered by Amazon Web Services that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Use this connector to export your data from a Domo DataSet to an\u00a0Amazon S3 Bucket. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nYou export data to S3\u00a0in the Data Center. This topic discusses the fields and menus that are specific to the Domo\u00a0to S3 Writeback\u00a0connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\n\n\n\n\n\nNote: The owner of a writeback dataset must also be an owner or co-owner of the input dataset.\n\n\n\n\nPrerequisites\nTo configure this connector, you will need the following:\nYour Domo Client ID and Client Secret.\u00a0Your AWS access key. You can find this in the\u00a0Security Credentials\u00a0section of the AWS Console. Alternatively, if you are using IAM, you can find it under\u00a0Users.Your AWS secret key, which was provided when you created your access key. You can generate a new secret key in the AWS Console.The name of the AWS S3 Bucket you want to push data to.\nTo obtain your Domo Client ID and Client Secret, do the following:\nLog into the\u00a0Domo developer account.In the top right corner under\u00a0My Account\u00a0click\u00a0New Client.Enter the application name and description.Provide the application scope by selecting the checkboxes for\u00a0Data\u00a0and\u00a0User.Click\u00a0Create.Once you have created a client, you can manage the client by clicking on\u00a0Manage Client.\nYour\u00a0Client Secret\u00a0will appear in the\u00a0Manage Client\u00a0section.", "source": "../../raw_kb/article/amazon_s3_writeback_connector/index.html", "title": "Amazon S3 Writeback Connector"}, {"objectID": "5dc3e7c9c7d4-1", "text": "Important: You will need the WRITE (s3:PutObject) permission on Amazon S3.", "source": "../../raw_kb/article/amazon_s3_writeback_connector/index.html", "title": "Amazon S3 Writeback Connector"}, {"objectID": "5dc3e7c9c7d4-2", "text": "Configuring the Connection\nThis section enumerates the options in the Credentials and Details panes in the Domo to S3 Writeback Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Domo\u00a0developer\u00a0account as well as your S3 bucket. The following table describes what is needed for each field: \u00a0\nFieldDescriptionDomo Client IDEnter your Domo client ID.Domo Client SecretEnter your Domo client secret.Access KeyEnter your AWS access key.Secret KeyEnter your AWS\u00a0secret key.BucketEnter the name of the bucket you want to push Domo data to.RegionSelect the Amazon S3 region where your bucket is located.\nOnce you have entered valid credentials, you can use the same account any time you go to set up a new Domo-S3 connection. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a number of fields for specifying your data and indicating where it's going.", "source": "../../raw_kb/article/amazon_s3_writeback_connector/index.html", "title": "Amazon S3 Writeback Connector"}, {"objectID": "5dc3e7c9c7d4-3", "text": "This pane contains a number of fields for specifying your data and indicating where it's going.\nMenuDescriptionHow would you like to select your folder path?Specify how would you like to select the path for your Amazon S3 folder.Search Folder PathSelect the folder path where you would like to save the file in your Amazon S3 bucket.Enter Folder PathEnter the folder path manually where you would like to save the file in your Amazon S3 bucket.Parameterized Date PathSelect the date (specific or relative) that you would like to use in the folder path where you want to save the file in your Amazon S3 bucket. If the folder does not already exist (in the S3 bucket), the connector will create one with the specified date parameter.PathEnter the path in your selected Amazon S3 bucket where you would like the data to be saved.", "source": "../../raw_kb/article/amazon_s3_writeback_connector/index.html", "title": "Amazon S3 Writeback Connector"}, {"objectID": "5dc3e7c9c7d4-4", "text": "Note: Ensure to add a front slash (/) at the end of your path.\nExample: testFolder1/testFolder2/", "source": "../../raw_kb/article/amazon_s3_writeback_connector/index.html", "title": "Amazon S3 Writeback Connector"}, {"objectID": "5dc3e7c9c7d4-5", "text": "Select Folder PathSelect the folder path that you would like to use.DatesSelect the date.Overwrite?Select this checkbox if you want to overwrite the existing file in your Amazon S3 bucket with the selected filename.\u00a0Skip HeaderCheck this box to skip header from the input dataset and export it to an S3 object.Remove QuotesCheck this box to remove quotes from the input dataset and export it to an S3 object.How would you like to select your input dataset?Specify whether you would like to search for your dataset ID or enter it manually.Input DataSet IDEnter the DataSet\u00a0ID (GUID) for the DataSet you want to copy to S3. You can find the ID by opening the details view for the DataSet in the Data Center and looking at the portion of the URL following datasources/.", "source": "../../raw_kb/article/amazon_s3_writeback_connector/index.html", "title": "Amazon S3 Writeback Connector"}, {"objectID": "5dc3e7c9c7d4-6", "text": "Example, in the URL\u00a0https://mycompany.domo.com/datasources/845305d8-da3d-4107-a9d6-13ef3f86d4a4/details/overview, the DataSet ID is\u00a0845305d8-da3d-4107-a9d6-13ef3f86d4a4.\u00a0Dataset IdSelect the dataset Id that you would like to use.Select FilenameSpecify how would you like to select the output filename.Use Input DataSet IDThe connector will use your Domo dataset ID (located in the dataset URL) as the output filename.Use Input DataSet NameThe connector will use your Domo dataset name as the output filename. If your dataset name contains a macro <current_date> or <current_date-1> without spaces, then <current_date> will be replaced by today's date and <current_date-1> will be replaced by yesterday's date or -31 for 31 days ago in format yyyy-MM-dd.Enter FilenameEnter the output filename that you want to save your dataset in.Parameterized File NameSelect the date (specific or relative) that you would like to use in the filename where you would like to save your dataset. If the filename does not already exist (in the S3 bucket), the connector will create one with the specified date parameter.FilenameEnter the output file name. \nIf you wish to add current date to the file name please use the macro <current_date> or <current_date-1>.", "source": "../../raw_kb/article/amazon_s3_writeback_connector/index.html", "title": "Amazon S3 Writeback Connector"}, {"objectID": "5dc3e7c9c7d4-7", "text": "Example: If fileName is domo_<current_date>, the output file generated in s3 bucket would be domo_(todays date in format yyyy-MM-dd) like domo_2019-07-16.Only Writeback New DataCheck this box to only pull your last execution data from the input dataset and export it to S3 Object.File ExtensionSelect the extension that you would like to use for your output file.Enter File ExtensionEnter the file extension.DelimiterSelect the character that should be used as a delimiter for your output file.CompressionSelect the compression type that should be used for your output file.Encryption MethodSelect the encryption type that you would like to use for your output file.Encryption KeyEnter the encryption key.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/amazon_s3_writeback_connector/index.html", "title": "Amazon S3 Writeback Connector"}, {"objectID": "dff82a63f966-0", "text": "TitleAmazon Selling PartnerArticle BodyIntro\nSelling Partner API is a REST-based API that helps Amazon sellers programmatically access their data on listings, orders, payments, reports, and more. Applications using Selling Partner API can increase selling efficiency, reduce labor requirements, and improve response time to customers, helping sellers grow their businesses. Selling Partner API builds on the functionality of Amazon Marketplace Web Service (Amazon MWS), but provides features to improve usability and security for developers and the sellers they work with. This guide documents the Amazon Selling Partner connector.\nYou connect to your Amazon Selling Partner account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon Selling Partner connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in \u00a0a DataSet Using a Data Connector.\nBest Practices\nFor most reports, be conservative on date ranges. Only pull the data you need.Be prepared for some reports to take hours to complete; this is a limitation of the Amazon API.Be advised that years-old data takes much longer to retrieve.\nPrerequisites\nTo connect to your Amazon Selling Partner account and create a DataSet, you must have the following:\nParameter NameWhere to find itUsernameCredentials used to log into Seller CentralPasswordCredentials used to log into Seller CentralCountry CodeSelect a Country code only for UK-EU Marketplace for others it will be NAMarketplaceSelect desired marketplace\nConnecting to Your Amazon Selling Partner\u00a0Account\nThis section enumerates the options in the Credentials and Details panes on the Amazon Selling Partner Connector page. The components of the other panes on this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-1", "text": "Credentials Pane\nThe Domo Amazon Selling Partner Connector uses OAuth to connect, so there is no need to enter credentials within Domo. Click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if you have existing Amazon Selling Partner accounts in Domo) to open the Amazon Selling Partner OAuth screen where you can enter your Amazon Selling Partner username and password. Once you have entered valid Amazon Selling Partner credentials, you can use the same account any time you go to create a new Amazon Selling Partner DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about the accounts tab, see\u00a0Managing User Accounts for Connectors.", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-2", "text": "Note:\u00a0If you are already logged into Amazon Selling Partner\u00a0when you connect in Domo, you are authenticated automatically when you click\u00a0Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of amazon\u00a0Selling Partner.", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-3", "text": "Details Pane\nThis pane contains a primary\u00a0Report\u00a0menu, along with various other fields which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-4", "text": "MenuDescriptionReportSelect the Amazon Selling Partner\u00a0report you want to run. The following reports are available:Fulfillment Inventory - List Inventory SupplyReturns a list of inventory summaries that have had changes after the date specified.Products - My Price for SKUReturns pricing information for your own offer listings, based on SellerSKU.Products - Competitive Prices By SKUReturns the current competitive price of a product, based on SellerSKU.Products - Product Categories for SKUReturns the parent product categories that a product belongs to, based on SellerSKU.Products - List Matching ProductsReturns a list of products and their attributes, based on a search query.Fulfillment Inbound Shipment - Inbound Shipment ItemsReturns a list of items in a specified inbound shipment, or a list of items that were updated within a specified time frame.Fulfillment Inbound Shipment - List Inbound ShipmentsReturns a list of inbound shipments based on criteria that you specify.Fulfillment Outbound Shipment - Fulfillment OrdersReturns a list of outbound shipments fulfilled by Amazon/Seller. This date is used to select fulfillment orders that were last", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-5", "text": "is used to select fulfillment orders that were last updated after (or at) the specified day. An update is defined as any change in fulfillment order status, including the creation of a new fulfillment order.Marketplace ParticipationsReturns a list of marketplaces that the seller submitting the request can sell in and information about the seller's participation in those marketplaces.Order DetailsReturns orders created or updated during a time frame that you specify. The update mode will be set to append by the connector.Order ItemsReturns line-item-level data for orders between the dates you specify. The update mode will be set to append by the connector.Orders - List Orders - UpsertReturns orders updated since the date you specify. The connector will automatically get the latest data up to 15 minutes ago. The update mode will be set to merge by the connector.Orders - List Order Items By Input DatasetReturns line item level data for orders you specify through an input dataset. The input dataset should be the dataset created for either Order Details or Orders - List Orders - Upsert report. The update", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-6", "text": "Orders - List Orders - Upsert report. The update mode will be set to merge by the connector.Orders - List Order Items UpsertReturns line item level data for orders created since the date you specify. The connector will automatically get the latest data up to 15 minutes ago. The update mode will be set to merge by the connector.Create and Download ReportAllows you to request a number of pre-built reports from Amazon that can help selling partners manage their businesses.\t\t\t\t\t\tThe following describes the different waiting periods for different ReportTypes:Report-Creation Time (when they select a report)Report-Import Time (when they run or re-run a report)\u00a0Frustration Time (when they are unable to understand as to why their report is failing)Report Detail FilterSelect the desired report detail filter.CSV Report Quote Character", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-7", "text": "Select the quote character to use to delineate columns in the CSV file version of your report. Double Quote is the default character. If your report data contains double quotes, try selecting the Null Character option.\nSelect StatusesSelect the status you want to retrieve data for. You can select up to 10 statuses.SKU Selection MethodSelect the method to enter SKUs. Selecting All will return all available SKUs from your Inventory Supply. We will dynamically show SKUs based on the query date selected. To enter a list of SKUs yourself, use the Advanced option.Enter Seller SKUsEnter the list of SKUs you want to retrieve data for, separated by commas.\u00a0Duration\nSelect the duration for the report (a Single Date, or a Date Range).", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-8", "text": "Select the duration for the report (a Single Date, or a Date Range).\nDays BackEnter the number of past days that should appear in the report. Specify either\u00a0today\u00a0(or\u00a00),\u00a0yesterday\u00a0(or\u00a01), or\u00a0today-7\u00a0(or\u00a07) to get data for 7 days into the past. \u00a0Date FromSelect report start date using relative or specific dates. Relative meaning number of days from today or a specific date using the date selector.Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0Select Specific Start DateSelect the first date in your date range.Select Specific End DateSelect the second date in your date range.Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Specify either\u00a0today\u00a0(or\u00a00),\u00a0yesterday\u00a0(or\u00a01), or\u00a0today-7\u00a0(or\u00a07) to get data for 7 days into the past. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Specify either\u00a0today\u00a0(or\u00a00),\u00a0yesterday\u00a0(or\u00a01), or\u00a0today-7\u00a0(or\u00a07) to get data for 7 days into the past. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-9", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for 10 days ago up until 5 days ago.Query Date TypeSelect the date type for the query.\u00a0Created\u00a0returns data based on the creation date.\u00a0Last Updated\u00a0will return data based on the date of the last update.Continue on Invalid SKU Error", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-10", "text": "Amazon Selling Partner may return an error for invalid SKUs, causing the Domo dataset to stop import with an error. Selecting 'Skip Invalid SKUs' will continue the dataset import if an invalid SKU error is encountered. Selecting 'Warn About Invalid SKUs' will stop dataset import with an error if an invalid SKU error is encountered.", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-11", "text": "QueryA comma-delimited list of words to search the Amazon catalog for.Basic SchedulingChoose a predefined update schedule (every day between 9 AM and 10 AM)Specify a daily window of activity for 15-minute, 30-minute, and hour intervalsWith a Manual schedule, you can opt-in to receive a reminder when the data needs to be updatedAdvanced SchedulingDefine a fine-grained schedule with a specific time or update frequencyLimit the schedule to run on specific days and monthsStart scheduled updates on a future date with Delay StartNote: Schedules are set from the current time in UTCUpdate ModeWhen Domo retrieves your data from the source, you can determine how that new data is imported into the current DataSet. For this connector, we offer to Append and Replace.Append ModeThis method will take your new data and add it to your current DataSet. Using this may create duplicate data entries.Replace ModeThis method will take your current DataSet and replace it with new data. Using this may result in some data loss.\nOther Panes\nA hybrid Selling Partner API application is an application that can make calls to both Selling Partner API and Amazon Marketplace Web Service (Amazon MWS). Use a hybrid application when your solution requires functionality from both services. When a seller authorizes your hybrid Selling Partner API application, they are (1) authorizing your Amazon MWS developer ID to make calls to Amazon MWS on their behalf, and (2) authorizing the application to make calls to Selling Partner API on their behalf.\nFAQ\nWhat API version does Amazon Selling partner use?\nAmazon Selling partner uses version 0 of the API.\nMy Amazon store sells in multiple marketplaces across the world. How do I get all of my orders, inventory and other important Amazon Seller Partner data?", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-12", "text": "You will need to create an account for each marketplace. We recommend naming the accounts with the marketplace location: for example, 'United States' or 'Japan'. Once you have created the needed accounts, create connector datasets using each account to receive the data that matters to you. If you would like to join the data, use Domo's Magic ETL or Dataflow tools to bring the different marketplaces together.\nHow should I configure the Orders details or Order Items report?\nThe connector can be set up with Append mode and Replace mode.\nAppend Mode: Once you select the append mode you can select the desired start date for your historic backfill, and use a relative end date with a value of 'yesterday' or '1'. Set the connector to only run once per day. The connector will get the most current day's data, then start getting backfill data from the most recent data to older, one day at a time. The connector will run for up to 24 hours and then will remember where it left off in the historical backfill. The next day, when the connector runs again, it will get the most current day's data appended to it first, then pick up where it left off with the historical backfill. This pattern will continue until the connector has received all data for the historical backfill. Once all the backfill is completed, it will continue to append the most current days data, one day at a time. If the connector doesn't run or is unsuccessful during a run, it is designed to receive the data it missed in both the current and historical backfill.\nReplace Mode: Once you select the replace mode, it will replace the existing data with new data for the selected date range\nHow frequently should I run the connector schedule?\nThe connector should only run once per day.\nTroubleshooting", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "dff82a63f966-13", "text": "The connector should only run once per day.\nTroubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/amazon_selling_partner/index.html", "title": "Amazon Selling Partner"}, {"objectID": "56fabe39f86b-0", "text": "TitleAmazon Vendor Partner ConnectorArticle BodyIntro\nAmazon Vendor Partner Connector programmatically access the data on listings, orders, payments, reports, and more. Applications using Selling Partner API can increase selling efficiency, reduce labor requirements, and improve response time to customers, helping sellers grow their businesses. Vendor Partner API builds on the functionality of Amazon Vendor Partner Connector but provides features to improve usability and security for developers and the sellers they work with. This guide documents the Amazon Vendor Partner Connector.\nYou connect to your Amazon Vendor Partner account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon Selling Partner connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in a DataSet Using a Data Connector.\nBest Practices\nFor most reports, be conservative on date ranges. Only pull the data you need.Be prepared for some reports to take hours to complete; this is a limitation of the Amazon API.Be advised that years-old data takes much longer to retrieve.\nPrerequisites\nTo connect to your Amazon Vendor Partner account and create a DataSet, you must have the following:\nParameter NameWhere to find itUsernameCredentials used to log into Seller CentralPasswordCredentials used to log into Seller Central\nConnecting to Your Amazon Vendor Partner Account\nThis section enumerates the options in the Credentials and Details panes on the Amazon Selling Partner Connector page. The components of the other panes on this page, Scheduling, and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-1", "text": "Credentials Pane\nThe DomoAmazon Selling PartnerConnector uses OAuth to connect, so there is no need to enter credentials within Domo. Click Connect(or select Add Account if you have existing Amazon Vendor Partner accounts in Domo) to open the Amazon Vendor Partner OAuth screen where you can enter your Amazon Vendor Partner username and password. Once you have entered valid Amazon Vendor Partner credentials, you can use the same account any time you go to create a new Amazon Selling PartnerDataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about the accounts tab, see Managing User Accounts for Connectors.", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-2", "text": "Note: If you are already logged into Amazon Vendor Partner when you connect in Domo, you are authenticated automatically when you click Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of amazon Vendor Partner.", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-3", "text": "Details Pane\nThis pane contains a primaryReportmenu, along with various other fields which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect theAmazon Selling Partnerreport you want to run. The following reports are available:Get Catalog ItemsRetrieves details for an item in the Amazon catalog.Search Catalog ItemsSearch for and return a list of Amazon catalog items and associated information.Create and Download ReportCreate an Amazon Report that is retrieved when Amazon has finished aggregating the informationGet ReportRetrieve the most recent version of the selected reportList ReportsRetrieve a list and status of reports in the queue.Country CodeSelect a Country code only for UK-EU Marketplace for others it will be NAMarket PlaceSelect the desired Marketplace.CSV Report Quote Character\nSelect the quote character to use to delineate columns in the CSV file version of your report. Double Quote is the default character. If your report data contains double quotes, try selecting the Null Characteroption.\nSelect StatusesSelect the status you want to retrieve data for. You can select up to 10 statuses.SKU Selection MethodSelect the method to enter SKUs. Selecting All will return all available SKUs from your Inventory Supply. We will dynamically show SKUs based on the query date selected. To enter a list of SKUs yourself, use the Advanced option.Enter Seller SKUsEnter the list of SKUs you want to retrieve data for, separated by commas.\u00a0Duration\nSelect the duration for the report (a Single Date, or a Date Range).", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-4", "text": "Select the duration for the report (a Single Date, or a Date Range).\nDays BackEnter the number of past days that should appear in the report. Specify either\u00a0today\u00a0(or\u00a00),\u00a0yesterday\u00a0(or\u00a01), or\u00a0today-7\u00a0(or\u00a07) to get data for 7 days into the past. \u00a0Date FromSelect report start date using relative or specific dates. Relative means the number of days from today or a specific date using the date selector.Start DateSpecify whether the first date in your date range is specific or relative. You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is specific or relative. You select the first date in your range in Start Date.\u00a0Select Specific Start DateSelect the first date in your date range.Select Specific End DateSelect the second date in your date range.Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Specify either today(or0),yesterday(or1), or today-7(or7) to get data for 7 days into the past. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data from 10 days ago up until 5 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Specify either today(or0),yesterday(or1), or today-7(or7) to get data for 7 days into the past. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-5", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data from 10 days ago up until 5 days ago.Query Date TypeSelect the date type for the query.\u00a0Created\u00a0returns data based on the creation date.\u00a0Last Updated\u00a0will return data based on the date of the last update.Continue on Invalid SKU Error", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-6", "text": "Amazon Selling Partner may return an error for invalid SKUs, causing the Domo dataset to stop importing with an error. Selecting 'Skip Invalid SKUs' will continue the dataset import if an invalid SKU error is encountered. Selecting 'Warn About Invalid SKUs' will stop dataset import with an error if an invalid SKU error is encountered.", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-7", "text": "QueryA comma-delimited list of words to search the Amazon catalog for.\nOther Panes\nA hybrid Selling Partner API application is an application that can make calls to both Selling Partner API and Amazon Marketplace Web Service (Amazon MWS). Use a hybrid application when your solution requires functionality from both services. When a seller authorizes your hybrid Selling Partner API application, they are (1) authorizing your Amazon MWS developer ID to make calls to Amazon MWS on their behalf, and (2) authorizing the application to make calls to Selling Partner API on their behalf.\nFAQ\n\u00a0\nMy Amazon store sells in multiple marketplaces across the world. How do I get all of my orders, inventory and other important Amazon MWS data?\nYou will need to create an account for each marketplace. We recommend naming the accounts with the marketplace location: for example, 'United States' or 'Italy'. Once you have created the needed accounts, create connector datasets using each account to receive the data that matters to you. If you would like to join the data, use Domo's Magic ETL or Dataflow tools to bring the different marketplaces together.\nHow should I configure the Orders or Order Items report?", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-8", "text": "How should I configure the Orders or Order Items report?\nDue to API limitations, the connector can only be set up with Append mode. Select the desired start date for your historic backfill, and use a relative end date with a value of 'yesterday' or '1'. Set the connector to only run once per day. The connector will get the most current day's data, then start getting backfill data from the most recent data to older, one day at a time. The connector will run up to 24 hours, and then will remember where it left off in the historical backfill. The next day, when the connector runs again, it will get the most current day's data appended to it first, then pick up where it left off with the historical backfill. This pattern will continue until the connector has received all data for the historical backfill. Once all the backfill is completed, it will continue to append the most current days data, one day at a time. If the connector doesn't run or is unsuccessful during a run, it is designed to receive the data it missed in both the current and historical backfill.\nCan Domo schedule Amazon Vendor Reports on my behalf?\nNo. The connector is not able to schedule Amazon MWS Reports on your behalf in the Amazon Vendor API. The connector will deliver all reports in the discovery dropdown for your convenience, but you may receive a \"Report not generated\" error if the report is not generated. The error message will contain the name of the report in the Amazon Vendor account that needs to be generated before the connector can run successfully. Generate the report and add it to the report scheduler in your Amazon Vendor dashboard and try again. The connector report 'Reports - Schedule List' lists all scheduled reports.\nHow frequently should I run the connector schedule?\nThe connector should only run once per day.\nTroubleshooting", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "56fabe39f86b-9", "text": "The connector should only run once per day.\nTroubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/amazon_vendor_partner_connector/index.html", "title": "Amazon Vendor Partner Connector"}, {"objectID": "19d16f635cc9-0", "text": "Title\n\nAmazon Web Services Billing Connector\n\nArticle Body", "source": "../../raw_kb/article/amazon_web_services_billing_connector/index.html", "title": "Amazon Web Services Billing Connector"}, {"objectID": "19d16f635cc9-1", "text": "Intro\nAmazon Web Services (AWS) is a collection of web services that make up a cloud computing platform by\u00a0Amazon.com.\u00a0\u00a0For more information about the AWS API, visit\u00a0their website. (http://docs.aws.amazon.com/AmazonS3/...tication2.html)\nYou connect to your AWS account in the Data Center. This topic discusses the fields and menus that are specific to the AWS connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0 a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your\u00a0AWS\u00a0account and create a DataSet, you must have the following:\nYou must have the access rights to work with the connector\nYour AWS access keyYour AWS secret keyYou must have the required permission to access the connector.\nFor information about obtaining these keys, refer to the following article: http://docs.aws.amazon.com/AWSSimple...edentials.html.\u00a0\nYou must also set up an S3 bucket with a scheduled push of ZIP files to use this connector. For more information, consult Amazon's documentation at http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports.html.\nConnecting to Your AWS Account\nThis section enumerates the options in the Credentials and Details panes in the AWS Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AWS account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAWS Access KeyEnter your AWS access key.AWS Secret KeyEnter your AWS secret key.", "source": "../../raw_kb/article/amazon_web_services_billing_connector/index.html", "title": "Amazon Web Services Billing Connector"}, {"objectID": "19d16f635cc9-2", "text": "FieldDescriptionAWS Access KeyEnter your AWS access key.AWS Secret KeyEnter your AWS secret key.\nSpecial characters, slashes and any other non-url compliant characters are not allowed as valid bucket names. Dash and Undescore are valid characters in the bucket name path. For more information on setting up the AWS Billing S3 bucket, please see the API Documentation.\nFor information about obtaining these credentials, see \"Prerequisites,\" above.\nOnce you have entered valid AWS credentials, you can use the same account any time you go to create a new AWS DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/amazon_web_services_billing_connector/index.html", "title": "Amazon Web Services Billing Connector"}, {"objectID": "19d16f635cc9-3", "text": "MenuDescriptionReportSelect the AWS report you want to run.\u00a0Currently only a single report type is available.Cost and UsageReturns Amazon Web Services Billing cost and usage data.FilterSelect a filter for your report data. The following filters are available:AllReceives data for both invoiced and estimated Billing reports.InvoicedReceives data for invoiced Billing report data.EstimatedReceives estimated Billing report data. When using this filter, you should only go back 2 months and use\u00a0Relative\u00a0mode.Bucket NameEnter the name of the bucket where billing records are redirected. For example: aws-billing-yourcompanyReport PrefixEnter the Cost and Usage report prefix. For more information, consult the AWS\u00a0Billing\u00a0Report NameSelect the AWS Billing Cost and Usage report name.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/amazon_web_services_billing_connector/index.html", "title": "Amazon Web Services Billing Connector"}, {"objectID": "19d16f635cc9-4", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0\u00a0a DataSet Using a Data Connector.\nTrobleshooting\nError message, \"Failed to authenticate. Verify the credentials and try again. Domo is ready, but the credentials you entered are invalid. Verify your account credentials and try again.\" appears while trying to connect to the connector.\nThis issue is related to authentication. You must have the required permission to use this connector.\nFAQ\nMy AWS DataSet is returning an error of \u201cDomo is ready, but Redshift returned: ERROR: current transaction is aborted, commands ignored until end of transaction block.\u201d What do I do?\nThis error is caused by a connection issue between AWS and Domo servers. This is an intermittent error that usually self-resolves. If it does not, you will need to submit a bug report to determine if the connector needs to have autocommit turned on.\nAre there any API limits that I need to be aware of?\nThe connector only supports the Cost and Usage report. Amazon AWS is ending support for all other reports at a future date.\nHow often can the data be updated?", "source": "../../raw_kb/article/amazon_web_services_billing_connector/index.html", "title": "Amazon Web Services Billing Connector"}, {"objectID": "19d16f635cc9-5", "text": "How often can the data be updated?\nDue to large API returns from Amazon AWS, the data should only be updated once per day.", "source": "../../raw_kb/article/amazon_web_services_billing_connector/index.html", "title": "Amazon Web Services Billing Connector"}, {"objectID": "c25af21463ca-0", "text": "TitleAnalyzer and DataSet Views IntegrationArticle BodyIntro\nDataSet Views have been integrated inside of Analyzer, allowing for light-weight ETL data changes while visualizing the data. You can now do the following, all without leaving Analyzer:\nJoin another DataSet.Clean up the data by renaming columns, removing columns, and changing the data type.Create nested Beast Modes.\nFor more information on Analyzer, see DataSet Views.\nUsing DataSet Views inside of Analyzer\nEdit an existing Card or create a new Card.Open the Analyzer.Click the Data tab.Make the desired changes to the DataSet.Click the Visualize tab.Design the Card with the updated DataSet.When finished with the Card, select Save and Close from the drop-down menu.\nViews Custom Grant\nTo help Admins and Major Domo\u2019s with Data Governance in their instance we\u2019ve added a new custom grant to give them more control on who can create DataSet Views in Analyzer.\nWe\u2019ve now added a new grant in the Admin > Roles section, which allows for Admins to enable or disable users' capabilities to create DataSet Views in Analyzer.\nWhen any user in your instance has \"Edit Data\" they have permissions to create a DataSet View in Analyzer. By deselecting this new grant you will remove users' permission from being able to create new DataSet Views in Analyzer. Users will only be able to view the DataSet in Analyzer and not make any changes to it.\nThis new grant is tied with \"Edit Data\", when users have \"Edit Data\" enabled, the new grant to create DataSet views will also be enabled. You must disable it to remove the permission.\nAnalyzer DataSet Tags\nTo help with tracking DataSet Views created in Analyzer, we also auto-tag any DataSet View that is now created in Analyzer. You can easily filter to this tag in the Data Center.\nFAQs\nHow do Custom Roles work with DataSet Views in Analyzer?", "source": "../../raw_kb/article/analyzer_and_dataset_views_integration/index.html", "title": "Analyzer and DataSet Views Integration"}, {"objectID": "c25af21463ca-1", "text": "FAQs\nHow do Custom Roles work with DataSet Views in Analyzer?\nUser grants apply to DataSet Views the same as other Domo features. A user must have \"Can Edit\" DataSet to be able to create or make edits to a DataSet View in Analyzer. If they do not have the \"Edit Data\" grant then the user will have view only permission to the DataSet Data tab in Analyzer. They will not be able to make any edits or changes to the DataSet, only view it.\nHow does PDP work with DataSet Views in Analyzer?\nPDP works the same way it does on other DataSets. When row level security is applied users will only be able to see the rows they have access to in the Data tab in Analyzer. When they create a DataSet View from a DataSet that has PDP applied to it then the PDP automatically gets applied to the new DataSet View.\nAre creation and edits to DataSet Views in Analyzer tracked in the Audit Log?\nYes, when a new DataSet View is created in Analyzer, an Audit Log event is added to the Activity Log. The log indicates the creation of a new DataSet and who created it. When edits are made to the DataSet View an additional audit event is added to the audit log.", "source": "../../raw_kb/article/analyzer_and_dataset_views_integration/index.html", "title": "Analyzer and DataSet Views Integration"}, {"objectID": "c53433c9ca38-0", "text": "TitleAnalyzer Card BasicsArticle BodyUse this one-page PDF to learn about the different elements of a Visualization card in Domo. To download this PDF to your computer, click here.\n\nFor more information about the layout of cards, see\u00a0Understanding Cards.", "source": "../../raw_kb/article/analyzer_card_basics/index.html", "title": "Analyzer Card Basics"}, {"objectID": "bed8a1d6347c-0", "text": "Title\n\nAnalyzer Layout\n\nArticle Body\n\nIntro\nIn the Analyzer view for a Visualization card (sometimes called the \"Card Builder\" or \"Edit\" view), you customize the appearance of your charts and apply DataSet columns to a chart. You can also apply and edit filters, chart properties, sorts, chart name and description, as well as view and manipulate the data table for the chart.\nThe Analyzer\u00a0appears as part of the workflow for creating a Visualization card. For more information, see\u00a0Visualization Card Building Part 1: Powering Your Card.\u00a0\nAnalyzer is only accessible by users with \"Admin,\" \"Privileged,\" or \"Editor\" default security roles OR a custom security role with the \"Edit Cards\" privilege enabled. For more information about default security roles, see\u00a0Managing Roles.\nFor a high-level overview of the Analyzer, see\u00a0Analyzer Overview.\nVideo - Interface Overview - Analyzer\n\n\u00a0\nAnalyzer Interface\nThe following screenshot shows the Analyzer\u00a0as it might appear for a typical Stacked Bar chart. For more information, see Visualization Card Building Part 2: The Analyzer.", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "bed8a1d6347c-1", "text": "You can learn more about these components in the following table:\nNameDescriptionData paneProvides access to data configuration options for the chart, including the following:DataSet picker. Lets you change the DataSet powering this chart.Column filter. Lets you quickly locate specific columns (dimensions and measures) in the DataSet to use in this chart. If you have created calculated columns for this chart using Beast Mode, you can use the filter to find those columns also.Available columns. Shows the columns from this DataSet that you can apply to your chart. These are divided into Measures (columns that contain items with a \"value\" data type) and Dimensions (columns that contain items with \"string,\" \"date,\" and \"date-time\" data types). If you have created calculated columns for this chart using Beast Mode, those columns appear here. You apply columns to your chart by dragging and dropping them from this area into the column fields above the chart preview area.\nIf the column names are too long to fit, you can expand the pane by clicking and dragging on the border of the right side of the pane.\nFor more information about data types, see Applying DataSet Columns to Your Chart.Name & Description fieldsLets you change the name/description of the Visualization card that contains this chart (the name of the powering DataSet is used by default). This is the name and description that appears at the top of the Visualization card in the page(s). For more information about naming a Visualization card, see\u00a0Editing the Name or Description of a Visualization Card.\u00a0Filter & Sort\u00a0paneLets you apply filters to filter the data in columns in your chart. For more information, see\u00a0Sorting the Data in Your Chart.ToolbarProvides access to buttons that let you show or hide various tools in the Analyzer interface. You can show or hide the Toolbar itself using the ^\u00a0button near the top center of the Analyzer window.", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "bed8a1d6347c-2", "text": "The buttons in the Toolbar include all of the following:ButtonDescriptionDataShows/hides the Data pane for this chart. For more information about the options available in this pane, see the row for \"Data pane\" at the top of this table.LineageOpens a window showing the lineage of DataSets feeding into the DataSet that powers this chart. For more information, see Viewing the Lineage of a DataSet in Analyzer.Data TableShows/hides the data table for this chart. This table shows all of the data powering this chart. You can apply filters and aggregations to columns in the data table; these filters are also applied to your chart preview. You can expand the data table pane by clicking and dragging the border at the top of the pane.Filter & SortShows/hides the Filters & Sorting panes for this chart.PropertiesShows/hides Chart Properties available for this chart.Quick FiltersEnables you to add Quick Filters to your chart. When you enable a Quick Filter, a pane appears for that filter in this card. Users viewing the card Details view can then quickly enable or", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "bed8a1d6347c-3", "text": "the card Details view can then quickly enable or disable filters for the card just by toggling the desired filters in the pane. For more information, see Adding Filters to Your Chart - Applying Quick Filters.Color RulesShows/hides the Color Rules dialog for the chart. In this dialog you can set colors on specific columns on a chart. For more information, see Setting Color Rules for a Chart.Auto PreviewLets you turn off automatic updating of the chart preview when changes are made, which could provide a faster editing experience, especially for very large cards.TooltipsShows/hides the Tooltip Fields for the chart.Chart TypesShows/hides the chart type picker for this chart. You can expand the chart picker by clicking and dragging the border on the left side.AnnotationAllows you to annotate on an item in the x-axis.Beast ModeShows/hides the Beast Mode window for this chart. For more information, see Beast Mode.SegmentsShows/hides the Segments window for this chart. For more information, see Creating Segments in Analyzer.YOYShows/hides period-over-period options for this chart. For more information, see", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "bed8a1d6347c-4", "text": "options for this chart. For more information, see Period-over-Period Charts.Column fieldsProvide the means for applying DataSet columns to your chart. To apply columns, you drag them from the available columns region on the left side of the Analyzer into the appropriate fields in this area. These fields differ between chart types. For example, for a standard vertical Bar chart, X Axis and Y Axis fields would appear. You would drag a value column into the Y Axis field and a category column into the X Axis field. Your chart preview would then update to show how the chart would look with these columns applied. For more information, see\u00a0Applying DataSet Columns to Your Chart.Show/Hide ToolbarAllows you to show or hide the Toolbar menu.Undo/redo buttonsAllow you to undo a previous sequence of actions or redo actions you have undone. All actions in Analyzer are available for Undo and Redo, including applying Filters and Chart Properties, making date changes, changing chart types, etc. Keyboard shortcuts for Undo and Redo on Mac and Windows are also available (Command + Z/Command + Shift", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "bed8a1d6347c-5", "text": "Windows are also available (Command + Z/Command + Shift +Z , respectively, on Mac, and Ctrl + Z/Ctrl + Y on Windows).\u00a0 \u00a0Summary numberSummarizes the data in the chart. Click the dropdown arrow to open an options menu for configuring the summary number. For more information, see\u00a0Configuring\u00a0Your Chart Summary Number.Invite buttonLets you share this card with specified team members and invite them to join Domo, all in a single action. If you do not have share access for this card, the button only allows you to invite users, not share the card. For more information, see\u00a0Inviting Others to Join Domo.Date filter menuLets you choose a date range and grain for the data in this chart. If you have selected a Period-over-Period chart type, you can find options for configuring your chart here. For more information, see\u00a0Period-over-Period Charts.Save buttonLets you save changes you have made to this Visualization card.", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "bed8a1d6347c-6", "text": "Clicking Save simply saves changes made to the card. If you click the dropdown arrow to the right of the Save button, a few more options are available, as follows:Save As. Lets you save your changes as a new card. When you choose this option you are prompted to enter a title and description as well as select the page where the new card should reside.Save And Comment. Lets you enter a comment about your changes when you save. Your comment is viewable with the revision in the History view of the Details view for this Visualization card.Save and Close. Saves your changes and closes the Analyzer, returning you to the Details view for the card.If you choose\u00a0Save As\u00a0or\u00a0Save and Comment\u00a0for a card that does not have an alert set on the summary number, you are prompted to set an alert on it now. For more information about setting alerts, see\u00a0Creating a Custom Alert for a Visualization Card.Beast Mode buttonLets you transform the data in your chart by creating new columns based on existing columns. For more information,", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "bed8a1d6347c-7", "text": "new columns based on existing columns. For more information, see\u00a0Beast Mode.Segment buttonLets you create a dynamic segment to compare your segment against another group. For more information, see Creating Segments in Analyzer.Data tableShows you the table of all of the data being used in this chart. This table provides various options for manipulating the data; for example, you can filter rows, apply filters, change aggregations, etc. Any changes you make to the data in the data table are reflected in the chart preview. For more information, see Understanding the Data Table in Analyzer.Chart PropertiesDisplays Chart Properties available for this chart. You can edit various properties of the chart, such as number and date formatting, the number of value scale divisions, scale labels, and so on. For more information about setting chart properties, see\u00a0Chart Properties.Chart previewShows a preview of your chart. Appears only when all required columns have been applied. You can expand the chart preview pane by clicking and dragging any of the borders on the left, right, or bottom. For more information,", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "bed8a1d6347c-8", "text": "the left, right, or bottom. For more information, see\u00a0Applying DataSet Columns to Your Chart.Data table optionsProvides additional options for working with the data table, including showing and hiding columns, showing totals, showing raw data or data as it appears in the chart, and opening a full-screen mode. For more information, see Understanding the Data Table in Analyzer.Chart pickerLets you specify or change the chart type. Chart types are divided into categories. For information about available chart types, see Chart Types.", "source": "../../raw_kb/article/analyzer_layout/index.html", "title": "Analyzer Layout"}, {"objectID": "2a9735df8c1c-0", "text": "Title\n\nAnalyzer Overview\n\nArticle Body\n\nIntro\nIn Analyzer, you build the chart or table for\u00a0your Visualization card. You can also specify visual characteristics for the chart such as the chart type, filters, sorting, summary number, date grain, and advanced chart properties. You can also use Beast Mode to create calculations for use in transforming columns.\u00a0\nFor information about the user interface components of the Analyzer, see\u00a0Analyzer Layout.\nBuilding your chart\nTo build and specify visual characteristics for a chart or table,\nIn Analyzer, specify the characteristics you want.Use the following table to locate the desired topic.TopicLinkNameEditing the Name or Description of a Visualization CardDescriptionEditing the Name or Description of a Visualization CardDataSet columnsApplying DataSet Columns to Your ChartTable labelsEditing the Column Header Labels for Your Chart Table ViewValue formattingFormatting Values in Your ChartGoal markersAdding a Goal Line to Your ChartChart typeAvailable Chart TypesFiltersAdding Filters to Your ChartSort criteriaSorting the Data in Your ChartSummary numberConfiguring Your Chart Summary NumberAdvanced chart propertiesChart PropertiesTransforms and calculationsBeast ModeConditional colors on DataSet columnsSetting Color Rules for a Chart Chart data tableUnderstanding the Data Table in Analyzer DataSet lineageViewing the Lineage of a DataSet in AnalyzerClick\u00a0Save Card.(Optional) In the dialog, enter a comment about your changes.The comment appears with the revision in the History tab in the Details view for a card. For more information about the History\u00a0tab, see History View in Card Details View Layout.Click Save & Finish.\nVideo - Interface Overview - Analyzer\n\n\u00a0\nVideo - Creating Cards in Analyzer", "source": "../../raw_kb/article/analyzer_overview/index.html", "title": "Analyzer Overview"}, {"objectID": "5750e3f931e7-0", "text": "TitleAnaplan Advanced V2 ConnectorArticle BodyIntro\nAnaplan is a cloud-based business modeling and planning platform for sales, operations and finance. Use Domo's\u00a0Anaplan\u00a0Advanced V2 connector to return data you have created as an export in the Anaplan user interface.\u00a0For more information about the Anaplan API, visit\u00a0their website. (https://docs.anaplan.apiary.io/)\nYou connect to your Anaplan account in the Data Center. This topic discusses the fields and menus that are specific to the Anaplan\u00a0Advanced V2 connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Anaplan account and create a DataSet, you must have the following: \nAn Anaplan\u00a0private key. A CA-issued X509 certificate.\u00a0\u00a0\nFor an in-depth guide to obtaining a certificate and exporting your private key, see\u00a0https://community.anaplan.com/t5/Content-Sandbox/A-Guide-to-CA-Certificates-in-Anaplan-Integrations/ta-p/39305.\u00a0\nConnecting to your Anaplan account\nThis section enumerates the options in the\u00a0 Credentials \u00a0and\u00a0 Details \u00a0panes in the Anaplan Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials pane\nThis pane contains fields for entering credentials to connect to your Anaplan account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionCA CertificateCopy and paste the text from your CA-issued X509 certificate.\u00a0Private KeyEnter your Anaplan private key.", "source": "../../raw_kb/article/anaplan_advanced_v2_connector/index.html", "title": "Anaplan Advanced V2 Connector"}, {"objectID": "5750e3f931e7-1", "text": "Once you have entered a valid certificate and key, you can use the same account any time you go to create a new Anaplan DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails pane\nIn this pane you select the Anaplan workspace and model with the file/process you want to retrieve, as well as the name of the file or process itself.\u00a0\nMenuDescriptionUsersSelect the user account\u00a0you want to retrieve data from.\u00a0WorkspacesSelect the Anaplan workspace that contains the\u00a0file you want to retrieve. \n\u00a0ModelsSelect the Anaplan model that contains the\u00a0file you want to retrieve.Report TypeSelect the type of report you want to import, either\u00a0Exports\u00a0or\u00a0Processes.\u00a0FilesSelect the file you want to pull into Domo.ProcessSelect the process you want to pull into Domo.", "source": "../../raw_kb/article/anaplan_advanced_v2_connector/index.html", "title": "Anaplan Advanced V2 Connector"}, {"objectID": "5750e3f931e7-2", "text": "Other panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/anaplan_advanced_v2_connector/index.html", "title": "Anaplan Advanced V2 Connector"}, {"objectID": "70f46a78bc27-0", "text": "TitleAnaplan Audit Service Advanced ConnectorArticle BodyIntro\nThe Anaplan Audit API is intended as a delivery mechanism for audit logs and is designed for audit information to be pulled frequently so that you can leverage your own technology for filtering, analysis, and storage. \u00a0For more information about the Anaplan API, visit\u00a0their website. (https://docs.anaplan.apiary.io/)\nYou connect to your Anaplan account in the Data Center. This topic discusses the fields and menus that are specific to the Anaplan\u00a0Audit Service connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Anaplan account and create a DataSet you must have the following.\nAn Anaplan\u00a0private key.A CA-issued X509 certificate.\u00a0\u00a0\nTo procure the CA certificate please click the link here\nConnecting to your Anaplan account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the Anaplan\u00a0Audit Service Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials pane\nThis pane contains fields for entering credentials to connect to your Anaplan account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionCA CertificateCopy and paste the text from your CA-issued X509 certificate.\u00a0Private KeyEnter your Anaplan private key.", "source": "../../raw_kb/article/anaplan_audit_service_advanced_connector/index.html", "title": "Anaplan Audit Service Advanced Connector"}, {"objectID": "70f46a78bc27-1", "text": "Once you have entered a valid certificate and key, you can use the same account any time you go to create a new Anaplan DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Anaplan report you want to run.\u00a0The following reports are available:Audit EventsRetrieves audit events for the tenant with which your user is associated.UsersRetrieves a list of users based on your Anaplan authentication token.\u00a0Audit Events Response FormatSelect the Audit Events output response format, either CEF or JSON.TypeSelect the type of events to return, either BYOK (Bring Your Own Key) or User Activity.Start DateSelect the desired start date for your report.End DateSelect the desired end date for your report.Interval in HoursEnter the hourly interval of events to return. For example, if you entered\u00a02, the report would return the previous 2 hours of events.\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/anaplan_audit_service_advanced_connector/index.html", "title": "Anaplan Audit Service Advanced Connector"}, {"objectID": "b5fbffc3170b-0", "text": "TitleAnaplan Audit Service ConnectorArticle BodyIntro\nThe Anaplan Audit API is intended as a delivery mechanism for audit logs and is designed for audit information to be pulled frequently so that you can leverage your own technology for filtering, analysis, and storage. \u00a0For more information about the Anaplan API, visit\u00a0their website. (https://docs.anaplan.apiary.io/)\nYou connect to your Anaplan account in the Data Center. This topic discusses the fields and menus that are specific to the Anaplan\u00a0Audit Service connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Anaplan account and create a DataSet, you must have your Anaplan username and password.\u00a0\nConnecting to your Anaplan account\nThis section enumerates the options in the Credentials and Details panes in the Anaplan\u00a0Audit Service Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials pane\nThis pane contains fields for entering credentials to connect to your Anaplan account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter your Anaplan username.PasswordEnter your Anaplan password.\u00a0\nOnce you have entered valid Anaplan credentials, you can use the same account any time you go to create a new Anaplan\u00a0Audit Service DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/anaplan_audit_service_connector/index.html", "title": "Anaplan Audit Service Connector"}, {"objectID": "b5fbffc3170b-1", "text": "MenuDescriptionReportSelect the Anaplan report you want to run.\u00a0The following reports are available:Audit EventsRetrieves audit events for the tenant with which your user is associated.UsersRetrieves a list of users based on your Anaplan authentication token.\u00a0Audit Events Response FormatSelect the Audit Events output response format, either CEF or JSON.TypeSelect the type of events to return, either BYOK (Bring Your Own Key) or User Activity.Start DateSelect the desired start date for your report.End DateSelect the desired end date for your report.Interval in HoursEnter the hourly interval of events to return. For example, if you entered 2, the report would return the previous 2 hours of events.\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/anaplan_audit_service_connector/index.html", "title": "Anaplan Audit Service Connector"}, {"objectID": "f787a4b28268-0", "text": "Title\n\nAnaplan Connector\n\nArticle Body", "source": "../../raw_kb/article/anaplan_connector/index.html", "title": "Anaplan Connector"}, {"objectID": "f787a4b28268-1", "text": "Intro\nAnaplan is a cloud-based business modeling and planning platform for sales, operations and finance. Use Domo's\u00a0Anaplan connector to return data you have created as an export in the Anaplan user interface.\u00a0For more information about the Anaplan API, visit\u00a0their website. (https://docs.anaplan.apiary.io/)\nYou connect to your Anaplan account in the Data Center. This topic discusses the fields and menus that are specific to the Anaplan connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Anaplan account and create a DataSet, you must have the username and password for your Anaplan account. You must also have an export file set up in Anaplan, and you must know the workspace and model in which the file is located. For information about creating an export file, see Setting Up an Export in Anaplan. \nConnecting to Your Anaplan Account\nThis section enumerates the options in the\u00a0 Credentials \u00a0and\u00a0 Details \u00a0panes in the Anaplan Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your A\ufeffnaplan account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter the username you use to log into your Anaplan account.PasswordEnter the password you use to log into your Anaplan account.", "source": "../../raw_kb/article/anaplan_connector/index.html", "title": "Anaplan Connector"}, {"objectID": "f787a4b28268-2", "text": "Once you have entered valid Anaplan credentials, you can use the same account any time you go to create a new Anaplan DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nIn this pane you select the Anaplan workspace and model with the export file you want to retrieve, as well as the name of the file itself. For information about setting up an export in Anaplan, see the next section. \nMenuDescriptionWorkspacesSelect the Anaplan workspace that contains your export file. \n\u00a0ModelsSelect the Anaplan model that contains your export file.FilesSelect the export file you want to pull into Domo.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0\u00a0a DataSet Using a Data Connector.\nSetting Up an Export in Anaplan\nFollow the instructions in this section to create a file for export in Anaplan. Files set up for export can then be pulled into Domo as DataSets.", "source": "../../raw_kb/article/anaplan_connector/index.html", "title": "Anaplan Connector"}, {"objectID": "f787a4b28268-3", "text": "Note: You must have full access in Anaplan to do this. Otherwise the export cannot be saved as an \"export definition\" so it will not appear in the File menu in the Details pane for the Anaplan connector.\n\n\n\nTo set up a file for export in Anaplan,\nIn Anaplan, open the module or list you want to export from by doing the following:Click the Gear icon.Click Modules.Click the module where your file is located.Click Open at the top of the screen.Click Data at the top of the screen.Click Export.Choose the desired export format.\tIt is recommended you select CSV as the file type and Tabular Single Column as the layout, and that you check the box for Export all line items while omitting empty rows.Check the box for Save Export Definition.Name your export.\tThis is the name that appears in the File menu in the Details pane for the Anaplan connector.Click Run Export.\nFAQs\nWhat version of Anaplan\u00a0does the connector use?\nThis connector uses major version 1 and minor version 3 of Anaplan. (https://api.anaplan.com/{major}/{minor})Major Version = 1Minor Version = 3\u00a0\nWhich endpoints does each discovery call using this connector?\nDiscovery NameEndpoint URLWorkspaces/workspacesModels/workspaces/{workspaceID}/modelsFiles/workspaces/{workspaceID}/models/{modelID}/files\nWhich endpoints are called to process the data?\n/workspaces/{workspaceID}/models/{modelID}/files/{fileID}/chunks/workspaces/{workspaceID}/models/{modelID}/exports/{fileID}/tasks/workspaces/{workspaceID}/models/{modelID}/exports/{fileID}/tasks/{taskID}\nCan I use the same account to create multiple DataSets?\nYou sure can.\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits I should be aware of?\nNo.", "source": "../../raw_kb/article/anaplan_connector/index.html", "title": "Anaplan Connector"}, {"objectID": "52cf9fe2171e-0", "text": "TitleAnonomatic Augmented Writeback ConnectorArticle BodyIntro\nPII Vault privacy compliance is accomplished through Poly-Anonymization. Poly-Anonymization involves taking any personal identifying pieces of information and swapping it out for our Poly-Anonymous Identifier (Poly-Id). The Anonomatic Augmented Writeback Connector takes a Domo dataset and uses the Anonomatic service to anonymize the data, and then writes it back to another Domo dataset. To power up this connector, you need your PII Vault Container host, port and service account ID, and the API key for your PII Vault service.\nYou configure your Domo-PIIVault Container connection in the Data Center. This topic discusses the fields and menus that are specific to the Anonomatic Augmented Writeback Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\n\n\n\n\n\nNote: The owner of a writeback dataset must also be an owner or co-owner of the input dataset.", "source": "../../raw_kb/article/anonomatic_augmented_writeback_connector/index.html", "title": "Anonomatic Augmented Writeback Connector"}, {"objectID": "52cf9fe2171e-1", "text": "Prerequisites\nTo configure this Connector, you must have the following:\nThe host of your PIIVault ContainerThe port number of your PIIVault ContainerYour PIIVault Service Account IDThe API Key for your PIIVault service\nConfiguring the Connection\nThis section enumerates the options in the Credentials and Details panes in the Anonomatic Augmented Writeback\u00a0Connector page. The components of the other panes in this page, Scheduling and Name & Describe Your DataSet, are universal across most Connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your PIIVault container. The following table describes what is needed for each field: \u00a0\nFieldDescriptionHostEnter the host of your PIIVault Container.PortEnter the port number of your PIIVault Container.Account IDEnter your PIIVault Service Account ID.API KeyEnter API Key for your PIIVault service.\nOnce you have entered valid credentials, you can use the same account any time you go to set up a new Anonomatic Augmented Writeback DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a number of fields for specifying your data and indicating where it's going.\nMenuDescriptionInput DataSet IDEnter the ID of your Domo dataset containing PII data to anonymize with PIIVault Anonomatic service.\nYou can find the ID by opening the details view for the DataSet in the Data Center and looking at the portion of the URL following datasources/.", "source": "../../raw_kb/article/anonomatic_augmented_writeback_connector/index.html", "title": "Anonomatic Augmented Writeback Connector"}, {"objectID": "52cf9fe2171e-2", "text": "For example, in the URL\u00a0https://mycompany.domo.com/datasources/845305d8-da3d-4107-a9d6-13ef3f86d4a4/details/overview, the DataSet ID is\u00a0845305d8-da3d-4107-a9d6-13ef3f86d4a4.\u00a0Map dataset PII fields and Anonomatic service fieldsSelect the dataset column for the required Anonomatic service field. Choose a field that should be unique.Match the desired Anonomatic service fields and dataset's PII data columns for optional fields.Use 'Redact Value' checkbox if you want to redact any PII field data from Domo.Identifier FieldNameEnter the input dataset id to map the fields here.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this connector?\nYou need the host name and port number of your PIIVault Container, and the account ID and API key associated with your PIIVault Service.\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits that I need to be aware of?\nNo\nHow do I find the Input Dataset ID?\nYour Domo input dataset id is in the URL of the dataset you are exporting data from. For example: https://customer.domo.com/datasources/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/details/settings", "source": "../../raw_kb/article/anonomatic_augmented_writeback_connector/index.html", "title": "Anonomatic Augmented Writeback Connector"}, {"objectID": "00ec9b3f21b4-0", "text": "TitleApacheImpala ConnectorArticle BodyIntro\nApache Impala is an open source massively parallel processing (MPP) SQL query engine for data stored in a computer cluster running Apache Hadoop. It consists of different processes that run on specific hosts within your CDH cluster. The Domo Apache Impala SSH connector brings your data from the Apache server securely through an SSH tunnel into Domo.\nThe ApacheImpala Connector is a \"Cloud App\" Connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the Connector page for this and other Cloud App Connectors by clicking\u00a0Cloud App\u00a0in the toolbar at the top of the window.\nYou connect to your ApacheImpala account in the Data Center. This topic discusses the fields and menus that are specific to the ApacheImpala Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your\u00a0Apache Impala database\u00a0and create a DataSet, you must have the following:\nThe username and password you use to log into your Apache Impala databaseThe host name\u00a0or IP address\u00a0for the database server (e.g.\u00a0db.company.com).The port number for the databaseThe database nameConnecting String Parameter\nBefore you can connect to an Apache Impala database, you must also whitelist a number of IP addresses on your database server on the port you want to connect to. For the full list of IP addresses, see\u00a0Whitelisting IP Addresses for Connectors.\nConnecting to Your ApacheImpala Connector Account", "source": "../../raw_kb/article/apacheimpala_connector/index.html", "title": "ApacheImpala Connector"}, {"objectID": "00ec9b3f21b4-1", "text": "Connecting to Your ApacheImpala Connector Account\nThis section enumerates the options in the Credentials and Details panes in the [insert Connector name here] Connector page. The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your (third-party tool) account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionHostEnter the hostname or IP address of your database server. Example: db.company.comDatabase PortEnter your Apache Impala port number.Database NameEnter your Apache Impala database/schema name.UsernameEnter your Apache Impala username.PasswordEnter your Apache Impala password.Database Connection String Parameter(s)Enter the parameter(s) you want to include in the database connection string. Multiple parameters are separated by a semicolon. (Example: AuthMech=3;SSL=1;AllowSelfSignedCerts=1)\nOnce you have entered valid Apache Impala credentials, you can use the same account any time you go to create a new Apache Impala DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.", "source": "../../raw_kb/article/apacheimpala_connector/index.html", "title": "ApacheImpala Connector"}, {"objectID": "00ec9b3f21b4-2", "text": "Note:\u00a0If you are already logged into ApacheImpala when you connect in Domo, you are authenticated automatically when you click\u00a0Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of ApacheImpala account.", "source": "../../raw_kb/article/apacheimpala_connector/index.html", "title": "ApacheImpala Connector"}, {"objectID": "00ec9b3f21b4-3", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionQuery TypeSelect a query type.QueryA regular SQL query without parameter.Query ParameterAn SQL query with parameter.QueryEnter the SQL query to execute. The query will execute on the Apache Impala server and fetch the data from it.Query ParameterEnter the query parameter value. It is the initial value for query parameter. The last run date is optional. The default value for the last date is '02/01/1700' if not provided.\nExample:\u00a0!{lastvalue:_id}!=1,!{lastrundate:start_date}!=02/01/1944Database TableSelect the database table.Table ColumnsSelect the table columns.Query HelperThis query is automatically generated when you select a table and columns in the Database Table and Table Columns fields, respectively. Copy and paste this query into the Query field if you need help building a query.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\n\u00a0\nWhat kind of credentials do I need to power up the Apache Impala connector?\nYou need the hostname, dat\nHow frequently will my data update?\nAs often as needed.\nAre there any API limits that I need to be aware of?\nLimits depend on your server configuration.\nCan I use the same Apache Impala account to create multiple datasets?\nYes\nWhat do I need to be aware of while writing a query?\nMake sure that all the words, table names and field names are correctly spelled.\nTroubleshooting", "source": "../../raw_kb/article/apacheimpala_connector/index.html", "title": "ApacheImpala Connector"}, {"objectID": "00ec9b3f21b4-4", "text": "Make sure that all the words, table names and field names are correctly spelled.\nTroubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/apacheimpala_connector/index.html", "title": "ApacheImpala Connector"}, {"objectID": "15e06dbcc1bc-0", "text": "TitleApache Impala SSH ConnectorArticle BodyIntro\nApache Impala is an open source massively parallel processing (MPP) SQL query engine for data stored in a computer cluster running Apache Hadoop. It consists of different processes that run on specific hosts within your CDH cluster. The Domo Apache Impala SSH connector brings your data from Apache server securely through an SSH tunnel into Domo.\nThe Apache Impala SSH Connector is a \"Database\" connector, meaning it retrieves data from a database using a query. In the Data Center, you can access the connector page for this and other Database connectors by clicking Database\u00a0in the toolbar at the top of the window.\nThis topic discusses the fields and menus that are specific to the Apache Impala SSH connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Apache Impala database and create a DataSet, you must have the following:\nThe username and password you use to log into\u00a0SSH HostThe SSH host you wish to tunnel throughThe port number of your SSH hostThe SSH private keyThe username and password you use to log into your Apache Impala databaseThe host name or IP address for the database server (e.g.\u00a0db.company.com).The port number for the databaseThe database name\nBefore you can connect to an Apache Impala database, you must also whitelist a number of IP addresses on your database server on the port you want to connect to. For the full list of IP addresses, see Whitelisting IP Addresses for Connectors.\n\n\n\u00a0\n\nNote: Domo does not support the SSH keys generated using ssh-keygen. The SSH keys need to be the DES or RSA keys (in PEM format) generated by OpenSSL.", "source": "../../raw_kb/article/apache_impala_ssh_connector/index.html", "title": "Apache Impala SSH Connector"}, {"objectID": "15e06dbcc1bc-1", "text": "Connecting to Your Apache Impala database\nThis section enumerates the options in the Credentials and Details panes in the Apache Impala SSH Connector page. The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your (third-party tool) account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionSSH Server Host nameEnter the SSH host name you wish to tunnel through.SSH PortEnter the port number of your SSH host.SSH UsernameEnter the username you use to log into SSH Host.SSH PasswordEnter the password you use to log into SSH Host.SSH Private KeyEnter the SSH private key.\u00a0HostEnter the hostname or IP address of your database server. Example: db.company.comDatabase PortEnter your Apache Impala port number.Database NameEnter your Apache Impala database/schema name.UsernameEnter your Apache Impala username.PasswordEnter your Apache Impala password.Database Connection String Parameter(s)Enter the parameter(s) you want to include in the database connection string. Multiple parameters are separated by a semicolon. (Example: AuthMech=3;SSL=1;AllowSelfSignedCerts=1)\nOnce you have entered valid Apache Impala credentials, you can use the same account any time you go to create a new Apache Impala SSH DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/apache_impala_ssh_connector/index.html", "title": "Apache Impala SSH Connector"}, {"objectID": "15e06dbcc1bc-2", "text": "MenuDescriptionQuery TypeSelect a query type.QueryA regular SQL query without parameter.Query ParameterAn SQL query with parameter.QueryEnter the SQL query to execute. The query will execute on the Apache Impala server and fetch the data from it.Query ParameterEnter the query parameter value. It is the initial value for query parameter. The last run date is optional. The default value for the last date is '02/01/1700' if not provided. \nExample:\u00a0!{lastvalue:_id}!=1,!{lastrundate:start_date}!=02/01/1944Database TableSelect the database table.Table ColumnsSelect the table columns.Query HelperThis query is automatically generated when you select a table and columns in the Database Table and Table Columns fields, respectively. Copy and paste this query into the Query field if you need help building a query.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this connector?\nYou need your Apache Impala SSH credentials (server hostname, port number, private key, username, and password) as well as your database credentials (hostname, port number, database name, username, and password). You may also provide the parameter(s) you want to include in the database connection string. Multiple parameters are separated by a semicolon. Example: AuthMech=3;SSL=1;AllowSelfSignedCerts=1.\nHow frequently will my data update?\nAs often as needed.\nAre there any API limits that I need to be aware of?\nLimits depend on your server configuration.\nCan I use the same Apache Impala account to create multiple datasets?\nYes\nWhat do I need to be aware of while writing a query?", "source": "../../raw_kb/article/apache_impala_ssh_connector/index.html", "title": "Apache Impala SSH Connector"}, {"objectID": "15e06dbcc1bc-3", "text": "Yes\nWhat do I need to be aware of while writing a query?\nMake sure that all the words, table names, and field names are correctly spelled. Refer to the Query Helper field if you need help building a query.\nWhy can't I connect to my Apache Impala database? Do I need to whitelist any IP addresses?\nBefore you can connect to an Apache Impala database in Domo, you must also whitelist a number of IP addresses on your database server on the port you want to connect to. For the full list of IP addresses, see Whitelisting IP Addresses.", "source": "../../raw_kb/article/apache_impala_ssh_connector/index.html", "title": "Apache Impala SSH Connector"}, {"objectID": "723e3d4d6159-0", "text": "Title\n\nApache Kafka Connector\n\nArticle Body", "source": "../../raw_kb/article/apache_kafka_connector/index.html", "title": "Apache Kafka Connector"}, {"objectID": "723e3d4d6159-1", "text": "Intro\nApache Kafka is a streams messaging platform built to handle high volumes of data very quickly. Domo's Kafka Connector lets you pull information on messaging topics, topic data, and partitions so that you can cut through the noise and focus on the communication that is most vital. To learn more about the Kafka API, visit their page (https://kafka.apache.org/documentation/).  \nYou connect to your Kafka account in the Data Center. This topic discusses the fields and menus that are specific to the Kafka connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Kafka account and create a DataSet, you must have the server name or IP address of your Kafka server. You must also whitelist a number of IP addresses on your database server on the\u00a0port\u00a0you want to connect to. For the full list of IP addresses, see\u00a0Whitelisting IP Addresses for Connectors.\nConnecting to Your Kafka Account\nThis section enumerates the options in the Credentials and Details panes in the Kafka Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Kafka account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionKafka ServerEnter the server name or IP address of your Kafka server.\nOnce you have entered valid Kafka credentials, you can use the same account any time you go to create a new Kafka DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane", "source": "../../raw_kb/article/apache_kafka_connector/index.html", "title": "Apache Kafka Connector"}, {"objectID": "723e3d4d6159-2", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Kafka report you want to run.\u00a0The following reports are available:PartitionsReturns a list of partitions for a selected topic.TopicsReturns a list of available topics.Topics DataReturns all data for the selected topic.\u00a0Topic NameSelect the topic you want to retrieve data for.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/apache_kafka_connector/index.html", "title": "Apache Kafka Connector"}, {"objectID": "5ead2a03ac73-0", "text": "Title\n\nApache Kafka REST Connector\n\nArticle Body", "source": "../../raw_kb/article/apache_kafka_rest_connector/index.html", "title": "Apache Kafka REST Connector"}, {"objectID": "5ead2a03ac73-1", "text": "Intro\nApache Kafka is a streams messaging platform built to handle high volumes of data very quickly. Domo's\u00a0Apache Kafka REST Proxy Connector lets you pull information on brokers, partitions, topics, topic metadata, and topic records so that you can cut through the noise and focus on the most vital communication. To learn more about the Kafka REST API, visit their page (https://docs.confluent.io/current/ka...ocs/intro.html).\nYou connect to your Kafka server and set up an acccount\u00a0in the Data Center. This topic discusses the fields and menus that are specific to the Kafka REST connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Kafka REST account and create a DataSet, you must know the name of your Kafka server.\u00a0You must also whitelist a number of IP addresses on your database server on the\u00a0port\u00a0you want to connect to. For the full list of IP addresses, see\u00a0Whitelisting IP Addresses for Connectors.\nConnecting to Your Kafka REST Server\nThis section enumerates the options in the Credentials and Details panes in the Kafka REST Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Kafka REST account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionKafka ServerEnter the list of host/port pairs for the Kafka server.\nOnce you have entered valid credentials, you can use the same account any time you go to create a new Kafka REST DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.", "source": "../../raw_kb/article/apache_kafka_rest_connector/index.html", "title": "Apache Kafka REST Connector"}, {"objectID": "5ead2a03ac73-2", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Kafka REST report you want to run.\u00a0The following reports are available:BrokersReturns a list of brokers.PartitionsReturns a list of partitions for the selected topic.TopicsReturns a list of Kafka topics.Topics MetadataReturns metadata for the selected topic.Topics RecordsReturns data for the selected topics or partitions using one of the subscribe/assign APIs.Topic NameSelect the topic you want to retrieve data for.Group NameEnter the name of the consumer group you want to retrieve data for.Name (Optional)Enter the name of the consumer instance you want to retrieve data for.Auto Offset Reset (Optional)Enter the auto.offset.reset value for the consumer.Auto Commit Enable (Optional)Enter the auto.commit.enable value for the consumer.Timeout (Optional)Enter the number of milliseconds for the underlying client library poll (timeout) request to fetch records.\u00a0Max Bytes (Optional)Enter the maximum number of bytes of unencoded keys and values that should be included in the response.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/apache_kafka_rest_connector/index.html", "title": "Apache Kafka REST Connector"}, {"objectID": "6b6fa0169516-0", "text": "TitleApple News Publisher ConnectorArticle BodyIntro\nApple News gives users the ability to to launch, monitor, and report on direct-sold and house advertising campaigns using Workbench for News Publishers.\u00a0Use Domo's Apple News\u00a0Publisher connector to retrieve pull data from your Apple News Publisher system in Domo. To learn more about the Apple News API, visit their page (https://developer.apple.com/document...apple_news_api).\nYou connect to your Apple News\u00a0account in the Data Center. This topic discusses the fields and menus that are specific to the Apple News\u00a0Publisher connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Apple News account and create a DataSet, you must have the following:\nAn Apple News\u00a0client-side SSL private keyAn Apple News client-side SSL certificate signed by Apple Ad PlatformsAn Apple News access token\nTo obtain these credentials, follow the instructions under \"Access, Authentication, and Authorization\" here:\u00a0https://developer.apple.com/library/.../Overview.html\nConnecting to Your Apple News Account\nThis section enumerates the options in the Credentials and Details panes in the Apple News Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Apple News account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionPrivate KeyEnter your client-side SSL private key.CertificateCopy and paste your client-side SSL certificate.Access TokenEnter your Apple News access token.\nFor information about obtaining these credentials, see \"Prerequisites,\" above.", "source": "../../raw_kb/article/apple_news_publisher_connector/index.html", "title": "Apple News Publisher Connector"}, {"objectID": "6b6fa0169516-1", "text": "For information about obtaining these credentials, see \"Prerequisites,\" above.\nOnce you have entered valid Apple News credentials, you can use the same account any time you go to create a new Apple News Publisher DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/apple_news_publisher_connector/index.html", "title": "Apple News Publisher Connector"}, {"objectID": "6b6fa0169516-2", "text": "MenuDescriptionReportSelect the Apple News report you want to run.\u00a0The following reports are available:Ad MetricsReturns metrics of the performance of a specified\u00a0ad.Campaign DetailsReturns campaign details for a period for one or more campaigns. Output includes details and metrics for the period for campaigns, corresponding lines, and ads.Campaign MetricsReturns metrics of the performance of a campaign.\u00a0Campaign SummaryReturns a summary of campaign data.Check InventoryReturns the available inventory for a particular set of targeting criteria without having to create a line.Check Line InventoryRetrieves the inventory landscape. The returned values are estimates of future inventory available to that line.Line DetailsReturns line details for one or more lines. Output includes details and metrics for the specified period for lines and corresponding ads. If no ads are associated with the line, no ad details are returned.Line MetricsReturns metrics for the performance of a line.Campaign IDsEnter the IDs of the campaigns you want to retrieve data for, separated by commas. You can find campaign IDs by running the \"Campaign Summary\" report.Sublist to FlattenSelect the sublist you want", "source": "../../raw_kb/article/apple_news_publisher_connector/index.html", "title": "Apple News Publisher Connector"}, {"objectID": "6b6fa0169516-3", "text": "report.Sublist to FlattenSelect the sublist you want to flatten.TimezoneSelect the time zone you want to retrieve data for.StateSelect the state you want to retrieve data for.Impression GoalEnter the number of impressions you want to retrieve data for.Line TypeSelect the line type you want to retrieve data for.Creative TypeSelect the creative type you want to retrieve data for.Ad PositionSelect the ad position you want to retrieve data for.Country CodeSelect the code for the country you want to retrieve data for.Line IDsSelect the line IDs you want to retrieve data for.Ad IDsSelect the ad IDs you want to retrieve data for.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range", "source": "../../raw_kb/article/apple_news_publisher_connector/index.html", "title": "Apple News Publisher Connector"}, {"objectID": "6b6fa0169516-4", "text": "relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/apple_news_publisher_connector/index.html", "title": "Apple News Publisher Connector"}, {"objectID": "6b6fa0169516-5", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nHow does the connector aggregate report data?\nReturned data will be aggregated depending on the values of the Start Date\u00a0and End Date\u00a0parameters. If you specify\u00a0between 2 and 31 days of data, daily data will be returned. If you specify one day of data, hourly data is returned. If you specify\u00a0more than 31 days but fewer than 28 weeks, weekly data is returned. If you specify more than 28 weeks, monthly data is returned.\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits I should be aware of?\nNo.", "source": "../../raw_kb/article/apple_news_publisher_connector/index.html", "title": "Apple News Publisher Connector"}, {"objectID": "c8a93f77254e-0", "text": "TitleApple Search Ads ConnectorArticle BodyIntro\nApple Search Ads allows larger advertisers or agencies to create and manage a large number of campaigns pragmatically and to run reports for customer insights and trends. To learn more about the Apple Search Ads API, visit their page (https://searchads.apple.com/advanced...gn-management/).\nYou connect to your Apple Search Ads account in the Data Center. This topic discusses the fields and menus that are specific to the Apple Search Ads connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Apple Search Ads account and create a DataSet, you must have the following:\nA client-side SSL private keyA client-side SSL certificate signed by Apple Ad Platforms, which can be\u00a0\u00a0generated and managed in your Search Ads account\u00a0in the web-based UI.\u00a0The password for your Apple Search Ads account\nConnecting to Your Apple Search Ads Account\nThis section enumerates the options in the Credentials and Details panes in the Apple Search Ads Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Apple Search Ads account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionPrivate KeyEnter your client-side SSL private key.CertificatePaste your client-side SSL certificate signed by Apple Ad Platforms.PasswordEnter the password for your Apple Search Ads account.\nOnce you have entered valid Apple Search Ads credentials, you can use the same account any time you go to create a new Apple Search Ads DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane", "source": "../../raw_kb/article/apple_search_ads_connector/index.html", "title": "Apple Search Ads Connector"}, {"objectID": "c8a93f77254e-1", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Apple Search Ads report you want to run.\u00a0The following reports are available:Campaign ReportReturns metadata and performance metrics of all the campaigns for the selected org.AdGroup ReportReturns metadata and performance metrics of all the ad groups for a selected campaign.Org IDSelect the org you want to retrieve data for.GranularitySelect the desired date granularity for your report.Campaign IDsSelect the campaign IDs you want to retrieve data for.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/apple_search_ads_connector/index.html", "title": "Apple Search Ads Connector"}, {"objectID": "c8a93f77254e-2", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits I should be aware of?\nNo.", "source": "../../raw_kb/article/apple_search_ads_connector/index.html", "title": "Apple Search Ads Connector"}, {"objectID": "df0814cc5f11-0", "text": "TitleApple Search Ads OAuth ConnectorArticle BodyIntro\nApple Search Ads allows larger advertisers or agencies to create and manage many campaigns programmatically and to run reports for customer insights and trends. The Apple Search Ads OAuth connector allows to pull the data about the performance metrics for your campaigns and ad groups from your Apple Search Ads system into Domo. Use Domo's\u00a0Apple Seach Ads OAuth connector to combine your Apple Search Ads data with the data from other systems securely for more insights than ever. To learn more about the Apple Search Ads API, visit their page (https://searchads.apple.com/advanced...gn-management/).\nYou connect to your Apple Search Ads OAuth account in the Data Center. This topic discusses the fields and menus that are specific to the Apple Search Ads OAuth connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Apple Search Ads OAuth account and create a DataSet, you must have the following:\nYour Apple Ads client IDYour Apple Ads team\u00a0IDYour Apple Ads key IDThe\u00a0private key, including begin and end EC private key tags.\nYou get your client ID, team\u00a0ID, and key ID after uploading your public key to Apple Search Ads.\nVisit\u00a0Apple Search Ads OAuth documentation\u00a0to learn about obtaining your credentials.\nConnecting to Your\u00a0Apple Search Ads OAuth\u00a0Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the\u00a0Apple Search Ads OAuth\u00a0Connector page.\u00a0 The components of the other panes in this page, Scheduling and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Apple Search Ads OAuth\u00a0account. The following table describes what is needed for each field:", "source": "../../raw_kb/article/apple_search_ads_oauth_connector/index.html", "title": "Apple Search Ads OAuth Connector"}, {"objectID": "df0814cc5f11-1", "text": "FieldDescriptionClient IDEnter your Apple Search client ID that you get after uploading your public key to Apple Search Ads.\u00a0Visit\u00a0Apple Search Ads OAuth documentation\u00a0to learn more.Team IDEnter your Apple Search team ID that you get after uploading your public key to Apple Search Ads.\u00a0Visit\u00a0Apple Search Ads OAuth documentation\u00a0to learn more.Key IDEnter your Apple Search key ID that you get after uploading your public key to Apple Search Ads.\u00a0Visit\u00a0Apple Search Ads OAuth documentation\u00a0to learn more.Private KeyEnter the private key, including begin and end EC private key tags.\u00a0Visit\u00a0Apple Search Ads OAuth documentation\u00a0to learn more.\nOnce you have entered valid\u00a0Apple Search Ads OAuth\u00a0credentials, you can use the same account any time you go to create a new\u00a0Apple Search Ads OAuth\u00a0DataSet.\u00a0You can manage Connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary Reports menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/apple_search_ads_oauth_connector/index.html", "title": "Apple Search Ads OAuth Connector"}, {"objectID": "df0814cc5f11-2", "text": "MenuDescriptionReportSelect the Apple Search Ads OAuth\u00a0report you want to run. The following reports are available:Campaign Report (New)Returns metadata and performance metrics of all the campaigns for selected org.AdGroup Report (New)Returns metadata and performance metrics of all the adgroups for a selected campaign.Org IDSelect the org you want to retrieve data for.GranularitySelect the desired date granularity for your report.Campaign IDsSelect the campaign IDs you want to retrieve data for.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.", "source": "../../raw_kb/article/apple_search_ads_oauth_connector/index.html", "title": "Apple Search Ads OAuth Connector"}, {"objectID": "df0814cc5f11-3", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see Adding a DataSet Using a Data Connector.\nFAQs\nWhat version of the Apple Search Ads API does this connector use?\nThis connector uses version 3 of the Apple Search Ads API (https://api.searchads.apple.com/api/v3).\nWhich endpoint(s) does each report call in this Connector?\nReport NameEndpoint URL(s)AdGroup Report/reports/campaigns/{CAMPAIGN_ID}/adgroupsCampaign Report/reports/campaigns\nWhat kind of credentials do I need to power up this Connector?\nYou need the client ID, team ID, key ID, and private key associated with your Apple Search Ads account. You get your client ID, team ID, and key ID after uploading your public key to Apple Search Ads. You need to enter your private key including the begin and end EC private key tags.Visit the\u00a0Apple Search Ads OAuth documentation to learn about obtaining your credentials.\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits that I need to be aware of?\nNo\nTroubleshooting", "source": "../../raw_kb/article/apple_search_ads_oauth_connector/index.html", "title": "Apple Search Ads OAuth Connector"}, {"objectID": "df0814cc5f11-4", "text": "Are there any API limits that I need to be aware of?\nNo\nTroubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/apple_search_ads_oauth_connector/index.html", "title": "Apple Search Ads OAuth Connector"}, {"objectID": "c5e9b9c34dd6-0", "text": "Title\n\nApplying and Resetting Chart Properties\n\nArticle Body\n\nIntro\n\nYou can apply advanced properties to a chart using the\u00a0Chart Properties\u00a0tool. This can be found in the Analyzer for a chart, to the right of the chart. Setting properties to a chart can have far-reaching effects, so you can reset them if needed.\u00a0\nUnderstanding chart properties\nAvailable chart properties are different for each chart. Most charts have a specific set of categories of chart properties. For example, the Vertical Bar chart includes categories for General, Bar Settings, Data Label Settings, etc. Clicking any category in the Chart Properties brings up the available properties within that category. For example, for the Vertical Bar chart, clicking General would display properties for changing the font size, sorting on totals, and disabling animations.\nA number to the right of a category name indicates the number of enabled properties within that category. In the following example, one property has been set within General, two have been set within Bar Settings, and so on.\n\nVideo - Applying and Resetting Chart Properties", "source": "../../raw_kb/article/applying_and_resetting_chart_properties/index.html", "title": "Applying and Resetting Chart Properties"}, {"objectID": "c5e9b9c34dd6-1", "text": "Video - Applying and Resetting Chart Properties\n\n\u00a0\nApplying chart properties\nTo set properties for a chart, you must first Analyzer Layout.\nTo\u00a0apply Chart Properties,\n(Conditional) If the Chart Properties pane is not open, open it by clicking\u00a0Properties\u00a0in the toolbar at the top of the Analyzer.\u00a0Drag column names into all required fields\u00a0to enable a preview of your chart.For more information about specifying chart data, see\u00a0Understanding Chart Data.Specify the chart properties you want in the Chart Properties pane.\nResetting chart properties\nYou can reset chart properties to their default settings either by resetting all chart properties at once or by setting the Default option for individual chart properties.\nTo reset all chart properties,\nIn the Chart Properties pane, click the Reset All Chart Properties option.\nTo reset an individual chart property,\nIn the\u00a0Chart Properties pane,\u00a0locate and click\u00a0the chart property you want to reset.In the option menu, select the\u00a0Default\u00a0option.", "source": "../../raw_kb/article/applying_and_resetting_chart_properties/index.html", "title": "Applying and Resetting Chart Properties"}, {"objectID": "5055d0adce7f-0", "text": "Title\n\nApplying DataSet Columns to Your Chart\n\nArticle Body", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-1", "text": "Intro\nThe most important task in constructing a chart is selecting category (dimension), value (metrics), and series data from columns in a DataSet to power that chart. All chart types except\u00a0tables, heat map tables, calendars, and\u00a0Gantt charts require value (or numerical) data, and most require category and series data. For more information about these data types, see\u00a0Understanding Chart Data.\nIn the Analyzer, all columns in the DataSet are divided into two groups: Dimensions\u00a0and Measures. For any given chart type,\u00a0one or more fields appear. These fields contain the names of regions or elements within the chart. For the chart to be powered up, data columns must be applied to the appropriate fields. To apply a column to your chart, you\u00a0drag it from its group to the desired\u00a0field in the chart.\u00a0For example, a\u00a0percent of total\u00a0chart is accompanied by two fields\u2014Bar Name and Bar Value. To power up this chart, you would drag a category (dimension) column into the Bar Name field and a value (measure) column into the Bar Value field.\nThe number of required fields\u00a0differs between chart types. Some chart types\u00a0have only two, while others may have seven or more. Many chart types also have optional fields. For example, single bar charts do not require a series column, but you can add one if you want (in which case, the chart is no longer a single bar chart, but a multi-bar chart\u00a0such as\u00a0a grouped or stacked bar chart). For tables and heat map tables, only one column is required but you can add as many as you want, without regard to data type.", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-2", "text": "Many chart types allow you to add additional series beyond what is required for the chart if the columns are aggregated. This produces a \"trellis chart,\" in which additional dimensions are represented as boxes in the chart. You can also do this with date columns using \"Tiered Dates\"\u2014this allows you to incorporate multiple time periods in a single view.\u00a0\nA preview of the chart appears when appropriate columns have been dragged into each field. If you change a column in a field, the preview changes accordingly.\nWhen you power a new Card, Domo builds an initial chart, depending on your data, by automatically selecting a chart type, filling in the required fields, and choosing a\u00a0date grain (if available). For example, if your DataSet contained a value column, a category column, and a series column, Domo might initially build a preview of a vertical stacked bar chart. You can change these automatic selections if you want.\nWhen you drop a column name in a field, a panel opens with\u00a0additional options you can apply to the data in the associated column. The options available for a column depends on the data type of the column and the chart type. For example, a value column\u00a0in a stacked bar chart\u00a0opens a pane with options for changing the goal, label, and number format, whereas a category column shows only the label option.\nIf you drag a column onto a field that already contains a column, the new column replaces the old one.\nIf you apply a column with PDP restrictions to a chart, you see only data permitted by your policy. However, if you were to save the chart, a viewer not restricted by that policy would see all of the data. For more information about PDP, see\u00a0Creating and Deleting PDP Policies.\nFor more information about the Analyzer, see\u00a0Analyzer Layout.\nFor more information about the specific requirements for each chart type, see\u00a0Chart Types.\nChoosing a category column", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-3", "text": "Choosing a category column\nCategory data is the data being measured in your chart. To\u00a0apply category data to your chart, you drag\u00a0a column\u00a0from the Dimensions group into the desired field.", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-4", "text": "Tip:\u00a0If the column names in the left-hand pane\u00a0are too long to read, you can expand them by clicking and dragging the border on the right side of the pane.\n\n\n\nFor example, in the\u00a0chart below, the creator wanted to\u00a0measure the rental prices\u00a0of five condominium complexes, so\u00a0she dragged the Condominium Park column into the X Axis field.\u00a0These items are then represented\u00a0along the horizontal axis.", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-5", "text": "In charts where information is recorded over time, the column with the date and/or time data is usually applied as the category.\nYou typically apply category columns to a chart by\u00a0dragging them into the X Axis field. However, in some chart types, the fields into which you typically drag category columns have different names:", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-6", "text": "Chart typeField where category data is usually appliedMost vertical bar charts, all horizontal line charts, all vertical area charts, all vertical lollipop charts, stream, vertical symbol, vertical symbol overlay\u00a0X AxisMost horizontal bar charts, all vertical line charts, all horizontal area charts, all horizontal lollipop charts, category scatter, stream funnelY AxisVertical percent of total, horizontal percent of totalBar NamePie, nautilusPie NameFunnel, folded funnel, funnel barFunnel Section NameDonut, nautilus donutDonut Slice NameParetoPareto NameRadar, histogramCategory NamesVertical bullet, horizontal bulletNamesUS map,\u00a0Australia map, Germany map, Austria map, Brazil map, India map, Malaysia map, Nigeria mapState NameJapan map\u00a0Prefecture Name\u00a0Canada map, China map, Indonesia map, Panama map, South Korea map, Spain mapProvince Name\u00a0New Zealand map, United Kingdom mapDivision NameChile map, France map, Ghana map, Italy mapRegion NamePeru mapDepartment NamePortugal mapDistrict NameSwitzerland mapCanton NameUAE mapEmirate NameMexico mapState Code/NameWorld map, all continental and regional mapsCountry Code/NameHeat mapCategory 1 and Category 2 (Heat maps require two category columns)TreeMapTreeMap NameRadial gauge, filled gauge, face gauge, progress bar gaugeGauge NameWord cloudWord NamesGantt, candlestick, high lowCategory (optional in Gantt charts)Vertical waterfall, Horizontal waterfall\u00a0Summary Group  and\u00a0 Item Names \u00a0(Waterfall charts require two category columns)Vertical box plot, Horizontal box plotCategory NamesAll data science charts, table, heat map tableNone\nChoosing a value column\u00a0\nThe specific measurements or metrics in your chart constitute value data. To\u00a0apply value data to your chart, you drag\u00a0a column\u00a0from the Measures\u00a0group into the desired field above the chart.", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-7", "text": "Tip:\u00a0If the column names in the left-hand pane\u00a0are too long to read, you can expand them by clicking and dragging the border on the right side of the pane.\n\n\n\nFor example, in the\u00a0Monthly Rental Prices chart, the creator wanted to\u00a0assign the\u00a0monthly rental prices as the values, so she dragged the\u00a0\"Price per Month\" column into the\u00a0Y  Axis field.\u00a0These items are then represented\u00a0along the vertical\u00a0axis.", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-8", "text": "You usually apply value columns to a chart by dragging them into the Y Axis field. However, in some chart types, the fields into which you typically drag category columns have different names:\nChart typeMenu with value dataMost horizontal bar charts, all vertical line charts, all horizontal area charts, all horizontal lollipop charts, category scatter, stream funnelX AxisMost vertical bar charts, all horizontal line charts, all vertical area charts, all vertical lollipop charts, stream, vertical symbol, vertical symbol overlay\u00a0Y AxisVertical percent of total, horizontal percent of totalBar ValuePie, nautilusPie ValueFunnel, folded funnel, funnel barFunnel Section ValueDonut, nautilus donutDonut Slice ValueParetoPareto ValueRadarRadar ValuesVertical bullet, horizontal bulletActual ValueUS map,\u00a0Australia map, Germany map, Austria map, Brazil map, India map, Malaysia map, Nigeria mapState ValueJapan map\u00a0Prefecture Value\u00a0Canada map, China map, Indonesia map, Panama map, South Korea map, Spain mapProvince Value\u00a0New Zealand map, United Kingdom mapDivision ValueChile map, France map, Ghana map, Italy mapRegion ValuePeru mapDepartment NamePortugal mapDistrict NameSwitzerland mapCanton NameUAE mapEmirate NameMexico mapState Code/NameWorld map, all continental and regional mapsCountry ValueHeat map, vertical and horizontal waterfallValueTreeMapTreeMap ValueRadial gauge, filled gauge, face gauge, progress bar gaugeGauge ValueAll data science chart typesX Axis and Y Axis (these chart types have two value scales)Word cloudWord ValuesTable, heat map table, Gantt, box plotNone\nChoosing a series column or row", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-9", "text": "Choosing a series column or row\nA data series is a component in a category. Most chart types require series data. For example, in a vertical grouped bar chart, each \"set\" of bars is a category, and each bar in a set is a series.\u00a0Series in a chart are often represented in a legend. Typically, to apply a series you drag textual (non-date-time) columns from the Dimensions grouping into the\u00a0Series field.\u00a0However, you can set date-time\u00a0and value columns\u00a0as series if you want. This is discussed in more detail in the next section.\nIn the\u00a0Monthly Rental Prices\u00a0chart, the prices for the condominium parks are broken down by unit type, so the user\u00a0dragged the\u00a0\"Unit Type\" column into the\u00a0Series field.", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-10", "text": "Vertical bar charts, horizontal bar charts, and line charts do not require series data, but for each of these chart types you can add a series if you want. You do this by simply dragging and dropping a column into the Series field.\nFor example, the following simple vertical bar chart lacks a series:\n\nAfter adding a series that breaks down the categories into \"Product\" and \"Service,\" the following displays:\n\nLikewise, this basic line chart has no series:\n\nAfter adding the same series as before, two separate line charts appear in one chart, each one for a different type, as follows:", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-11", "text": "Adding value and date columns as series\nYou can also apply a column containing value or date data as a series. However, to add multiple series that contain value or date data, these columns must be aggregated (applying a count to the column). This functionality makes it possible to have multiple values or date columns represented as series in your chart.\nFor example, to add an additional column in your series field,\nDrag and drop a column in the Series fieldClick the Aggregation drop-down and select the appropriate aggregationAnother Series field becomes available\nAdding additional categories as series (\"trellis chart\")\nIn most kinds of charts that include series data, you can add up to two additional dimensions besides the required series. The additional dimensions are represented using boxes overlaid on top of each grouping (sometimes referred to as a \"trellis chart\").\u00a0The following screenshot shows an example of this, in which an additional dimension has been added to show the lead source. \u00a0\u00a0\u00a0\n\u00a0\u00a0\nTo add an additional dimension, you must first turn on the Trellis feature by navigating to the chart properties and clicking Trellis/Tiered Date Settings. Then, select Trellis Categories from the\u00a0Show As drop-down (If this chart property isn't available, then the trellis functionality is not available on that chart type.) Drag the desired category from the\u00a0Dimensions\u00a0grouping into the\u00a0Category 2\u00a0field above the chart preview area. When you do this, another field,\u00a0Category 3, appears. If you want a second additional dimension in the chart, you can include it by dragging another category into this field.\nNote that the above functionality is recommended only for\u00a0string\u00a0columns. When you add date columns as additional categories in this way, your dates may appear out of order. If you want to show multiple levels of dates in a chart, it is recommended that you use \"Tiered Dates,\" which are described in the next section.", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "5055d0adce7f-12", "text": "Adding additional date categories\nTiered Dates\u00a0provide for greater ease of understanding by presenting multiple time periods in a single view. For example, you can show weekly data that is also grouped by month and by quarter\u2014all on the same chart. This provides a view similar to that of the \"trellis chart\" as described above. For any chart, you can choose up to 3 date tiers.\u00a0\u00a0\n\u00a0\nThe process for adding Tiered Dates is different from that of adding a trellis chart, as you must do so using the Chart Properties rather than dragging columns into fields over the chart. To add Tiered Dates, navigate to the Chart Properties, click Trellis/Tiered Date Settings, and choose Tiered Dates in the\u00a0Show As drop-down. Then, select\u00a0the desired number of tiers in the\u00a0Tiered Date Levels\u00a0drop-down.\u00a0\nAggregating your data\nWhen you drop a column in a value or series field, a menu appears with options you can select to aggregate your data.\nOptionDescriptionNo AggregationThe chart shows the final value for each category in the DataSet.SumThe chart shows the sum of all values for each category in the DataSet.MinimumThe chart shows the smallest value for each category in the DataSet.MaximumThe chart shows the largest value for each category in the DataSet.AverageThe chart shows the average value for each category in the DataSet.CountThe chart shows the number of values for each category in the DataSet.\nFor value columns, all options are available. For series columns, typically only No Aggregation and Count are available. However, if you add a value or date column as a series, then all the options are available.\nFor information about changing the aggregation type for your summary number, see\u00a0Configuring Your Chart Summary Number.\nFor information about building each specific chart type by plugging in data, see\u00a0Building Each Chart Type.\nVideo - Aggregating Columns in Analyzer", "source": "../../raw_kb/article/applying_dataset_columns_to_your_chart/index.html", "title": "Applying DataSet Columns to Your Chart"}, {"objectID": "77ba94c2c0ca-0", "text": "Title\n\nApplying Page-Level Filters with Filter Views\n\nArticle Body\n\nNote: Dashboard is the term for the section of the Domo interface where you can organize and view your cards. It replaces the term Page.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-1", "text": "Intro\nYou can use Page Filters to apply data filters that affect all of the KPI, Sumo, and Domo App Cards powered by a specified DataSet on a dashboard. All such cards with applied filters are marked with a  icon. In this way, you can quickly spot all cards on the dashboard for which you have applied filters. For example, you could set a filter for all cards powered by a specified DataSet to show values falling under a specified threshold. All cards powered by that DataSet would then be filtered to show only the specified information.\nIf PDP\u00a0(Personalized Data Permissions) is enabled for the selected\u00a0DataSet, your filter options are limited to what is available to you based on your policy. For example, if you are a member of a policy\u00a0that can view content\u00a0only for western states, you\u00a0can filter\u00a0only on content that is available to you. Likewise, if you have access to all rows in a policy, you\u00a0can choose whether to filter on all data or select specific policies. For more information about PDP, see\u00a0Creating and Deleting PDP Policies.\nThe Page Filters applied to a dashboard are\nsomewhat personal customizations.\u00a0When a Page Filter is saved, the values selected affect your view only and others do not see the Page Filter values you add.\u00a0 You can save specific filter configurations as \"Filter Views\" and then choose your desired Filter View when you navigate to the dashboard. Only you see your saved Filter Views. The only exceptions to this are when Filter Views are saved as the \"default\" or are shared with everyone, in which case all visitors to the page see them.\u00a0\"sticky\" customizations that remain until you remove them.inherited by a card using the filtered DataSet when viewing or drilling to the card in the Details view.not applied to a card when viewing the card directly, either through a link or a search.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-2", "text": "You can apply or edit Card Filters in the Details view of a card. For information about applying Filters to KPI Cards, see Adding Filters to Your Chart.\nWhen adding date Filters, you can choose to apply Filters to a range of dates or to individual time units (which may be days, weeks, months, etc., based on the date grain of the data).", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-3", "text": "Important:\u00a0When adding subsequent cards to a dashboard (that is, all cards after the first), you must use the Existing Data option and select your original DataSet. If you re-add the same DataSet using the Excel or Google options, filtering does not work.\n\n\n\n\n\n\n \n\n\nNote: If you have a Participant default security role, you cannot share filter views or set default filter views for the page, but you can add and save filter views for personal use. Participants can also select a filter shared with them, make changes to the filter and save it as a new filter view. The original shared filter view will not be affected. If you have an Admin, Privileged, or Editor default security role, you can create, share, and edit existing shared views, or set a shared filter view as the default. You can also do this if you have a custom role with Edit Pages enabled. For more information about default security roles, see Managing Custom Roles.\n\n\n\nVideo - Filtering Pages with Filter Views\n\nControlling How Page Filters Affect Your Dashboard\nDashboard owners, users with an Admin default security role, and users with a custom role with the Manage pages grant enabled can control whether Page Filters are available on a dashboard, allow users to add new filters, and hide the filter icon on their cards.\nTo access Filter options,\n1. Navigate to the dashboard in Domo that you want to filter.\n2. Select Dashboard Options.The dashboard options menu displays.\n3. Select Filter options from the menu.\n4. To disable the adding of Filters to your Page, deselect the toggle for Show filter bar.\n\n\n\n\n\nNote: Hiding the filter bar will clear all filters. Any saved Filter Views are still saved but are not accessible when hidden.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-4", "text": "5. (Conditional) If Show filter bar is enabled, you can select:\nAllow adding of new filters\nAllow global date filters\nAllow segments\nThese settings are enabled by default.\n6. To disable the filter icons on your cards, deselect the toggle for Show filter icons.\nApplying Page Filters\nUse the following instructions to apply Page Filters to cards on a dashboard.\n1. Navigate to the dashboard in Domo that you want to filter.\n2. Select  Show Page filters. (Some system dashboards, such as the Shared dashboard, do not include this option.)A ribbon displays with the Filter Views menu,\u00a0a  button, a date picker, and the Save Filters button.\u00a0If PDP\u00a0policies are enabled on DataSets\u00a0used in any Cards in the Page,\u00a0and\u00a0you have access to all the\u00a0rows, a button reading All Rows\u00a0appears.\u00a0\n3. (Conditional) If you want to filter on all rows in PDP-enabled DataSets used on the dashboard, leave the PDP options set to All Rows. If you want to filter on a specific policy, do the following:\nClick the dropdown arrow next to All Rows.Click Select Policies.Check the boxes for all policies you want to Filter on.\u00a0Click Apply.For more information about PDP, see\u00a0Creating and Deleting PDP Policies.\n4. Select  Add Filter.A list of all column names used in DataSets in cards on this dashboard appears. If a column name is used in more than one DataSet, an arrow appears to the right of the name so you can filter down to the desired DataSet.\n5. Select the name of the column you want to filter.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-5", "text": "5. Select the name of the column you want to filter.\nAlternatively, if the column name is used in more than one DataSet and you want to filter to show just the rows in a specific DataSet, you can click the arrow next to the column name to bring up\u00a0a dialog with the names of all the applicable DataSets. Here you can check the boxes for all the DataSets you want to show rows for (or simply select all DataSets by checking the\u00a0Select all\u00a0box).\u00a0If you select a column that appears in more than one DataSet without filtering down to a specific DataSet, the columns from the DataSet powering the most cards on the dashboard will be used for the Filter.\u00a0\u00a0\u00a0After you select a column or DataSets, a Filter button appears in the ribbon and a modal appears in which you can select your Filters. The interface components of the modal differ depending on whether the column or row you selected contains series, amount, or date data.\n6. Do one of the following:", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-6", "text": "(Conditional) If the selected column contains string data,a. Select the checkboxes for each series you want to show in the cards on this dashboard.You can choose all of the checkboxes by selecting All or deselect all of the checkboxes by selecting None.You can filter the series that appear in the list by entering a keyword or a string of characters found in a keyword in the Filter by field.NOTEFinally, if you want to load values from a specific DataSet, you can select  XXXNAME then choose the desired DataSet. This loads the unique values on that DataSet but applies to all DataSets with that same column name. b. Specify whether the items you have checked appear or do not appear in your chart by selecting In or Not In from the menu in the top right.c. Select Apply.(Conditional) If the selected column contains amount data, do one of the following.- If you want to filter amounts based on a range, follow these steps:a. Leave the menu in the upper right corner set to Range.XXXXIMAGEXXXb. Select a condition statement from", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-7", "text": "Range.XXXXIMAGEXXXb. Select a condition statement from the menu.One or more fields may display, depending on the condition statement.c. (Conditional) If one or more fields display when you select a condition statement, enter the desired amounts in the fields.For example, if you wanted your Cards to show information for amounts between 200,000 and 300,000, you would select is between from the menu, enter \"200,000\" in the first field, and enter \"300,000\" in the second field.- If you want to filter amounts based on specific values from your DataSet, follow these steps:a. Select Selection in the menu in the XXXb. Check the boxes for all of the values you want to filter on. You can select all of the checkboxes by clicking All or deselect all of the checkboxes by clicking None. Or you can filter the values that appear in the list by entering a string of numbers in the Filter by field.If you want to load values from a specific DataSet, you can click\u00a0 COPY FROM ABOVE then choose the desired DataSet. This", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-8", "text": "COPY FROM ABOVE then choose the desired DataSet. This will load the unique values on that DataSet but apply to all DataSets with that same column name. c. Finally, you can specify whether the items you have checked appear or do not appear in your chart by selecting In or Not In from the menu in the top right.d. Select Apply.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-9", "text": "(Conditional) If the selected column contains date data,\nDo one of the following:If you want to filter\u00a0dates\u00a0based on a range...\u00a0Leave the dropdown menu in the upper right corner set to\u00a0Range.Select a condition statement from the menu.One or more fields may appear, depending on the condition statement.Enter the desired dates in the fields as necessary.For example, if you wanted your Cards to show information for dates between 1-31-2014 and 4-30-2015, you would select is between from the menu, select January 1, 2014\u00a0in the first field, and select April 30, 2015 in the second field.If you want to filter dates based on specific dates from your DataSet...Select\u00a0Selection\u00a0in the dropdown menu in the upper right corner.Check the boxes for all of the dates you want to filter on.\u00a0You can select all of the checkboxes by clicking All or deselect all of the checkboxes by clicking None. Or you can filter the dates that appear in the list by entering a filter string\u00a0in the Filter by field. (For example, if you wanted to filter down to dates from 2010, you would enter \"2010\" into this field.)If you want to load values from a specific DataSet, you can click\u00a0\u00a0then choose\u00a0the desired DataSet. This\u00a0will load\u00a0the\u00a0unique\u00a0values on that DataSet but apply to all DataSets with that same column name.\u00a0Finally, you can specify whether the dates you have checked appear or do not appear in your chart by selecting In or Not In from the menu in the top right.Click Apply.\nThe affected Cards in the Page update to reflect the Filters you have selected. In addition,\u00a0a  icon appears on each affected Card.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-10", "text": "Note: You can also use Aggregated Filters to filter a Dashboard. For more information, see Aggregated Filters.\n\n\n\nTo edit a Filter,\nClick the button for the Filter in the gray bar at the top of the Page.The dialog for the Filter appears.Edit the Filter settings as desired.Click Apply.\nYour changes are applied to the Filter.\nTo remove a Filter,\nClick the button for the Filter in the gray bar at the top of the Page.The dialog for the Filter appears.Click .Click Continue to confirm the removal.\nThis Filter is removed, and all of the Cards that used the Filter are updated accordingly.\nTo hide Page Filters,\nClick , which is located in the upper right area of the Page.\nYour Page Filters are hidden and all of the affected Cards are updated. Hiding Filters in this way does not deactivate them completely. If you want to deactivate Filters, you must remove them using the steps above.\nYou can reapply your hidden Filters by clicking\u00a0.\n\n\n \n\n\nNote:\u00a0Turning off Page Filters does not remove the Filter for all users.\u00a0The only way to make a Filter on a Card apply to all users is to apply a Filter while editing a Card. Consequently, you cannot persist a Page Filter to multiple Cards for all users.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-11", "text": "Using Filter Views\nFilter views allow you to customize a dashboard to meet the needs of all audiences. Using Filter views, you can do all of the following:\nCreate and save your own Filters without affecting anyone else's view.Curate and share important views to create alignment on a specific perspective.Provide any number of personalized data stories for any person in any role.Designate which saved Filter view is the \"default\" for first-time visitors to a dashboard.\nAny user with a default security role of Participant or higher can add Filter views as well as rename, copy, and delete their own Filter views. To share Filter views with all dashboard visitors or set them as the default view for a LOCKED Page, you must either be the dashboard owner or have a role with the Manage Cards and Pages or Edit Pages grant enabled. For UNLOCKED Pages, any user with a default security role of Editor or higher can share Filter views and set default views. For more information about custom roles, see Locking or Unlocking Page Content.\nSaving a Filter View\nOnce you save a Filter view, it is added to your\u00a0Filter Views menu on the left side of the gray filter bar. XXXX From here, you can choose it anytime you visit this page to quickly apply the Filter.\u00a0You are the only one who sees Filters you have created,\u00a0unless a Filter has been designated as the \"Default\" Filter for the dashboard or is shared with everyone, in which case all users are able to select it. However, just because a default Filter has been set, this does not mean users are required to use it. They are still able to create and set their own Filter views as desired.\nWhen a default Filter view is configured, those Filters will load anytime somebody visits the dashboard for the first time. For each subsequent visit, the Filters that user had configured the last time they were on the dashboard will load again, starting them where they left off.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-12", "text": "Do the following to create and save a Filter view:\n1. Create the Filter as explained in Applying Page\u00a0Filters, above.\n2. Select Save Filters in the ribbon.RESULT??XXXX\n3. Select Create New Filter View.\n4. Enter a descriptive name for the new Filter.\n5. (Optional) If you want this to be the default Filter for the dashboard, (in which case, this Filter configuration will be the first thing users see when they open the dashboard,) check the Set as default for everyone box. Be aware that if a default Filter already exists for this dashboard, the new Filter will replace it as the default.\u00a0\n6. Select Save.\nThe new view is added to your\u00a0Filters view\u00a0menu.\u00a0\nUpdating a Filter View\nIf you make a change to a Filter, that change is not applied automatically to any Filter views that include it. If you want the Filter views to update, you must do this manually.\nDo the following to update a filter view.\n1. Select Save Filters in the ribbon.\n2. Select Update Existing Filter View.\n3. Select the Filter view to update.\n4. Select Update.\nThe selected Filter view is now updated.\nRenaming a Filter View\nYou can rename any Filter view you have created, as long as you meet the following criteria:\nYou are the dashboard ownerYou have a role with the Manage Cards and Pages or Edit Pages grant enabledThe dashboard is unlocked and you have a default security role of Editor or higher\nFor more information about custom roles, see\u00a0Managing Custom Roles.\nFollow these steps to rename a Filter view:\n1. Select the Filter views menu in the ribbon.\u00a0\nMouse over the Filter View you want to rename and click the\u00a0icon that appears.\nSelect\u00a0Rename.\nEnter the desired new name for the Filter view.\nClick\u00a0Save.\nCopying a Filter view", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-13", "text": "Enter the desired new name for the Filter view.\nClick\u00a0Save.\nCopying a Filter view\nYou can copy any Filter view in a Page, even those you haven't created that are set to default. This is a good way to make small changes to an existing configuration without having to rebuild it from scratch.\nTo copy a Filter view,\nClick into the\u00a0Filter views\u00a0menu on the left side of the gray filter bar.Mouse over the Filter View you want to copy\u00a0and click the\u00a0icon that appears.Select\u00a0Copy.(Optional) Enter a new descriptive name for the Filter View.Click\u00a0Save.\nDeleting a Filter view\nYou can delete\u00a0any Filter view you have created.\u00a0You can only delete a default Filter view if...\nyou are the Page owner,you have a role with the \"Manage Cards and Pages\" or \"Edit Pages\" grant enabled, orthe page is unlocked and you have a default security role of \"Editor\" or higher.\n\u00a0For more information about custom roles, see\u00a0Managing Custom Roles.\nTo delete a Filter view,\nClick into the\u00a0Filter views\u00a0menu on the left side of the gray filter bar.Mouse over the Filter view you want to delete and click the\u00a0icon that appears.Select Delete.Click Delete\u00a0to confirm.\nSharing a Filter view\nIf you want a Filter view to be available to all visitors to a Page without necessarily being the default view, you can do so by toggling on the\u00a0Share with everyone\u00a0option for that Filter View. You can only do this if...\nyou are the Page owner,you have a role with the \"Manage Cards and Pages\" or \"Edit Pages\" grant enabled, orthe page is unlocked and you have a default security role of \"Editor\" or higher.\n\u00a0For more information about custom roles, see\u00a0Managing Custom Roles.\nTo delete a Filter view,", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-14", "text": "To delete a Filter view,\nClick into the\u00a0Filter Views\u00a0menu on the left side of the gray filter bar.Mouse over the Filter View you want to share and click the\u00a0icon that appears.Turn on the Share with everyone\u00a0option.\nApplying dynamic date range Filters\nYou can use dynamic date range Filters to adjust the date range window for all the Cards on a Page. These Filters can be saved and used as the default for all users in a Page. They can also be saved to different Filter Views (described in the previous section). Dynamic date range Filters can be a huge time-saver, since users do not need to manually apply filters to see data for a given date or time range. Also, with dynamic date range Filters, the date range data for your Cards automatically \"rolls over\" when the current period ends.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-15", "text": "Note: Dynamic date range filters only apply to Visualization Cards and Sumo Cards. They do NOT work on Doc Cards, Poll Cards, Notebook Cards, and Custom Apps.\n\n\n\n\n\n\n\n\nImportant: Dynamic date rang filters will not work on a Card if, inside of Analyzer, the Graph By of the Card has been set to None.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-16", "text": "You choose dynamic date range Filters in the\u00a0Choose Date\u00a0menu in the gray filter bar at the top of a Page.The Filters you choose here are applied to all Cards in the Page that have had a date column applied in Analyzer. Filters are applied to each Card's unique date column and take into account date filters that have already been set on the Card. So for example, if a Page contained a Card that was powered by the \"Sales Dates\" column and the dates were filtered in Analyzer to show March through June, if dynamic date range Filters were set on the Page, the Filters would be applied to the \"Sales Dates\" column and would only work within the March-June time period. If the dynamic Filters were set outside that time period, the Card would display \"No data in filtered range.\"\nYou are able to choose Cards that will not be affected by the dynamic date, by deselecting the Allow global date feature. More information can be found in the Controlling how Page Filters affect your Page section above. If you have chosen the Hide Date on Card Details option for a Card, the dynamic date will also not be applied to the Card.\nMost of the options for dynamic date range Filters are the same as those for filtering dates in the Analyzer. In essence, these options are as follows:", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-17", "text": "Date Range. This lets you choose the range of data shown for all Cards in the Page being powered by date columns. By default\u00a0this is\u00a0set to\u00a0Default, meaning that all Cards use their own date ranges. A wide variety of other options is available, including the current date period, period-to-date options such as\u00a0Week to Date, and so on.\u00a0Graph By. This lets you choose the date grain for all Cards in the Page being powered by date columns.\u00a0The date grain determines\u00a0whether the data is shown by week, by month, etc. This menu is described in more detail in\u00a0Changing the unit of time used to represent data. By default no date grain is selected, meaning the Cards use the same date grain used in their powering DataSets. (For example, if data for a Card was broken down by week in the DataSet, it would be automatically broken down by week in the Card).", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-18", "text": "Note: The Date Graph By does not impact Single Value charts. This is to ensure best data practices as Graph By will impact which row is returned in the chart query.\n\n\n\nApplying Card-to-Card Filters\nWhen you apply Card-to-Card Filters in a Page, you can mouse over a specific chart element in a Card, and that same element will be highlighted in all other Cards in the Page that share it. This allows you to make quick correlations between your Cards that might otherwise take hours.\nFor example, let's say user Tanya is a manager in her company's retail department. She has access to a Domo Page called \"Retail\" that includes a number of Cards related to profits\u00a0and spending for her company, as shown here:\n\u00a0 \u00a0\u00a0\nAll of these Cards show disparate but related data for roughly the same time period\u2014early 2013 to early 2015\u2014with a monthly date grain. As it is now, Tanya can go into each Card one at a time and see data for a given month. This is useful, but what Tanya really wants to do is compare data for all of the different Cards across the same month. Specifically, she wants to correlate the data for \"RT: Gross Profit and Margin,\" \"RT: Gross Margin by Product Line,\" and \"RT: Cost per Conversion\" for January 2014. Before Card-to-Card\u00a0Filters, she probably would have had to look at each Card one at a time, write down the values for January 2014, and finally compare them.\u00a0But now, all she has to do is mouse over the bar for January 2014 in any of the Cards, and the same bar highlights in all of the others:\n\n\n\n \n\n\nTip:\u00a0Card-to-Card Filters work best when the Card size is set to \"Large\" or \"Full.\" For more information about Card sizing, see\u00a0Changing the Size of Cards in a Page.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-19", "text": "With Card-to-Card Filters, you can also click on a chart element to apply a Filter for that element to all Cards in the Page. For example, if you clicked on the bar for \"2014-Jan,\" all Cards would be filtered to that month. This also works for rows in tables. You can turn off this Filter by clicking the element again.\nCard-to-Card Filters are automatically enabled in Storytelling Dashboards. In standard Pages, they can be toggled on or off by Page owners, users with an \"Admin\" default security role, or users with a custom role with the \"Manage pages\" grant enabled.\u00a0They do this by toggling the Interaction filters option, which is located in the wrench menu on the right side of the filter bar.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-20", "text": "When this option is enabled on a Page, all users (\"Participants\" included) can mouse over Cards to see Card-to-Card Filters in action. When the option is disabled, \"Participant\"-level users cannot apply Card-to-Card Filters.\nUsing Filter Cards to\u00a0filter Card content\u00a0\nYou can use Date Selector Cards to apply Filters to all of the Cards on a Page.\u00a0You build these Cards\u00a0in Analyzer just like any other chart types.\nFilter Cards only work when Card-to-Card Filters are turned on (see the preceding section for more information). If Card-to-Card\u00a0Filters are not turned on, clicking a Filter opens the Details view for the Card instead of applying the Filter.\nSlicer, Checkbox, and Radio Button Cards\nSlicer,\u00a0Checkbox, and Radio Button Cards contains Filters from a DataSet column. Once you have added one of these Cards\u00a0to a Page, you can click any of the Filters in the Card to apply that Filter to all of the Cards in the Page. With Slicer and Checkbox Cards, you can apply as many Filters as you want. With Radio Button Cards, you can apply only one Filter at a time.\nYou can deactivate a Filter by clicking on it a second time.\nThe following animated GIF shows an example of this. Here, marketing manager Scott wants to view Eloqua campaign data for email and social media channels. Instead of having to configure Page Filters, all Scott has to do is click all of these Filters in the Slicer Cards. He clicks \"Email\" and \"Social Media\" in the \"Eloqua: RT Marketing\" slicer Card. All of the other Cards in the Page are then Filtered accordingly.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-21", "text": "(If you would like to download a full-size version of this GIF, just click on it.)\nFor information about building Slicer,\u00a0Checkbox, and Radio Button Cards, see\u00a0Slicer, Checkbox, and Radio Button Cards.\nRange Selector Cards\nRange Selector Cards let users filter all Cards on a Page to reflect a selected range of values or dates. A slider appears with a minimum and maximum value. When a user changes these minimum and maximum values, all Cards update to show data within the new range. In the following animated GIF, the user sets the minimum value (which was originally set to 2,000 by the Card creator) to 3,011, and the maximum value (originally 4,998) to 3786. All of the other Cards in the Page update to reflect the new range.\n\n(If you would like to download a full-size version of this GIF, just click on it.)\nFor information about building Range Selector Cards, see\u00a0Range Selector Card.\nDate Selector Cards\nBy default, a Date Selector Card displays a series of dates in calendar format, either in a yearly or monthly view (depending on settings you have applied in Chart Properties). You can click and drag over a range of dates to filter all other Cards in the Page to show data for those dates. In the following animated GIF, the user highlights all dates from April 1st to August 31st in the \"Street Metrics Date Selector\" Card. The four other Cards in the dashboard reflect this selected range.\n\n(If you would like to download a full-size version of this GIF, just click on it.)\nAlternately, you can click the\u00a0Presets link on the Card to display a number of premade date Filters, such as \"This week,\" \"Last year,\" etc.\n\n\nYou can show different premade Filters by clicking the blue links over the buttons (\"Weeks,\" \"Months,\" etc.).", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "77ba94c2c0ca-22", "text": "Note: Using the Presets filters will act as a Global Date filter on the Page just like using the dynamic date range filter and will filter all Cards on the Page, not just the Cards that contain the same date column as your Filter Card.\n\n\n\nYou can clear date Filters from a Date Selector Card by clicking the\u00a0Clear\u00a0link or by mousing over the Card and clicking\u00a0.\nFor more information about building Date Selector Cards, see\u00a0Date Selector Card.", "source": "../../raw_kb/article/applying_pagelevel_filters_with_filter_views/index.html", "title": "Applying Page-Level Filters with Filter Views"}, {"objectID": "40ef27a2b84b-0", "text": "TitleAppNeta AppView ConnectorArticle BodyIntro\nAppNeta provides proactive end-user performance monitoring for any application, network, or cloud. To learn more about the AppNeta AppView API, visit their page (https://docs.appneta.com/api.html).\nYou connect to your AppNeta AppView account in the Data Center. This topic discusses the fields and menus that are specific to the AppNeta AppView connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your AppNeta AppView account and create a DataSet, you must have the following:\nYour AppNeta username and password.The instance for the AppNeta AppView account.\nConnecting to Your AppNeta AppView Account\nThis section enumerates the options in the Credentials and Details panes in the AppNeta AppView Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AppNeta AppView account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter your AppNeta username.PasswordEnter your AppNeta password.InstanceEnter your AppNeta instance URL.\nOnce you have entered valid AppNeta AppView credentials, you can use the same account any time you go to create a new AppNeta AppView DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/appneta_appview_connector/index.html", "title": "AppNeta AppView Connector"}, {"objectID": "40ef27a2b84b-1", "text": "MenuDescriptionReportSelect the AppNeta AppView report you want to run.\u00a0The following reports are available:Alert ProfileReturns a list of alert profiles.ApplianceReturns a list of appliances.DiagnosticReturns a list of diagnostics.Diagnostic DetailsReturns detailed information about diagnostics.\u00a0FlowReturns a list of flows.Flow ApplicationReturns flow application details data.ObserverReturns a list of observers.PathReturns a list of paths.Path DataReturns detailed information about paths.Saved ListReturns saved list data.Web ApplicationReturns a list of web applications.Web Monitors by WebApplication IDReturns a list of web monitors by WebApplication ID.Performance Data by WebApplicationID and WebMonitorIDReturns performance data.AppNeta AppView Organization DetailsEnter the ID of the organization you want to retrieve data for.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/appneta_appview_connector/index.html", "title": "AppNeta AppView Connector"}, {"objectID": "4c3fb9fdbdf2-0", "text": "TitleAppNeta TraceView ConnectorArticle BodyIntro\nAppNeta provides proactive end-user performance monitoring for any application, network, or cloud. To learn more about the AppNeta TraceView API, visit their page (https://docs.appneta.com/api.html).\nYou connect to your AppNeta TraceView account in the Data Center. This topic discusses the fields and menus that are specific to the AppNeta TraceView connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your AppNeta TraceView account and create a DataSet, you must have the AppNeta access key for your\u00a0organization.\u00a0\u00a0You can find your access key either from\u00a0your organization page\u00a0or in your config file (/etc/tracelytics.conf) on a machine with TraceView installed.\nConnecting to Your AppNeta TraceView Account\nThis section enumerates the options in the Credentials and Details panes in the AppNeta TraceView Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AppNeta TraceView account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAccess KeyEnter the AppNeta access key for your organization. For more information about obtaining an access key, see \"Prerequisites,\" above.\nOnce you have entered valid AppNeta TraceView credentials, you can use the same account any time you go to create a new AppNeta TraceView DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane", "source": "../../raw_kb/article/appneta_traceview_connector/index.html", "title": "AppNeta TraceView Connector"}, {"objectID": "4c3fb9fdbdf2-1", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the AppNeta TraceView report you want to run.\u00a0The following reports are available:Latency Client SeriesReturns a client-side time series of the application's latency and volume.Latency Client SummaryReturns a client-side summary of the latency and volume traced.Latency Server by LayerReturns time series data for each layer in the application.\u00a0Latency Server SeriesReturns a server-side time series of the application's latency and volume.Latency Server SummaryReturns a server-side summary of the latency and volume traced.Layers by AppNameReturns a list of layers assigned to the specified app.Hosts by AppNameReturns a list of hosts assigned to the specified app.\u00a0Total Requests SeriesReturns a time series for the application's total requests.Total Requests SummaryReturns a summary of the total requests.Trace SummaryReturns a high-level overview of a particular trace's performance characteristics.\u00a0AppNeta\u00a0AppNameSelect the app you want to retrieve data for.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/appneta_traceview_connector/index.html", "title": "AppNeta TraceView Connector"}, {"objectID": "4acf63f13fa0-0", "text": "Title\n\nAppNexus Connector\n\nArticle Body", "source": "../../raw_kb/article/appnexus_connector/index.html", "title": "AppNexus Connector"}, {"objectID": "4acf63f13fa0-1", "text": "Intro\nAppNexus is a global technology company whose cloud-based software platform enables and optimizes programmatic online advertising. To learn more about the AppNexus API, visit their page (https://wiki.appnexus.com/display/documentation/APIs).\nThe AppNexus API will get you access to the reporting API for the Ad Delivery and Campaign Management environment. From here you'll be able to get reporting on your campaigns by a number of dimensions and metrics. Virtually any metric or dimension available by the AppNexus Web Console will be allowed via the API.\nYou connect to your AppNexus account in the Data Center. This topic discusses the fields and menus that are specific to the AppNexus connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is helpful for monitoring campaign ad impressions and clicks.Primary MetricsImpressionsClicksCTRCPMPrimary Company RolesData specialistsAnalystsAverage Implementation TimeCredentials - 15 minutes or less (unless AppNexus hasn't permissioned your username for API calls).Configuring the connector - 5 minutes for pre-configured reports. 60 minutes for each custom report.Most runs of the connector will run in less than an hour.Ease of Use (on a 1-to-10 scale with 1 being easiest)5\nBest Practices", "source": "../../raw_kb/article/appnexus_connector/index.html", "title": "AppNexus Connector"}, {"objectID": "4acf63f13fa0-2", "text": "Best Practices\nThe AppNexus API may limit your API calls for fields and metrics that don't \"play nicely\" together. For example, Device API calls don't work well with Network Analytics reporting.This connector is easy to use if you can find what you need in the pre-configured reports. If you must use the Custom Reporting and JSON text box, you should have a good knowledge of JSON API calls. It is suggested you use the console to ensure the metrics you are attempting to select in JSON work well together.If you intend to append the report each day, it is suggested you just pull yesterday's data and schedule the report to run daily. \u00a0\nPrerequisites\nTo connect to your AppNexus account and create a DataSet, you must have the username and password you use to log into the AppNexus Console. You'll need to ensure that your credentials are properly permissioned by AppNexus. They may make you take an API quiz in order to submit calls with your credentials.\nConnecting to Your AppNexus Account\nThis section enumerates the options in the Credentials and Details panes in the AppNexus Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AppNexus account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter the username you use to log into the AppNexus Console.PasswordEnter the password you use to log into the AppNexus Console.\nOnce you have entered valid AppNexus credentials, you can use the same account any time you go to create a new AppNexus DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.", "source": "../../raw_kb/article/appnexus_connector/index.html", "title": "AppNexus Connector"}, {"objectID": "4acf63f13fa0-3", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the AppNexus report you want to run.\u00a0 You have access to 5 preconfigured reports: \"Advertisers,\" \"Campaigns,\" \"Line Items,\" \"Members,\" and \"Pixels.\" If the fields provided in these 5 preconfigured reports are not all you need, you can use the \"Custom\" report type and provide your own JSON API call in the text box.", "source": "../../raw_kb/article/appnexus_connector/index.html", "title": "AppNexus Connector"}, {"objectID": "4acf63f13fa0-4", "text": "The following reports are available:AdvertisersRetrieves data about advertisers in the account.CampaignsRetrieves data about campaigns controlled by the account.Custom ReportReturns a custom report, which you query using JSON.Line ItemsRetrieves data about line items tracked by the account.MembersRetrieves data about members of the AppNexus account.PixelsRetrieves data about conversion pixels managed by the account.Start DaysEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0End Days\u00a0to create a range of represented days. For example, if you entered\u00a010\u00a0for\u00a0Start Days\u00a0and\u00a05\u00a0for\u00a0End Days, the report would contain data for\u00a010 days ago up until\u00a05 days ago. Use\u00a00\u00a0for today.\u00a0End DaysEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Start Days\u00a0to create a range of represented days. For example, if you entered\u00a010\u00a0for\u00a0Start Days\u00a0and\u00a05\u00a0for\u00a0End Days, the report would contain data for 10 days ago up until 5 days ago. Use\u00a00\u00a0for today.IntervalSelect the interval for which certain statistics columns will be calculated. This does not affect which items will be displayed in the report.Report JSONEnter the JSON query for your custom report. For more information about JSON, see the AppNexus documentation.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nTroubleshooting\nEnsure you have API access for the credentials you supply.", "source": "../../raw_kb/article/appnexus_connector/index.html", "title": "AppNexus Connector"}, {"objectID": "6542a395a243-0", "text": "Title\n\nAppNexus Log-Level Connector\n\nArticle Body", "source": "../../raw_kb/article/appnexus_loglevel_connector/index.html", "title": "AppNexus Log-Level Connector"}, {"objectID": "6542a395a243-1", "text": "Intro\nThe AppNexus Log-Level data service (formerly known as the Data Siphon Service) allows users to retrieve and track feeds of log-level event data that include dimensions not available in the AppNexus Console or via the API\u00a0Report Service.\u00a0 To learn more about the AppNexus Log-Level API, visit their page (https://wiki.appnexus.com/display/api/).\nYou connect to your AppNexus Log-Level account in the Data Center. This topic discusses the fields and menus that are specific to the AppNexus Log-Level connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0 a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your AppNexus Log-Level account and create a DataSet, you must have the username and password you use to log into console.appnexus.com.\u00a0\nConnecting to Your AppNexus Log-Level Account\nThis section enumerates the options in the Credentials and Details panes in the AppNexus Log-Level Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AppNexus Log-Level account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter\u00a0the username you use to log into console.appnexus.com.\u00a0PasswordEnter the password you use to log into console.appnexus.com.\u00a0\nOnce you have entered valid AppNexus Log-Level credentials, you can use the same account any time you go to create a new AppNexus Log-Level DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane", "source": "../../raw_kb/article/appnexus_loglevel_connector/index.html", "title": "AppNexus Log-Level Connector"}, {"objectID": "6542a395a243-2", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus for configuring your report.\nMenuDescriptionReportSelect the AppNexus Log-Level report you want to run.\u00a0Currently only a single report type is available.Feeds ReportReturns a feed\u00a0of log-level event data that include dimensions not available in the AppNexus Console or via the API.Siphon NameSelect the siphon you want to retrieve data for.DurationSelect the duration type for the report. If you select\u00a0Single Date and Hour, you are asked to select the specific date and hour for the log data you want to see. If you select\u00a0Since Last Successful Run, the report pulls in all new data since the report was last successfully run.DateSelect the date you want to pull log data for. This date should be within the last 10 days.HourSelect the hour of the selected date you want to pull log data for.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/appnexus_loglevel_connector/index.html", "title": "AppNexus Log-Level Connector"}, {"objectID": "96fdd91fad3e-0", "text": "Title\n\nAppstore Overview\n\nArticle Body\n\nIntro\nIn the Appstore you can view apps;\u00a0deploy apps to your Domo; and add DataSets by connecting to third-party connectors. For information about the layout of the Appstore, see\u00a0Appstore Layout.\nVideo - Appstore Overview", "source": "../../raw_kb/article/appstore_overview/index.html", "title": "Appstore Overview"}, {"objectID": "96fdd91fad3e-1", "text": "App types\nApps in Domo come in\u00a0the following\u00a0types:\nQuickStart apps. These are packs of template KPI cards, each associated with a particular third-party connector, that you can power up with your own data in a matter of minutes. You power up QuickStart apps by connecting to a connector, usually by entering credentials and/or other information.\u00a0When you install a\u00a0QuickStart app, a page for the\u00a0QuickStart app is added to your Domo. This page contains all of the cards associated with this connector, powered by your own live data. For example, if you were to add the LinkedIn\u00a0QuickStart app to your Domo, a\u00a0LinkedIn\u00a0page would appear, with cards for \"Company Followers,\" \"Likes,\" \"Clicks vs. Impressions,\" and so on, all powered up with your own LinkedIn data. For more information about KPI cards, see Understanding KPI cards.\u00a0Card Builder apps. These are similar to QuickStart apps in that pages of template KPI cards you can power with your own data are produced. The major difference is that you power them not by connecting to a third-party connector but by\u00a0selecting a DataSet\u00a0with\u00a0matching\u00a0column names.\u00a0Custom\u00a0apps. These are cards that use multiple charts and images to present data measurements\u00a0using rich visualizations.\u00a0You can create\u00a0Custom apps yourself\u00a0using\u00a0app Design Studio\u00a0(an\u00a0Adobe Illustrator-based plugin) or app Dev Studio (a tool best suited for developers with web-based experience). For information about creating and publishing Domo\u00a0apps, see\u00a0https://developers.domo.com.\nConnectors in Domo", "source": "../../raw_kb/article/appstore_overview/index.html", "title": "Appstore Overview"}, {"objectID": "96fdd91fad3e-2", "text": "Connectors in Domo\nConnectors are an extremely efficient method for accessing your data. They allow you to use your existing data service to power your apps within Domo. By simply inputting your data\u00a0provider credentials (where necessary), the connector will map the data from the provider to power your app, and there\u2019s no need to involve any people to get your apps up and running.\nPublishers\nIn an effort to meet the needs of as many different customers as possible, Domo has given the opportunity to third-party developers (called publishers) to create apps and publish them to the Appstore to be installed by anybody. This partnership allows publishers to specialize in the content they produce while simultaneously increasing value in the Appstore experience. All publisher apps are subject to the same approval process and criteria as any Domo-made app, so you can rest assured that all content in the Appstore meets Domo\u2019s standard.\nFor more information about publishing apps, see https://developer.domo.com.", "source": "../../raw_kb/article/appstore_overview/index.html", "title": "Appstore Overview"}, {"objectID": "398c56708edc-0", "text": "Title\n\nApp Annie Connector\n\nArticle Body", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "398c56708edc-1", "text": "Intro\nApp Annie is a business intelligence company and analyst firm that\u00a0produces a range of business intelligence tools and market reports for the apps and digital goods industry. Use Domo's\u00a0App Annie\u00a0connector to compile reports about accounts, products, advertising metrics, and the like. To learn more about the\u00a0App Annie\u00a0API, visit their website ( https://support.appannie.com/hc/en-u...-Analytics-API ).\nYou connect to your App Annie account in the Data Center. This topic discusses the fields and menus that are specific to the App Annie connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesUse this connector to pull data about user accounts and mobile products.Primary MetricsAccount ConnectionsProduct SalesAdvertising SalesTop AppsProduct RatingsPrimary Company RolesMobile app advertisingManagerAnalyticsAverage Implementation TimeLess than 1 hourEase of Use (on a 1-to-10 scale with 1 being easiest)3\nPrerequisites\nTo connect to your\u00a0App Annie\u00a0account and create a DataSet, you must have the email address you use to log into App Annie, as well as your App Annie password.\nConnecting to Your App Annie Account\nThis section enumerates the options in the Credentials and Details panes in the App Annie Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "398c56708edc-2", "text": "Credentials Pane\nThe Domo App Annie connector uses OAuth to connect, so there is no need to enter credentials within Domo. Click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if\u00a0you have\u00a0existing App Annie accounts in Domo) to open the App Annie OAuth screen where you can enter your App Annie credentials. Once you have entered valid credentials, you can use the same account any time you go to create a new App Annie DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "398c56708edc-3", "text": "Note:\u00a0If you are already logged into App Annie when you connect in Domo, you are authenticated automatically when you click Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of Facebook or switch accounts (using the Switch Accounts option in the OAuth screen).", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "398c56708edc-4", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "398c56708edc-5", "text": "MenuDescriptionReportSelect the App Annie report you want to run.\u00a0The following reports are available:Account Connections ListRetrieves all account connections available in\u00a0the authenticated user's account.Account Connection Products ListRetrieves the product list\u00a0for\u00a0the\u00a0selected account connection.Account Connection SalesRetrieves sales information for the selected account connection.Advertising Account Site & Campaign ListRetrieves a list\u00a0of all sites and/or campaigns for a specific ad platform account.App Site & Campaign ListRetrieves a list of all sites and campaigns linked to\u00a0the selected app.IAP ListRetrieves the In Product Purchase list\u00a0for the selected\u00a0product.Product SalesRetrieves sales data for the selected\u00a0product, such as number of downloads, refunds, etc.Shared Products ListRetrieves a list of shared products the authenticated user may have access to.User Advertising SalesReturns advertising metrics for the authenticated user.Account NameSelect the account you want to retrieve information for.BreakdownSelect the dimension the\u00a0report data should be broken down by.CurrencySelect the world currency you want to retrieve data for.ProductSelect whether to return data for a specified product or all products.Product NameSelect the product you want", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "398c56708edc-6", "text": "product or all products.Product NameSelect the product you want to retrieve information for.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of days back you want to retrieve data for. For example,\u00a00 would return today's data;\u00a01 or yesterday\u00a0would retrieve yesterday's data;\u00a07 or\u00a0today-7 would retrieve data for the past 7 days; and so on.Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "398c56708edc-7", "text": "Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "398c56708edc-8", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Number of Days Back for Historical Data on First RunEnter the number of past days to backfill on the first run of the DataSet.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/app_annie_connector/index.html", "title": "App Annie Connector"}, {"objectID": "862ddb384279-0", "text": "Title\n\nApril 2016 Release Notes\n\nArticle Body\n\nNote: Depending on the product version you are using, the documentation may include information about features that may not be available or may have changed.", "source": "../../raw_kb/article/april_2016_release_notes/index.html", "title": "April 2016 Release Notes"}, {"objectID": "862ddb384279-1", "text": "New features and enhancements\nFeatures and enhancements in this release include the following:\nAndroid App\nDomo has been hyper-focused on the modern business executive since we began. That person is not tied down to a desk\u2014they are often on the move. In February we released our new Mobile Web experience to provide a mobile optimized experience for iOS and Android users in a mobile browser. In March we released a newly rewritten iOS app. This month we are completing the mobile\u00a0triple play with the\u00a0release of the new Android app we announced at Domopalooza.\nWith this app you get the same features as the iOS app, with a design that will be familiar to Android users while also distinctly Domo. Android users will now have access to such features as filtering cards using Analyzer,\u00a0support for new publication groups, and a new Home experience. As with Home on iOS, Android's Home will give you trending content, allow you to personalize your daily hotlist of critical content, and help you discover new people and cards.\nThe new Android app\u00a0will be available in the Google Play Store.\u00a0\nUpdated menus\nIn the interest of providing a more consistent and enjoyable user experience, the main menu bar in Domo\u00a0has been consolidated and most options moved to one of two menus. The icon\u00a0that previously resembled a downward-pointing arrow has been replaced with a more intuitive \"grid\" icon, and nearly all tools have been relocated into this menu:\nIn addition, notifications, profile access, and user settings\u00a0have been moved into a new menu accessible by clicking the user profile picture:\nFor more information, see\u00a0Domo Application Layout.\nDataSet Import Manual Cancellation", "source": "../../raw_kb/article/april_2016_release_notes/index.html", "title": "April 2016 Release Notes"}, {"objectID": "862ddb384279-2", "text": "For more information, see\u00a0Domo Application Layout.\nDataSet Import Manual Cancellation\nDomo now provides you with\u00a0additional control over your DataSets by allowing you to stop long-running imports instead of waiting for the job to complete. If you have a DataSet that is taking too long to update and you need to stop it, simply select Stop Import\u00a0in\u00a0the DataSet options menu. This stops the import from running and allows you to make adjustments to the configuration before re-running the import. The DataSet history will be updated to indicate that the import has been cancelled. You can re-run the job manually or let the import run on the existing schedule.", "source": "../../raw_kb/article/april_2016_release_notes/index.html", "title": "April 2016 Release Notes"}, {"objectID": "862ddb384279-3", "text": "DataFlow\u00a0updates\nDataFlows has been updated with a number of new enhancements designed to improve the user experience and increase consistency.\nOptions menu on Details page. This menu provides the same options as the Options menu in the DataSet Details page, such as\u00a0Edit,\u00a0Run,\u00a0Duplicate, etc. This allows for a more consistent and pleasing user\u00a0experience.\u00a0  Ability to check permissions on run. You can now control who has access to your data through sharing of cards powered by DataFlow inputs.  Ability to reassign owner.\u00a0Admin users can now change the owner of a DataFlow\u00a0from the DataFlow Details page.   \u00a0Magic ETL\u00a0interface improvements.\u00a0All\u00a0Magic ETL\u00a0actions now have their own unique icons, and the icons in the palette match those in the sidebar.\u00a0These improvements\u00a0will help\u00a0alleviate confusion experienced by users new to\u00a0the Magic ETL interface. Also, you can now do multi-select for duplicating and deleting.\u00a0  \nFull-screen cards\nWhile in the Details view for a card, you can now access a full-screen\u00a0mode. This full-screen viewing mode\u00a0provides all of the\u00a0same functionality found in the standard\u00a0Details view, including drilldown, alert functionality, filtering, access information, and so on.\u00a0The full-screen mode persists until you\u00a0leave the\u00a0Details view, so if you navigate to another card using a link,\u00a0that card will also appear in the full-screen mode. This mode is available for all types of cards.\nFor more information, see\u00a0Card Details View Layout.\nDocument card clicker capability\nYou can now navigate through document cards in preview mode using a clicker, just as you could in a Microsoft PowerPoint presentation.\nFrozen table headers", "source": "../../raw_kb/article/april_2016_release_notes/index.html", "title": "April 2016 Release Notes"}, {"objectID": "862ddb384279-4", "text": "Frozen table headers\nUsers of tables can now scroll down a table to view rows while still seeing the header rows. This improves the user experience by allowing users to preserve their context while navigating tables.\u00a0\nFor more information, see\u00a0Card Details View Layout.\nFuture date range options on cards\nThe list of available date ranges for cards now includes several \"future\" date ranges, such as \"Tomorrow,\" \"Next Quarter,\" etc. This data is\u00a0not forecast/projection data; it merely allows you to include future dates already included in your DataSet.\u00a0\u00a0\nFor more information, see\u00a0Filtering Dates.\nGetting Help\nYou can view the latest release notes information in the Help Center, which you can access from Domo by clicking\u00a0\u00a0> Help Center.\nIf you have questions about Domo,\nsearch for a topic in the Help Centertrain in Domo University at http://www.domo.com/universityget answers in the Domo Community at dojo.domo.comcontact Technical Support by using @DomoSupport in Buzz, by email at support@domo.com\u00a0or by phone at 801-805-9505 (Monday through Friday, 7 am to 6 pm MST)reach out to your Domo Customer Success Manager or Technical Consultant\nIf you have feedback, please send it from within Domo (\u00a0> Feedback). Or send an email to product.feedback@domo.com.\nFor more information about getting help, see Getting Help.", "source": "../../raw_kb/article/april_2016_release_notes/index.html", "title": "April 2016 Release Notes"}, {"objectID": "f5b01dbbce80-0", "text": "Title\n\nApril 2017 Release Notes\n\nArticle Body\n\nNew features and enhancements\nFeatures and enhancements in this release include the following:\nBuzz Updates\nBased on your feedback, Buzz has been updated to include a number of improvements to facilitate your communication. These improvements include...\nAbility to edit messages. Edit mistakes out of your messages after you have posted them.  \u00a0 Mention filter that sorts based on category. @-mentioning in Buzz now filters based on category to allow you to more easily navigate to that card or group you are looking for.  Display who is online. Just like in your old DomoBuzz, you can see who in your organization is logged into Domo. For more information, see\u00a0Seeing Online Users in Domo.  Ability to collapse attachments. Hide attachments and GIFs that may be distracting you.  \nFor more information, see Buzz Layout.\nHybrid SSO\nHybrid SSO allows users with Single Sign-On enabled to specify which of their users can log in directly to Domo. \nPreviously, hybrid SSO was available only to Administrators using a feature switch. For SSO customers trying to grant Domo access to contractors or non-employees, this often created a dilemma because such users were not usually given accounts in the company\u2019s email or directory system. Now, however, Admins can go into the Single Sign-On tab in the Admin Settings and specify a set of users who are able to sign into Domo directly, just as they would without SSO enabled. Users can then toggle login modes (Direct or Single Sign-on) from the Domo login screen.", "source": "../../raw_kb/article/april_2017_release_notes/index.html", "title": "April 2017 Release Notes"}, {"objectID": "f5b01dbbce80-1", "text": "For more information, see Adding Users to Direct Sign-On.\nApp Deploy Improvements\nFor this release we are pushing the following improvements to our App deploy process:\nSupport for multi-connector content.\u00a0 Apps can now be powered by 1:n connectors, providing a simple-to-understand interface.Multi-user deploy capability. With more complicated Apps, it\u2019s unlikely a single person has all the credentials needed to be successful.\u00a0 We\u2019ve leveraged Buzz and added the ability to collaborate in the power-up process. You can assign specific portions to the right person.\u00a0 This effectively removes the need for sticky notes and other uncomfortable requests for credentials.\nUpdated UI for Google Analytics Connector\nThe user interface for the Google Analytics connector now offers a more intuitive, step-by-step workflow. Instead of providing a single pane for selecting parameters and report types, the new interface divides up options between five panes\u2014Select Views, Select Reports, Select Segments, Select Duration, and Test My Configuration.\nThe following screenshot shows the new view for selecting Google Analytics reports in Domo:", "source": "../../raw_kb/article/april_2017_release_notes/index.html", "title": "April 2017 Release Notes"}, {"objectID": "f5b01dbbce80-2", "text": "For more information including FAQs, see Google Analytics Connector.\nPage and Card Analyzer Filter Search Box\nWhen you add a filter to a page or card, you can now more easily find the columns you're looking for using a new search box.\nGetting Help\nYou can view the latest release notes information in the Help Center, which you can access from Domo by clicking\u00a0\u00a0> Help Center.\nIf you have questions about Domo,\nsearch for a topic in the Help Centertrain in Domo University at http://www.domo.com/universityget answers in the Domo Community at dojo.domo.comcontact Technical Support by entering a help ticket in the Domo Support Portal, by sending a Buzz message to \\support, or by emailing support@domo.com.reach out to your Domo Customer Success Manager or Technical Consultant\nIf you have feedback, please send it from within Domo (\u00a0> Feedback). Or send an email to product.feedback@domo.com.\nFor more information about getting help, see Getting Help.", "source": "../../raw_kb/article/april_2017_release_notes/index.html", "title": "April 2017 Release Notes"}, {"objectID": "97f14a2d3de5-0", "text": "Title\n\nApril 2018 Release Notes\n\nArticle Body", "source": "../../raw_kb/article/april_2018_release_notes/index.html", "title": "April 2018 Release Notes"}, {"objectID": "97f14a2d3de5-1", "text": "New features and enhancements\u00a0\nFeatures and enhancements in this release include the following:\nNew Data Center Enhancements\nDomo's updated Data Center\u00a0makes it easy to find, organize, manage, and clean up all your data, giving users\u2014especially Major Domos\u2014clarity and control over their data.\nFind the DataSets you're looking for so you can understand and take action on them. Use search facets such as date, number of cards, and owner. Use our new sort-by menu to sort by Name, Number of Cards, Update Date, and Status. And by choosing to sort either Ascending or Descending, you can quickly find that DataSet you need, or maybe all those old, unused DataSets you want to delete.  \u00a0Use our new tagging functionality to really up your organizational game. Once you have DataSets tagged to get a better understanding of your DataSets, you can find them quickly and easily. Use tags to show which DataSets are works-in-progress and which are marked to be deleted at a future date.\u00a0  Take bulk actions on DataSets. When you hover over an item, select the checkbox that appears on the left. Once you have them selected, you can delete, run, tag, or change owner on these DataSets in bulk. Try holding down the Shift key to select many DataSets at once.  \nWelcome to the new Data Center. Managing data has never been this much fun.For more information, see\u00a0Data Center Layout.\nSSO for Everyone", "source": "../../raw_kb/article/april_2018_release_notes/index.html", "title": "April 2018 Release Notes"}, {"objectID": "97f14a2d3de5-2", "text": "SSO for Everyone\nDomo now includes tools that enable customers to quickly set up Single Sign-on (SSO) in their instance. SSO is widely considered the best way to access enterprise applications\u00a0because it simplifies login, while improving management and security. But because of the complexity of SAML 2.0, managing\u00a0organizational processes, and configuring Domo itself, setup can take a very long time. Now, with Domo's new SSO setup tools, you\u00a0can confidently roll out SSO in a fraction of the\u00a0time you could before. The two new tools are...\nMetadata upload. Using Metadata Upload, you can enter the URL for your identity provider. Domo then automatically retrieves its configuration information and digital certificate. Domo can provide information back to the identity provider as well. This is the easiest and most reliable way to configure your SSO. Be aware that\u00a0not all identity providers offer this service.Wizard. A new wizard interface guides you through the difficult jargon and details of SSO\u00a0configuration. The wizard offers context-based instructions for a number of leading identity provider services, including Okta, Ping, Microsoft Azure, Salesforce, and more.\u00a0  \nFor more information about SSO, see\u00a0Understanding and Configuring Domo Single Sign-On.\u00a0\nDate Annotations\nYou can add now add\u00a0annotations to date items in any chart to provide the proper context for activity that took place on\u00a0those dates. For example, you could add date annotations to a chart to help you remember why you had that great day of sales or how you were able to drive so much traffic over a particular\u00a0weekend. These date annotations stay with their associated dates so your company knowledge stays with the data.\u00a0\nEach annotation appears in an\u00a0Annotations\u00a0sidebar, with a colored flag that allows users to quickly match the annotation with its associated data point.\nThis functionality is available\u00a0only\u00a0for charts containing date/time data.", "source": "../../raw_kb/article/april_2018_release_notes/index.html", "title": "April 2018 Release Notes"}, {"objectID": "97f14a2d3de5-3", "text": "For more information, see\u00a0Adding Date Annotations.\nUpdated Licensing Manager\nThe\u00a0Licenses\u00a0page in the Admin Settings now gives you a quick view of the licenses you've already paid for as well as the number of \"courtesy licenses\"\u2014the number of users in your company using Domo in excess of your paid license count. You can now pay for these courtesy licenses right from Domo by clicking the\u00a0Pay for Licenses\u00a0button. This opens the self-service payments flow, where you can pay using a credit card or request an invoice from Domo.\nFrom the\u00a0Licenses\u00a0page, you can also view all of the Social users in your instance. If you want to upgrade these users, you can do so right from this view. To help\u00a0inform your decisions, Domo provides information about when these users last logged in, who they report to, and their department and job titles. Because social users can request upgrades to full licenses of Domo, you can also view the list of social users who have requested licenses. You can easily approve or deny any of these requests and specify the Domo role each person receives. You can also email requesters about your decisions.\nThese new license management features help you get all your people collaborating in Domo\u2014for the good of the company.", "source": "../../raw_kb/article/april_2018_release_notes/index.html", "title": "April 2018 Release Notes"}, {"objectID": "97f14a2d3de5-4", "text": "For more information about licensing, see\u00a0Adding User Licenses in Domo.\u00a0\u00a0\nMobile Help Center\nTo provide better self-service for mobile users of Domo, we have added a Help Center within our iOS and Android applications. From the Help Center you can access any of our help resources\u2014Domo University, Dojo, Knowledge Base, Developer Portal, or Domo Support\u2014without ever having to leave your mobile applications. To access the Help Center, simply swipe right from the home screen and tap on\u00a0Help Center.\u00a0\u00a0\nMobile Alert Details\nTo provide you with\u00a0more context and information when sharing and viewing alerts, we have added a details screen for alerts on our iOS and Android applications. When viewing any alert on your mobile app you can tap into the alert to see the full details for that alert. You will be able to see who created the alert, the description, trigger history and alert criteria set for the alert.\n\nDataFlows Explain Plan\nSQL explain plans are now available for both\u00a0MySQL and Redshift DataFlows. Get a better understanding about how your SQL queries run on the server, allowing you to optimize your queries for the best possible performance. To show the explain plan for SQL in a transform or output DataSet, you simply open up that transform or output DataSet and select\u00a0Explain SQL\u00a0in the menu in the bottom right corner.\u00a0\n\nUse of explain plans is recommended for advanced SQL users only, as the generated steps are highly technical and complex.\u00a0\nFor more information about SQL DataFlows, see\u00a0Creating an SQL DataFlow.\nSummary Number Alert Suggestions\nAlerts help you unlock the full power of Domo. Now, alert\u00a0suggestions in Analyzer make creating those alerts even easier. When you save a card with a summary number in Analyzer, you are prompted to set an alert on that summary number.\u00a0This way, you are always informed on your business and updates in your data.", "source": "../../raw_kb/article/april_2018_release_notes/index.html", "title": "April 2018 Release Notes"}, {"objectID": "97f14a2d3de5-5", "text": "Getting Help\u00a0\nYou can view the latest release notes information in the Help Center, which you can access from Domo by clicking\u00a0\u00a0> Help Center.\nIf you have questions about Domo,\nsearch for a topic in the Help Centertrain in Domo University at\u00a0http://university.domo.comsearch for training apps in the Appstoreget answers in the Domo Community at\u00a0https://dojo.domo.comcontact Technical Support by entering a help ticket in the Domo Support Portal or\u00a0by sending a Buzz message to\u00a0\\support.reach out to your Domo Customer Success Manager or Technical Consultant\nIf you have feedback, please send it from within Domo (\u00a0> Feedback). Or send an email to\u00a0product.feedback@domo.com.\nFor more information about getting help, see\u00a0Getting Help.", "source": "../../raw_kb/article/april_2018_release_notes/index.html", "title": "April 2018 Release Notes"}, {"objectID": "b5c31b9cf63f-0", "text": "Title\n\nApril 2019 Release Notes\n\nArticle Body\n\nNew features and enhancements\nFeatures and enhancements in this release include the following:\nDomo Stories enhancements\nDomo Stories Dashboards are now enhanced with Drill Paths, summary metrics, and background colors for even more control over the presentation of your data.\nDrill in Place\nDrill Paths\u00a0are now available from Pages, so you don't\u00a0have to leave your Dashboard\u00a0to drill into a Card. Deep-dive on a single Card while maintaining the context of the other Cards on a Page, or apply Dashboard filters to change the view as you drill.\nTo enable Drill Paths on Dashboards, select\u00a0Edit Content > Change interaction\u00a0for the Card, then choose the\u00a0Drill in place\u00a0radio button.\u00a0You must be in\u00a0Edit Dashboard\u00a0mode for these options to be available.\u00a0\n\nFor more information, see\u00a0Creating Domo Stories.\nDashboard background colors\nCustomize your Dashboards with background colors to match any branding or theme. Choose colors from a palette or use hexadecimal values to personalize.", "source": "../../raw_kb/article/april_2019_release_notes/index.html", "title": "April 2019 Release Notes"}, {"objectID": "b5c31b9cf63f-1", "text": "For more information, see\u00a0Creating Domo Stories.\nSummary metric view\nThe new summary metric view allows you to toggle charts on and off using the Display settings option in Domo Stories. This creates a formatted Summary Number so you can leverage existing content in the context that works best for your Domo Story.\nFor more information, see\u00a0Creating Domo Stories.\nImproved DataSet picker\nUse the new DataSet picker to search for DataSets with any filter criteria, like tags, certification state, row count, Card count, last execution time, execution status, and more. You can also sort your results for easier discovery.\n\u00a0\nFor more information, see\u00a0Powering a KPI Card with Data.\nEnhanced Dashboard Filters\u00a0\nEnhanced Dashboard Filters bring more clarity and context to Page filtering. Instead of choosing a DataSet to filter on, you can choose your filter columns from a list of all available columns used in Cards on the Page. Summary text on each Page appears whenever filters are applied, so you quickly see which filters are applied. Plus, Beast Mode calculations shared on a DataSet will now appear in the Dashboard Filter list.\u00a0\n\u00a0\nFor more information, see\u00a0Applying Page-Level Filters.\nEnhanced Analyzer Card Filters\u00a0\nConfigure filters for Cards faster than ever before with enhanced Analyzer filters. With the new filtering improvements, you can:\nFilter on values containing specific words\u00a0without having to create Beast Mode LIKE functions.Change the date type for a column without having to open a DataFlow.Choose specific dates and numbers to filter on rather than indicating a range.\u00a0\t\u00a0\n\u00a0\nFor more information, see\u00a0Adding Filters to Your Chart.\nNew and updated Connectors\nConnect to all your data with our collection of over 1,000 connectors. We\u2019ve added several new connectors, including new bi-directional connectors, and we\u2019ve made updates to some existing ones.\nNew Connectors", "source": "../../raw_kb/article/april_2019_release_notes/index.html", "title": "April 2019 Release Notes"}, {"objectID": "b5c31b9cf63f-2", "text": "New Connectors\nParticleInstagram BusinessSalesforce with upsertUSGS WildfireSnapchat AdsSEVENROOMS\nDocumentation coming soon!\nWriteback\u00a0(Bi-directional) Connectors\nNew Writeback Connectors allow you to push Domo DataSets to third-party applications instead of pulling data from those applications into Domo. Available applications include Dropbox, Google Cloud Storage, Amazon S3, and many more. To have Writeback Connectors implemented for your Domo instance, please reach out to your account representative.\nFor a full list of available Writeback\u00a0Connectors and comprehensive documentation for each, please visit\u00a0Writeback Connectors.\nUpdated Connectors\nCallRailAnaplanLinkedIn\nGetting Help\u00a0\nYou can view the latest release notes information in the Help Center, which you can access from Domo by clicking\u00a0by clicking \u00a0in the top navigation bar .\nIf you have questions about Domo,\nsearch for a topic in the Help Centertrain in Domo University at\u00a0http://university.domo.comsearch for training apps in the Appstoreget answers in the Domo Community at\u00a0https://dojo.domo.comcontact Technical Support by entering a help ticket in the Domo Support Portal or\u00a0by sending a Buzz message to\u00a0/support.reach out to your Domo Customer Success Manager or Technical Consultant\nIf you have feedback, please send it from within Domo (\u00a0> Feedback). Or send an email to\u00a0product.feedback@domo.com.\nFor more information about getting help, see\u00a0Getting Help.", "source": "../../raw_kb/article/april_2019_release_notes/index.html", "title": "April 2019 Release Notes"}, {"objectID": "951ea0e9f04d-0", "text": "TitleArchiving Historical Data Using a DataFlowArticle BodyIntro\nLet's say you put your\u00a0information into a year-to-date DataSet that\u2019s one month behind, starting by loading January\u2019s data in February. Because this is a year-to-date DataSet, once February rolls around the next year, the data from the previous year disappears from the DataSet to make way for the new current year\u2019s data. What if you wanted to archive the data into another DataSet once\u00a0December\u2019s data is loaded in the year-to-date file? As it turns out, there are a couple of ways to accomplish that. They can be done both in MySQL and Magic ETL.\nThis process applies to any DataSet containing any year-to-date data. Once it hits a year mark, you can archive it for historical purposes.\nMethod #1\nMethod #1 Using a MySQL DataFlow\nThis can be done using a stored procedure. This is the most flexible method and can be adapted to account for any extra logic that might need to be applied.\nThe code in this stored procedure assumes a few things:\nThe current year-to-date data will be appended to a historical archive via a recursive DataFlow.This recursive DataFlow has already been created.The DataFlow has been set up to automatically run when the year-to-date DataSet updates.\nIn the below example, the data will be coming from a DataSet called ytd_premium_data, and the historical archive will be called move_to_historical_method_1.  \nBelow is the code for the stored procedure:\nCREATE PROCEDURE archive()\nBEGIN\n-- Get the max date from the premium data\n\u00a0\u00a0\u00a0 SELECT @maxDataDate := MAX(`Date`) \n\u00a0\u00a0\u00a0 FROM\u00a0\u00a0 ytd_premium_data;\n-- Get the max date from the archived data\n\u00a0\u00a0\u00a0 SELECT @maxArchiveDate := MAX(`Date`) \n\u00a0\u00a0\u00a0 FROM\u00a0\u00a0 move_to_historical_method_1;", "source": "../../raw_kb/article/archiving_historical_data_using_a_dataflow/index.html", "title": "Archiving Historical Data Using a DataFlow"}, {"objectID": "951ea0e9f04d-1", "text": "FROM\u00a0\u00a0 move_to_historical_method_1;\n-- We will use these two dates later to determine whether or not the\n-- current year-to-date data has already been added to the historical\n-- archive\n-- The following is a shortcut to dynamically create a table structure \n-- that's identical to another table, but leave it with no data in it. \n-- We\u2019re going to use this trick to create a table with the same\n-- structure as the year-to-date-data\n-- Create a table by selecting only one row from the premium data\n\u00a0\u00a0\u00a0 CREATE TABLE archive_data \n\u00a0\u00a0\u00a0 SELECT\u00a0\u00a0\u00a0\u00a0 * \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 FROM\u00a0\u00a0\u00a0 ytd_premium_data LIMIT 1;\n-- Get rid of all rows from the newly created table\n\u00a0\u00a0\u00a0 TRUNCATE TABLE archive_data;\n-- If the conditions are met to archive the year-to-date data, this\n-- new table will be populated with the year-to-date data and appended\n-- to the historical DataSet. If the conditions are not met, the table\n-- will remain empty, and an empty table will be appended to the\n-- historical DataSet.\n-- The year of data to be archived is last year's data. Remember that\n-- the year-to-date data is a month behind, so December\u2019s data won\u2019t\n-- be loaded until January of the next year.\n\u00a0\u00a0\u00a0 IF YEAR(CURRENT_DATE()) - 1 = YEAR(@maxDataDate) \n-- It's January, which means we should have December's data\n\u00a0\u00a0\u00a0 AND MONTH(CURRENT_DATE()) = 1\u00a0\u00a0\u00a0 \n-- The data to be archived contains December's data\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 AND MONTH(@maxDataDate) = 12 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n-- The data from the year-to-date has not already been added to the\n-- archive\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 AND @maxDataDate != @maxArchiveDate\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n-- If every condition checks out, then insert the year-to-date data to", "source": "../../raw_kb/article/archiving_historical_data_using_a_dataflow/index.html", "title": "Archiving Historical Data Using a DataFlow"}, {"objectID": "951ea0e9f04d-2", "text": "-- If every condition checks out, then insert the year-to-date data to\n-- be archived into the archive_data table for unioning to the \n-- historical data. Because of the date checks in the previous step,\n-- the following code will only ever be executed once.\n\u00a0\u00a0\u00a0\u00a0 THEN INSERT INTO archive_data\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SELECT\u00a0\u00a0\u00a0\u00a0 * \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0 FROM ytd_premium_data;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 END IF;\nEND;\nTo call\u00a0the stored procedure...\nCALL archive();\nAppending year-to-date data (if it\u2019s ready to archive) to historical archive data\u00a0(step name is DataFlow_output)...\n-- Union whatever's in archive_data to thie historical table. If the coniditons for moving the data weren't met, archive_data will be empty.\n\u00a0SELECT\u00a0\u00a0\u00a0\u00a0 *\nFROM move_to_historical_method_1\nUNION ALL\nSELECT\u00a0\u00a0\u00a0\u00a0 *\nFROM archive_data\nThis compares what\u2019s already in the archived DataSet to what\u2019s being put in. The MAX dates for the data are matched. Then, we create a table that\u2019s going to hold the data to be archived. We can use a shortcut of SELECT * from the table with a limit of 1. \u00a0Then you truncate the table. This is a quick way to create the table schema without having to go through the process manually. That way, we don\u2019t have to replicate the table schema; it does it automatically. \nRun a couple of checks:\nIs it the right year?Is it January? (If so, December's DataSet\u00a0will be loaded.)Is the month to be archived December?\u00a0\nThen the last piece of code checks to make sure that the data to be archived hasn\u2019t already been archived. Once it\u2019s archived you don\u2019t want it to archive it again.", "source": "../../raw_kb/article/archiving_historical_data_using_a_dataflow/index.html", "title": "Archiving Historical Data Using a DataFlow"}, {"objectID": "951ea0e9f04d-3", "text": "If it passes, it will be put in the archive data table that was created above. Then, later, all you need to do is just a simple union all with the rest of the archived data. This puts everything together. The process only archives it the one time.\nMethod #1 Using Magic\nThis method is not as straightforward as using MySQL, but the DataFlow uses the same logic. \nTo get the MAX date in Magic, you have to do a Group By\u00a0transform on a column that is identical in every row. So the first step is to add a column with a constant value using the Add Constant transform. In this case, I\u2019ve given it a numerical value of 1 and named it \"One.\" This is done twice\u2014once for the year-to-date premium data and once for the historical archive data. Insert a Group By transform and group by the column we just created (\"One\"), create a new column for each grouping, and select Maximum for the \"Date\" column. This step will also be done twice\u2014once for the year-to-date premium data and once for the historical archive data. But you need to give different names to each new aggregated column. Use a Join transform to join the two DataSets in Step 2 together on column \"One.\"   Your results will contain the maximum date in the year-to-date data, the maximum date in the historical archive data, and two columns containing \"One.\" Use an Add Constant to add today\u2019s date to the other dates in the DataSet. Insert a Date Operations transform and create the columns\u00a0\"Current Month\" and \"Current Year\" from the column \"Today\" and \"Data Month\" and \"Data Year\" from the column \"MaxDataDate.\" \nYour data will now look something like this: \nYou now have all the dates and information you need to determine whether or not to append year-to-date data to the historical archive data.", "source": "../../raw_kb/article/archiving_historical_data_using_a_dataflow/index.html", "title": "Archiving Historical Data Using a DataFlow"}, {"objectID": "951ea0e9f04d-4", "text": "To ensure every row in the year-to-date DataSet contains the data just created, insert a Join Data transform to an inner join on both DataSets and join on the column \"One.\" Using the Calculator transform, subtract 1 from \"Current Year\" to determine \"Prior Year.\" Add a Filter Rows transform to apply the logic to determine whether or not the year-to-date DataSet contains data from December and whether or not it has already been added to the historical data archive. \nIf this step passes all the checks, it will contain all the data from the year-to-date DataSet. If not, it will contain nothing. \nUse a Select Columns transform to get rid of all the extra columns we created to make the schema match what\u2019s in the historical data archive. Add an Append Rows transform to append the year-to-date DataSet (if applicable) to the historical archive DataSet. Be sure to select Include all columns from the dropdown. If you\u2019ve done everything correctly, a\u00a0\u201cNo changes\u201d message will appear beside each input DataSet. Under Settings, check the box to make sure the transform will run whenever the year-to-date DataSet gets updated. \nMethod #2\nMethod #2 Using MySQL\u00a0\nIf there are no complex requirements for determining whether or not to archive the year-to-date data, and you know that the year-to-date DataSet will be complete by a certain date, then you can create a DataFlow that does nothing but archive the data, and then you can use a trick to schedule to run it on a certain date every year.\nFor our purposes, let\u2019s suppose that we know the year-to-date DataSet will be complete and final by January 20 every year. The first thing we will need to do is create a CSV DataSet and schedule it to update every year on January 20.", "source": "../../raw_kb/article/archiving_historical_data_using_a_dataflow/index.html", "title": "Archiving Historical Data Using a DataFlow"}, {"objectID": "951ea0e9f04d-5", "text": "Navigate to https://yourinstancename.domo.com/connectors/com.domo.connector.csv.easy.Put anything that makes sense into the box. Here we create a column called \"Date\" with a value of 1800-01-01. Click Next to\u00a0bring\u00a0up the Scheduling section. Click the Advanced Scheduling tab. Check the appropriate Month(s) and Days of Month. Click the Time tab and choose a time for the job to run. Click Next.Give the CSV DataSet a meaningful name and click Save.\nNow that we have a DataSet that automatically updatex every January 20, we just need a MySQL DataFlow that appends the year-to-date data to the archive DataSet. We will add this CSV DataSet to the DataSets in the DataFlow, and change the settings so that the DataFlow will run once that DataSet gets updated. Because the DataSet only gets update every January 20, then this has the effect of creating a scheduled DataFlow. \nHere is the SQL used in the DataFlow Output transform: \u00a0\n-- This DataFlow won't automatically trigger until the scheduled\n-- update file gets updated. When that happens, it will append last \n-- year's data to historical data.\nSELECT\u00a0\u00a0\u00a0\u00a0 * \nFROM move_to_historical_method_2\nUNION ALL\nSELECT\u00a0\u00a0\u00a0\u00a0 *\nFROM ytd_premium_data \u00a0\nFinally, save, but do not run the DataFlow! Otherwise\u00a0it immediately archives the year-to-date data.", "source": "../../raw_kb/article/archiving_historical_data_using_a_dataflow/index.html", "title": "Archiving Historical Data Using a DataFlow"}, {"objectID": "951ea0e9f04d-6", "text": "Method #2 Using\u00a0Magic ETL\n\nThe automatically scheduled Magic ETL simply appends year-to-date data to historical archive data. To add the automatic scheduling, you have to add the Auto Update DataSet and check the setting to run the transform whenever it updates. Unlike with a MySQL DataFlow, with Magic ETL, you can\u2019t just add an input DataSet and not incorporate it into a transform because Magic will error out if you try. So you have do a couple of things to incorporate it into the DataFlow, but make sure it doesn\u2019t actually do anything to the data. \nAdd the \"Auto Update\" DataSet as an Input Dataset.Insert a Filter transform. Since one of the columns in this DataSet is a date, set the filter to select only the records where the \"Date\" IS NULL. This will result in no rows of data, which is good\u00a0because we will later append it to the rest of the data, and because there are no rows, there will be zero effect on anything.\u00a0   Because the CSV file adds two columns when it updates,\u00a0\"_BATCH_ID_\" and \"_BATCH_LAST_RUN_,\" we need to eliminate them from the data.Insert a Select Columns transform and select only \"Date.\" Insert an Append transform in which you select Include All Columns\u00a0in the first dropdown. Save, but do not run the DataFlow! If you run this DataFlow, it will append whatever\u2019s currently in the year-to-date DataSet to the archived data DataSet.", "source": "../../raw_kb/article/archiving_historical_data_using_a_dataflow/index.html", "title": "Archiving Historical Data Using a DataFlow"}, {"objectID": "d847fe528c2a-0", "text": "Title\n\nArchiving Project Tasks and Lists\n\nArticle Body", "source": "../../raw_kb/article/archiving_project_tasks_and_lists/index.html", "title": "Archiving Project Tasks and Lists"}, {"objectID": "d847fe528c2a-1", "text": "In the Details page for a project, you can archive a task, an entire list, or all of the tasks from a list (in which case the list itself remains in your Details page but the tasks are archived). Archived tasks and lists can be \"unarchived,\" or restored to their original locations. Archiving provides a way of \"cleaning up\" your project page by removing completed or deferred tasks or unnecessary lists while making it possible to view or retrieve those tasks or lists whenever you need to.\nYou can only archive tasks and lists from projects you are assigned to, unless you have an \"Admin\" default security role or a custom role with \"Manage All Projects and Tasks\" enabled. For more information about default security roles, see\u00a0Managing Custom Roles.\nTo archive a task,\nOpen the\u00a0Projects & Tasks\u00a0page by selecting\u00a0Projects and Tasks\u00a0in the\u00a0More menu at the top of the screen.Click the name of the project with tasks you want to archive.Click the task you want to archive. The Edit dialog for this task appears.Click\u00a0Archive.\nThe task is sent to the archives.\nTo archive a list or all the tasks from a list,\nOpen the\u00a0Projects & Tasks\u00a0page by selecting\u00a0Projects and Tasks\u00a0in the\u00a0More\u200b\u200b\u200b\u200b\u200b\u200b\u200b menu at the top of the screen.Click the name of the project with tasks or lists you want to archive.Mouse over the top right corner of the list then mouse over the arrow that appears.Do one of the following:To archive the entire list, click\u00a0Archive List.To archive the tasks from a list but not the list itself, click\u00a0Archive All Tasks in List.\nThe list or tasks is/are sent to the archives.\nTo unarchive individual tasks or an entire list,", "source": "../../raw_kb/article/archiving_project_tasks_and_lists/index.html", "title": "Archiving Project Tasks and Lists"}, {"objectID": "d847fe528c2a-2", "text": "To unarchive individual tasks or an entire list,\nOpen the\u00a0Projects & Tasks\u00a0page by selecting\u00a0Projects and Tasks\u00a0in the\u00a0More\u200b\u200b\u200b\u200b\u200b\u200b\u200b menu at the top of the screen.Click the name of the project with tasks or lists you want to unarchive.Mouse over the arrow to the right of the project name then select\u00a0View Archives. The archived lists and tasks for this project appear.Do one of the following:To unarchive a task, click the\u00a0Unarchive\u00a0link on the task card.To unarchive a list, click the\u00a0Unarchive\u00a0button at the bottom of the list.\nThe following screenshot points out the controls for unarchiving a task and a list.", "source": "../../raw_kb/article/archiving_project_tasks_and_lists/index.html", "title": "Archiving Project Tasks and Lists"}, {"objectID": "44c1ecc34b58-0", "text": "Title\n\nArcSight Connector\n\nArticle Body", "source": "../../raw_kb/article/arcsight_connector/index.html", "title": "ArcSight Connector"}, {"objectID": "44c1ecc34b58-1", "text": "Intro\nHP ArcSight is a cyber security company that provides big data security analytics and intelligence software for security information and event management (SIEM) and log management solutions. You can use Domo\u2019s\u00a0ArcSight connector to\u00a0pull Report Service and Query Viewer service reports.\u00a0To learn more about ArcSight, visit their page (https://h41382.www4.hpe.com/gfs-shared/downloads-273.pdf).  \nYou connect to your ArcSight account in the Data Center. This topic discusses the fields and menus that are specific to the ArcSight connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your ArcSight account and create a DataSet, you must have the following:\nThe username and password for your\u00a0ArcSight accountThe hostname and port number for your ArcSight serverThe report ID if you are retrieving a Report Service report, or the query ID if you are retrieving a Query Viewer Service report\nConnecting to Your ArcSight Account\nThis section enumerates the options in the Credentials and Details panes in the ArcSight Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your ArcSight account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionUsernameEnter the username you use to log into your ArcSight account.PasswordEnter the password you use to log into your ArcSight account.Host NameEnter the hostname of the server where ArcSight is located.Port NumberEnter the port number of the server where ArcSight is located.", "source": "../../raw_kb/article/arcsight_connector/index.html", "title": "ArcSight Connector"}, {"objectID": "44c1ecc34b58-2", "text": "Once you have entered valid ArcSight credentials, you can use the same account any time you go to create a new ArcSight DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the ArcSight report you want to run.\u00a0The following reports are available:Report ServiceReturns the report associated with the specified Report ID.Query Viewer ServiceRuns the query indicated in the Query ID field.Report IDEnter the ID of the Report Service report you want to retrieve.Query IDEnter the ID of the Query Viewer Service query you want to run.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/arcsight_connector/index.html", "title": "ArcSight Connector"}, {"objectID": "805bb9cb4c91-0", "text": "Title\n\nArea Overlay Chart\n\nArticle Body\n\nIntro\nIn area overlay charts (referred to simply as \"area\" charts in Domo), all series begin from the same axis and are overlaid on top of each other. In their default state these charts are typically not very useful because data for some series is usually hidden. However, you can distinguish an individual series by mousing over that series in the legend or in the chart itself. Because each series begins from the same baseline, this type of area chart is useful for quickly discerning the value for individual series.\nAn area overlay chart is similar to a stacked area chart; however, in a stacked area chart, all segments are shown at once, either side by side or on top of each other, whereas in an area overlay chart, segments overlap.\u00a0\nSubtypes\nStacked area charts have a number of subtypes, including all of the following:\nVertical curved areaVertical step areaHorizontal areaHorizontal curved areaHorizontal step area\nFor explanations and screenshots for these subtypes, see Available Chart Types.\u00a0\nPowering area overlay charts\nArea overlay charts require three columns or rows of data from your DataSet\u2014one for categories (generally dates), one for series in those categories, and one for values. For information about value, category, and series data, see Understanding Chart Data.\nIn the Analyzer,\u00a0you choose the columns containing the data for your area overlay chart. For more information about choosing data columns, see\u00a0Applying DataSet Columns to Your Chart.\nFor more information about formatting charts in the Analyzer, see\u00a0Visualization Card Building Part 2: The Analyzer.\nThe following graphic shows you how the data from a typical column-based spreadsheet is converted into a vertical area overlay chart:\n\nThe following graphic shows you how the data from a typical column-based spreadsheet is converted into a horizontal area overlay chart:", "source": "../../raw_kb/article/area_overlay_chart/index.html", "title": "Area Overlay Chart"}, {"objectID": "805bb9cb4c91-1", "text": "Customizing area overlay charts\nYou can customize the appearance of an area overlay chart by editing its Chart Properties. For information about all chart properties, see Chart Properties.\u00a0Unique properties of area overlay charts include the following. You can click a thumbnail image to see a larger image.\nPropertyDescriptionExampleGeneral > StyleDetermines the style for an area chart, either \"Flat,\" \"Folded,\" \"Curved\" or \"Step.\" The default style of the chart depends on the specific subtype you have selected. If you select \"Folded,\" the chart takes on a 3-D appearance, as shown in the example at right.", "source": "../../raw_kb/article/area_overlay_chart/index.html", "title": "Area Overlay Chart"}, {"objectID": "0476f9c4d893-0", "text": "Title\n\nAsana Connector\n\nArticle Body\n\nIntro\nAsana is a cloud-based project management platform that helps users track projects, tasks, stories and other project-related workflows. To learn more about the Asana API, visit their page (https://asana.com/developers/documen...arted/overview).\nYou connect to your Asana account in the Data Center. This topic discusses the fields and menus that are specific to the Asana connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Asana account and create a DataSet, you must have the email address you use to log into Asana, as well as your Asana password. Alternatively, you can use Google credentials to connect.\nConnecting to Your Asana Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the Asana Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThe Domo Asana connector uses OAuth to connect, so there is no need to enter credentials within Domo. Click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if\u00a0you have\u00a0existing Asana accounts in Domo) to open the Asana OAuth screen where you can enter your Asana or Google credentials. Once you have entered valid credentials, you can use the same account any time you go to create a new Asana DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.", "source": "../../raw_kb/article/asana_connector/index.html", "title": "Asana Connector"}, {"objectID": "0476f9c4d893-1", "text": "Note:\u00a0If you are already logged into Asana when you connect in Domo, you are authenticated automatically when you click Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of Asana.", "source": "../../raw_kb/article/asana_connector/index.html", "title": "Asana Connector"}, {"objectID": "0476f9c4d893-2", "text": "You will also\u00a0need an API Key for your Asana account. Any user can get their own API key by visiting the Account Settings dialog and clicking on the Apps tab. The API key is located at the bottom of this dialog box.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select. If you select the \"Tasks\" report, a \"Subreport\" menu appears from which you may select a secondary report.\nMenuDescriptionReportSelect the Asana report you want to run.\u00a0The following reports are available:AttachmentsRetrieves all attachments for a specified workspace.ProjectsRetrieves all projects in a specified workspace.StoriesRetrieves all stories in a specified workspace.TagsRetrieves all tags made on tasks in a specified workspace.TasksRetrieves all tasks for a specified workspace. With this report, you may also select a subreport.TeamsRetrieves all teams for a specified workspace.UsersRetrieves all users for a specified workspace.WorkspacesRetrieves all workspaces for the authenticated user.SubreportSelect the desired subreport for the \"Tasks\" report. The following subreports are available:NoneOnly task data is returned.FollowersRetrieves a list of tasks enhanced with followers data.HeartsRetrieves a list of tasks enhanced with hearts data.MembershipsRetrieves a list of tasks enhanced with memberships data.ProjectsRetrieves a list of tasks enhanced with project data.WorkspaceSelect the workspace you want to retrieve data for.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector. \u00a0\nFAQ\nHow often can the data be updated?\nDataSets can be updated as often as once an hour.\nAre there any API limits that I need to be aware of?", "source": "../../raw_kb/article/asana_connector/index.html", "title": "Asana Connector"}, {"objectID": "0476f9c4d893-3", "text": "Are there any API limits that I need to be aware of?\nAsana does not document any limits in their API.", "source": "../../raw_kb/article/asana_connector/index.html", "title": "Asana Connector"}, {"objectID": "69c5360f88fa-0", "text": "TitleAsana Personal Access Token ConnectorArticle BodyIntro\nAsana's cloud-based project management platform helps organizations track their productivity. Domo brings your Asana projects into clearer focus by analyzing every piece of the process-users, tasks, and projects-so users can create reports, share the data with stakeholders, and get more done in less time.The Asana Connector is a safe and quick data connection that you can use to measure success and progress of initiatives at your company. Get automated reports in just seconds. Domo pulls only the most pertinent data to create lightning fast Asana reports, so you're never making choices based on outdated information. Domo's easy-to-use platform encourages more productive collaboration. To learn more about the Asana API, visit their page (https://asana.com/developers/documen...arted/overview).\nYou connect to your Asana account in the Data Center. This topic discusses the fields and menus that are specific to the Asana connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Asana account and create a DataSet, you must have the personal access token associated with your Asana account..\nConnecting to Your Asana Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the Asana Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Asana account. The following table describes what is needed for each field:\nFieldDescriptionPersonal Access TokenEnter your Asana personal access token.", "source": "../../raw_kb/article/asana_personal_access_token_connector/index.html", "title": "Asana Personal Access Token Connector"}, {"objectID": "69c5360f88fa-1", "text": "FieldDescriptionPersonal Access TokenEnter your Asana personal access token.\nOnce you have entered valid Asana credentials, you can use the same account any time you go to create a new Asana DataSet. You can manage Connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select. If you select the \"Tasks\" report, a \"Subreport\" menu appears from which you may select a secondary report.", "source": "../../raw_kb/article/asana_personal_access_token_connector/index.html", "title": "Asana Personal Access Token Connector"}, {"objectID": "69c5360f88fa-2", "text": "MenuDescriptionReportSelect the Asana report you want to run.\u00a0The following reports are available:ReportDescriptionAttachmentsThis report pulls your asana attachments for a specified workspace.Custom FieldsThis report pulls your asana custom fields for a specified workspace.GoalsThis gets detail Goals record for all Goals associated with the selected workspace.Parent GoalsThis gets detail Parent Goals records for all Goals associated to a selected workspace.ProjectsThis report gets all projects in a workspace.StoriesThis report gets all stories in a workspaceSubgoalsThis gets detail Subgoals records for all Goals associated to a selected workspace.SubtasksThis report gets all subtasks in a workspaceSupporting WorkThis gets detail Supporting Work records for all Goals associated to a selected workspace.TagsThis report gets all tags made on all tasks in a workspace.TasksThis report gets all tasks in a workspace.TeamsThis report gets all teams in a workspace.UsersThis report gets all users on a workspaceWorkspacesThis gets all workspaces for a client.SubreportSelect the desired subreport for the \"Tasks\" report. The following subreports are available:Sub-reportDescriptionNoneThis report gets all tasks in a workspace.Custom FieldsThis report gets all tasks in a workspace enhanced with custom fields data.FollowersThis report gets all tasks in a workspace enhanced with followers data.HeartsThis report gets all tasks in a workspace enhanced with hearts data.MembershipsThis report gets all tasks in a workspace enhanced with memberships data.ProjectsThis report gets all tasks in a workspace enhanced with task projects.WorkspaceSelect the workspace you want to retrieve data for.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector. \u00a0\nFAQ\nWhat version of the Asana API does this connector use?\nThis connector uses version 1 of the Asana API (https://app.asana.com/api/1.0).", "source": "../../raw_kb/article/asana_personal_access_token_connector/index.html", "title": "Asana Personal Access Token Connector"}, {"objectID": "69c5360f88fa-3", "text": "Which endpoint(s) does each report call in this connector?\nReport NameEndpoint URL(s)Attachments/tasks/{task-id}/attachmentsCustom Fields/workspaces/{workspace-id}/custom_fieldsGoals/goals/{id}Parent Goals/goals/{id}/parentgoalsProjects/workspaces/{workspace-id}/usersStories/tasks/{task-id}/storiesSubgoals/goals/{id}/subgoalsSubtasks/tasks/{task-id}/subtasksSupporting Work/goals/{id}/supportingWorkTags/workspaces/{workspace-id}/tagsTasks/projects/{project-id}/tasksTeams/organizations/{organization-id}/teamsUsers/usersWorkspaces/workspaces\nWhat kind of credentials do I need to power up this connector?\nYou need the personal access token associated with your Asana account.\nCan I use the same account multiple times to create datasets?\nYes\nHow often can the data be updated?\nDatasets can be updated as often as once an hour.\nAre there any API limits that I need to be aware of?\nNo", "source": "../../raw_kb/article/asana_personal_access_token_connector/index.html", "title": "Asana Personal Access Token Connector"}, {"objectID": "2ceafa32507f-0", "text": "TitleAspectiva ConnectorArticle BodyIntro\nAspectiva\u00a0analyzes massive volumes of consumer opinions from across the web, turning them into comprehensive and valuable insights. To learn more about the Aspectiva API, visit their page (http://www.aspectiva.com/api/).\nYou connect to your Aspectiva account in the Data Center. This topic discusses the fields and menus that are specific to the Aspectiva connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Aspectiva account and create a DataSet, you must have your Aspectiva client ID.\nConnecting to Your Aspectiva Account\nThis section enumerates the options in the Credentials and Details panes in the Aspectiva Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Aspectiva account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionClient IDEnter your Aspectiva client ID.\nOnce you have entered valid Aspectiva credentials, you can use the same account any time you go to create a new Aspectiva DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/aspectiva_connector/index.html", "title": "Aspectiva Connector"}, {"objectID": "2ceafa32507f-1", "text": "MenuDescriptionReportSelect the Aspectiva report you want to run.\u00a0The following reports are available:ProductsReturns information about a specified product.AspectsReturns a list of aspects for a specified product.Example ReviewsReturns reviews from which positive and negative options were extracted, given a specific product and aspect.ProductsEnter the ID(s) for the product(s) you want to retrieve data for. Separate multiple product IDs with commas.\u00a0ProductEnter the ID for the product you want to retrieve data for.SentimentSelect whether you want to retrieve data for positive or negative sentiment.AspectSelect all aspects you want to retrieve data for.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/aspectiva_connector/index.html", "title": "Aspectiva Connector"}, {"objectID": "a91383da67dd-0", "text": "Title\n\nAsset Library\n\nArticle Body\n\nIntro\nThe Asset Library contains all of the Apps that have been created in your instance. You can manage apps that you created, apps that have been shared with you, or (if you are an Admin) all of the apps that exist in the instance.\n\u00a0\n\nFilters\nQuick Filters provide users the ability to display apps by a selected filter.\nThe My Apps filter shows all the apps you have created.The Shared With Me filter shows all the apps that have been shared with youThe All Apps filter shows all the apps created in that environment. \n\n\n\n\nNote: you must have administrative access to have this filter option.", "source": "../../raw_kb/article/asset_library/index.html", "title": "Asset Library"}, {"objectID": "a91383da67dd-1", "text": "Note: you must have administrative access to have this filter option.\n\n\n\nSearch\nA Search field lets users perform a search by entering characters to find specific apps.\nSorting\nSorting options enable users to sort apps in various sequences.\nName sorts apps alphabetically by A to Z or Z to A.Created Date sorts by the date the app is created either by Newest to Oldest or Oldest to Newest.Updated Date sorts by the last date the app has been updated by Newest to Oldest or Oldest to Newest.\nView Options\nThe Select View provides three different options to view apps in the asset library.\nThe List View displays apps vertically in rows (similar to DataSets.)The Tile View displays apps in smaller tiles, so more apps are visible on the page.The Original View displays apps in larger tiles.\nApp Details\nApps displayed in the Asset Library show additional app details in the tile or row, including the version, people with access, and the number of Cards powered by the app.Users can click on an app to view additional information or make administrative updates with different tabs.The Overview tab shows the Design Id along with the same information that is displayed on the app tile. However, users have options to create a new Card, share the app, or review information for each version of the app from this tab.The Cards tab shows the Cards that have been created with the app and allows users to create a new Card.The Data tab shows the Cards that have been created with the app and the DataSets or collections that are used for each Card.The Versions tab allows you to see all of the versions of the app and allow them to be released.", "source": "../../raw_kb/article/asset_library/index.html", "title": "Asset Library"}, {"objectID": "62352160cc26-0", "text": "Title\n\nAssigning a Security Role to a User\n\nArticle Body\n\nThe initial security role for a user is assigned when the user is invited to join Domo or is added through SSO. This default role for new users is initially set to \"Privileged,\" but you can change it to any role you want (either a default Domo role or user-created role) in More > Admin > Governance > Roles. For more information, see Managing Custom Roles.\nIf you have an \"Admin\" security role or a customized role with the \"Manage Roles\" privilege, you can change a user's security role. If you have a \"Privileged\" security role, you can set a user's security role to \"Privileged,\" \"Editor,\" or \"Participant\"\u00a0only\u00a0when inviting the user to Domo. For more information about inviting a user, see\u00a0Inviting Others to Join Domo.\n\n\n \n\n\nNote: If a user's access level is changed, they may not be able to see certain assets anymore, depending on their ownership of those assets. For example, if a Privileged user who owns a Page or Card is reassigned to the level of Participant user, he or she will still have access to that Card or Page. If on the other hand a Card or Page was merely shared with that user, he/she would no longer have access. It will essentially disappear from their view.\n\n\n\nTo change a user's security role,\nClick\u00a0More\u00a0> Admin.The Admin Settings appears.Select the Governance tab.In the People panel, select the checkbox next to the user whose security role you want to change.Select the  icon in the top right-hand corner.The Profile menu appears.In the top section under the user's name, hover over the user's role and select the  icon.Click the dropdown menu that appears, then select the desired security role.Click\u00a0Save.\nFor more information about security roles, see Managing Custom Roles.", "source": "../../raw_kb/article/assigning_a_security_role_to_a_user/index.html", "title": "Assigning a Security Role to a User"}, {"objectID": "a089ad44c922-0", "text": "Title\n\nAssigning Users to Power Up Template Visualization Cards\n\nArticle Body", "source": "../../raw_kb/article/assigning_users_to_power_up_template_visualization_cards/index.html", "title": "Assigning Users to Power Up Template Visualization Cards"}, {"objectID": "a089ad44c922-1", "text": "Intro\nAs an executive or manager of your organization, you may want to assign a team member to create and power up a Visualization Card. Domo provides a simple way to do this. You can choose the \u00a0>\u00a0Assign Card creation\u00a0option in a\u00a0Page and assign a user in your Domo to power a Card. Domo\u00a0then creates a\u00a0template Visualization\u00a0Card\u2014a \"dummy\" Card not yet connected to a DataSet\u2014and adds it to the same Page in the\u00a0assigned user's Domo. The assigned user also receives a notification that he/she is responsible for powering and owning the Card. For more information about notifications, see\u00a0Creating an Alert for a Visualization Card.\nAssigning a user to create a Visualization Card\nWhen\u00a0you assign a Card to a user, Domo sends a message to the user and adds an empty template Card to the Page.\u00a0\nTo assign a user to create a Visualization Card,\nNavigate to the Page where you want the new Card to appear.Click .Click Assign Card creation.\tThe Assign Card dialog appears.Designate an owner by typing a name in the Assignee field.Enter a name for the Card.(Optional) Enter a message for the new owner if desired.Click Assign.\nPowering a template Card\nWhen a user assigns you to create a Visualization Card, it is your job to power it with meaningful data. Domo lets you power up charts using\u00a0DataSets that have already been uploaded to Domo. You can also upload new Microsoft Excel or Google Sheet DataSets on the fly.\u00a0\nPowering up a template Visualization Card with an existing DataSet\nIf the DataSet you want to use to power up the Card has already been uploaded, you can use the Existing Data option to retrieve the DataSet. All DataSets that you upload to Domo are stored in a database so you can locate them for use in powering up new Cards.\nTo power up a template Visualization Card using an existing DataSet,", "source": "../../raw_kb/article/assigning_users_to_power_up_template_visualization_cards/index.html", "title": "Assigning Users to Power Up Template Visualization Cards"}, {"objectID": "a089ad44c922-2", "text": "Click Power this Card at the bottom of the Card.Click Existing Data. \tA dialog appears in which you can choose a DataSet.   \tThis dialog includes many options for locating your desired DataSet. In addition to being able to search by name, you can also apply various filters to your search. If you click the\u00a0\u00a0icon, a dialog appears with a number of selectable filters such as\u00a0Owned by,\u00a0Created Date,\u00a0Tag, and so on. You can choose any of these filters then specify the criteria for the filter. For example, if you chose\u00a0Tag, you would then be prompted to select the tag associated with your desired DataSet. You can also customize filters by applying operators. These differ between filters. Most filters include the\u00a0Not\u00a0operator so you can indicate that specified criteria are excluded from your filter. For example, you might build a filter in which the status is\u00a0not\u00a0disabled. Date-based filters include the\u00a0On,\u00a0Not On,\u00a0Before, and\u00a0After\u00a0operators so you can narrow down when a DataSet was created or last run. Number-based filters", "source": "../../raw_kb/article/assigning_users_to_power_up_template_visualization_cards/index.html", "title": "Assigning Users to Power Up Template Visualization Cards"}, {"objectID": "a089ad44c922-3", "text": "a DataSet was created or last run. Number-based filters include\u00a0Equal To,\u00a0Not Equal To,\u00a0Less Than, and\u00a0Greater Than\u00a0operators.\u00a0 \tIn the filter dialog, you can also choose from several quick filters by clicking\u00a0Favorite Filters. These include\u00a0Recently run\u00a0(which defaults to 7 days),\u00a0Owned by you, and\u00a0Needs attention.\u00a0\u00a0\u00a0 \tYou can apply multiple filters to narrow down your results; for example, you might apply an\u00a0Owned by\u00a0filter specifying user \"Kate Nickelby,\" a\u00a0Type\u00a0filter specifying\u00a0\"Adobe Analytics,\" and a\u00a0Status\u00a0filter specifying\u00a0\"Disabled.\" \tOnce you have your results set, you can change the sort by clicking\u00a0Created Date (oldest to newest)\u00a0and then choosing the desired sort method and direction.\u00a0 \u00a0 \tYou can add or manage DataSets in the Data Center. For more information, see Data Center Layout.Locate and select the desired DataSet.\u00a0 \tThe Analyzer\u00a0appears. For more information, see Visualization\u00a0Card Building Part 2: The Analyzer.", "source": "../../raw_kb/article/assigning_users_to_power_up_template_visualization_cards/index.html", "title": "Assigning Users to Power Up Template Visualization Cards"}, {"objectID": "a089ad44c922-4", "text": "Powering up a template Visualization Card with a Microsoft Excel or CSV spreadsheet\nYou can power up a Visualization Card using a Microsoft Excel spreadsheet or CSV file.\nTo power up a static Visualization Card using a Microsoft Excel spreadsheet or CSV file,\nClick Power this Card at the bottom of the Card.Click Excel\u00a0(even if you are planning to upload a CSV file).Upload your file by following the instructions in\u00a0File Upload Connector.\nAfter you upload\u00a0your spreadsheet, the Analyzer\u00a0appears. For more information, see Visualization\u00a0Card Building Part 2: The Analyzer.\nPowering up a Visualization Card with a Google Sheet spreadsheet\nAs with a Microsoft Excel spreadsheet, you can power up a Card using a Google Sheet spreadsheet.\nTo power up a static Visualization Card using a Google Sheet spreadsheet,", "source": "../../raw_kb/article/assigning_users_to_power_up_template_visualization_cards/index.html", "title": "Assigning Users to Power Up Template Visualization Cards"}, {"objectID": "a089ad44c922-5", "text": "To power up a static Visualization Card using a Google Sheet spreadsheet,\nSelect Power this Card at the bottom of the Card.Click Connect Spreadsheet.(Conditional) If necessary, link your Google account to Domo by entering your Google account information. For more information, see\u00a0Google Sheets Connector.Select the desired Google Sheet spreadsheet.Specify whether the data in your spreadsheet is laid out in a column or row format. For more information about the distinction between column- and row-based spreadsheets, see\u00a0Understanding Chart Data.Click Continue.\tThe spreadsheet data appears on your screen for you to review.(Optional) Add a description for the DataSet by doing the following:Click Add DataSet Description.\t\tA Description field appears.Enter the desired description text in the field.Click Save.\t\tYou can add or manage DataSets in the Data Center. For more information, see\u00a0Data Center Layout.(Optional) Specify a range of rows or columns in the spreadsheet to be uploaded by doing the following:Click Specify where your data is.In the Label Cells fields, enter the numbers of the header cells in your spreadsheet that constitute your range boundaries. For example, if you entered \"A1\" and \"J1\", only the columns in your spreadsheet between columns A1 and J1 would be uploaded.\t\tThis pulls all of the data for the range you specified.To specify only a single column or row of data in the range you specified in the previous step, check Manually specify data range?, then enter the range for the data cells in the two fields marked Data Cells.Click Preview to see the cells in the range(s) you specified.(Conditional) If the desired spreadsheet is found on a specific Page in your document, select the correct Page from the menu under the \"Success\" message.Click Continue.\tThe Analyzer\u00a0appears. For more information, see Visualization\u00a0Card Building Part 2: The Analyzer.", "source": "../../raw_kb/article/assigning_users_to_power_up_template_visualization_cards/index.html", "title": "Assigning Users to Power Up Template Visualization Cards"}, {"objectID": "9539b5746dc1-0", "text": "TitleAssociated Press ConnectorArticle BodyIntro\nThe Associated Press (AP) is an American multinational nonprofit news agency headquartered in New York City.\u00a0You can use\u00a0Domo's\u00a0Associated Press\u00a0connector\u00a0to compile reports\u00a0containing current Associated Press feed content. For more information about the Associated Press API, see  https://developer.ap.org/api-console.\nThe Associated Press connector is a \"Cloud App\" connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the connector page for this and other Cloud App connectors by clicking Cloud App in the toolbar at the top of the window.\nYou create an Associated Press\u00a0DataSet in the Data Center. This topic discusses the fields and menus that are specific to the\u00a0Associated Press\u00a0connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is useful for pulling current news headlines into cards in Domo.Primary MetricsThis connector provides a qualitative list of news stories by category. As such there are not many metrics that can be pulled. However, some users may want a count of headlines by category or the results of specific topic mentions.Primary Company RolesExecutiveDirectorManagerAnalystAverage Implementation TimeA few minutes at mostEase of Use (on a 1-to-10 scale with 1 being easiest)1\n\u00a0\nPrerequisites\nNone. (Associated Press's data is publicly accessible.)\nCreating an Associated Press\u00a0DataSet\nThis section describes how to create an Associated Press\u00a0DataSet in the\u00a0Associated Press\u00a0Connector page in Domo. The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nDetails Pane\nThis pane contains a single menu in which you select an Associated Press\u00a0report type.", "source": "../../raw_kb/article/associated_press_connector/index.html", "title": "Associated Press Connector"}, {"objectID": "9539b5746dc1-1", "text": "Details Pane\nThis pane contains a single menu in which you select an Associated Press\u00a0report type.\nThe following table describes all of the predefined reports you can create using the Domo\u00a0Associated Press connector.\nMenuDescriptionBusinessReturns business news.EntertainmentReturns entertainment news.HealthReturns health news.Politics\u00a0Returns political news.ScienceReturns\u00a0science news.\u00a0SportsReturns\u00a0sports news.Strange\u00a0Returns offbeat news.\u00a0Technology\u00a0Returns technology news.\u00a0Top\u00a0Headlines\u00a0Returns top headlines.U.S.\u00a0NationalReturns\u00a0U.S. national news. \u00a0World\u00a0Returns world news.\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nTroubleshooting\nThis is an open connection with no credentials required. Just ensure that the correct report type and schedule have been selected.\nFAQ\nCan this connector pull custom fields?\nNo.", "source": "../../raw_kb/article/associated_press_connector/index.html", "title": "Associated Press Connector"}, {"objectID": "2a73bc4721f4-0", "text": "Title\n\nAttributes\n\nArticle Body", "source": "../../raw_kb/article/attributes/index.html", "title": "Attributes"}, {"objectID": "2a73bc4721f4-1", "text": "Intro\nYou can use attributes to drive dynamic groups and dynamic personalized data permissions (PDP). This can significantly reduce the overhead of managing group and data access. Depending on your needs, attributes can be managed by your identity provider, your Domo admins, or your users.\nWhy attributes?\nThese are examples\u00a0of two of the most common problems that attributes can solve:\nDepartment-Based Access\u00a0\nThe Sales and Marketing departments should have access to customer performance dashboards.\u00a0However, team members in these departments change frequently,\u00a0and managing group membership manually is a challenge.\nSolution\n\u00a0Use dynamic groups to automatically populate group membership based on the department attribute in a user profile.\nOrg Structure-Based Access\nBI department team members should have access to instance monitoring dashboards. The department attribute is not properly maintained in our identity provider,\u00a0but\u00a0only BI team members contribute to the BI team cost center. This assignment is tracked in the user's profile as a list of \"Contributing Cost Centers\".\nSolution \nUse dynamic groups to populate group membership based on whether this attribute contains the BI team cost center.\nThis article provides more information about attributes in the following topics:\nAdd attributeManage attributesManage attribute valuesUsageSecurityFAQ\nAdd Attribute\nFollow these steps to add an attribute:\nIn the Domo navigation header, select More\u00a0> Admin.\tThe Admin Settings display.In the Governance\u00a0menu, select Attributes.\u00a0\tThe Attributes section displays.\u00a0Select +\u00a0New Attribute.\tThe\u00a0Create new attribute\u00a0modal displays.\u00a0\n\tIn the modal, fill in the following fields:", "source": "../../raw_kb/article/attributes/index.html", "title": "Attributes"}, {"objectID": "2a73bc4721f4-2", "text": "In the modal, fill in the following fields:\n\tName\u00a0\u2014\u00a0A descriptive name shown within Domo. This does not need to be unique, although we recommend unique names to avoid confusion.(Optional) Description \u2014\u00a0Details to help users understand\u00a0what this attribute represents.Key \u2014\u00a0A unique, case-sensitive\u00a0name for\u00a0the attribute. If\u00a0the\u00a0attribute is sourced from your IdP, this must exactly match the name used in your IdP configuration. After it is\u00a0set, this value cannot be changed.Visibility \u2014\u00a0Selecting Limited visibility indicates that this attribute's\u00a0value\u00a0is not directly visible to non-admin users. However, this attribute can potentially be used to identify users. For example,\u00a0if the attribute is trusted, it can be used to populate group membership, which in turn can\u00a0identify users with a given value.Value Source \u2014\u00a0Indicates allowable sources of values for this attribute:IdP \u2014\u00a0If enabled, values are sourced from your IdP each time a user signs in\u00a0to Domo. Note that this also requires your SSO configuration to allow attribute population via the\u00a0Enable custom attribute ingestion\u00a0option.\t\t\tSee the SSO articles\u00a0for more information.Grant: Edit Users \u2014\u00a0If enabled, users with the\u00a0Edit Users\u00a0grant can edit values for this attribute. If the\u00a0IdP\u00a0source is also enabled and your IdP contains values for this attribute, those values\u00a0overwrite any user-entered values\u00a0the next time the user signs in.Managed \u2014\u00a0If enabled, this attribute is managed and users cannot edit their own values. This permits using the\u00a0attribute in security policies.Select\u00a0Save.\nManage Attributes\nYou can edit and remove certain existing attributes. System attributes are not removable.\u00a0Other attributes can be removed and edited, but only if they are not used in dynamic groups or PDP policies. This ensures the original intent of the attribute is not changed while it's in use.", "source": "../../raw_kb/article/attributes/index.html", "title": "Attributes"}, {"objectID": "2a73bc4721f4-3", "text": "The Attribute management table shows properties of your defined attributes:\nSystem \u2014\u00a0System attributes are defined by Domo and cannot be removed. However,\u00a0certain properties of system attributes are editable.Managed \u2014\u00a0Managed attributes are not editable by the user themselves;\u00a0 the value source is Managed. These attributes can be used to drive dynamic group membership and dynamic PDP policies.Data type \u2014\u00a0For system attributes, the data type is defined by Domo. For user-defined attributes, data types are inferred\u00a0from\u00a0the values received/entered. Data types follow the Java definition for that type.\tFor more details, see Java's documentation.\u00a0Attributes can also be multi-valued (list/array).PDP policies \u2014\u00a0Indicates the number of dynamic PDP policies that use this attribute. Select the number to view a list of those policies.Groups \u2014\u00a0Indicates the dynamic groups that use this attribute.Actions \u2014\u00a0Non-system attributes that are not being used as part of a PDP policy or group can be edited or deleted. System attributes cannot be removed.\nManage Attribute Values\nYou can edit attribute values in the People\u00a0section of the Governance\u00a0settings. You can only edit unmanaged attributes with the\u00a0Grant: Edit Users\u00a0value source from this page.\n\n\n\u00a0\n\nNote: If the attribute is also sourced from your\u00a0IdP, any manually changed values are reset to the IdP-sourced value the next time the user signs in.", "source": "../../raw_kb/article/attributes/index.html", "title": "Attributes"}, {"objectID": "2a73bc4721f4-4", "text": "Usage\nManaged attributes (both system and custom) can be used to populate dynamic groups based on values in a user's profile. See our Creating and Managing User Groups article\u00a0for more detail.\nSystem attributes can also be used to create dynamic PDP policies. See our Creating and Deleting PDP Policies article\u00a0for more information.\nSecurity\nUsers with the\u00a0Manage all company settings\u00a0grant can manage attributes.\nFAQ\nCan I use custom attributes in dynamic groups and dynamic PDP?\nCustom (non-system) attributes can be used in dynamic groups. Support for custom attributes in dynamic PDP is not available at this time.\nCan I export my attribute configuration?\nNot at this time.\nI'm renaming an attribute key in my IdP. How do I change the key in Domo?\nAttribute keys cannot be changed. Instead, create a new attribute.\nWhy can't I edit an attribute?\nAttributes are only editable when they are not being used to drive group membership and PDP rules. See\u00a0Manage Attributes\u00a0above for more information.\nHow do I edit attribute values in bulk?\nBulk user import does not currently support editing custom attribute values, but we are exploring options to allow that.", "source": "../../raw_kb/article/attributes/index.html", "title": "Attributes"}, {"objectID": "9eb4a7fa53a8-0", "text": "Title\n\nAugust 2016 Release Notes\n\nArticle Body\n\nNote: Depending on the product version you are using, the documentation may include information about features that may not be available or may have changed.\n\n\n\nNew features and enhancements\nFeatures and enhancements in this release include the following:\nJapanese Knowledge Base\nOur Help Center/Knowledge Base site has been live in Domo for a few months, but we\u2019ve finally gotten the entire portal translated and ready for our Japanese customers. The link to the new site will go live with this release.", "source": "../../raw_kb/article/august_2016_release_notes/index.html", "title": "August 2016 Release Notes"}, {"objectID": "9eb4a7fa53a8-1", "text": "DataFlow kill ability\u00a0\nYou can now cancel any DataFlow mid-run. This option is available in the DataFlow listings page or in the\u00a0\u00a0menu on the DataFlow details view.\u00a0\n\u00a0 \u00a0 \u00a0\nFor more information, see\u00a0Running a DataFlow.\nAutomatic retry for failed DataFlows\nFailed DataFlows will be retried once automatically without a notification being sent. If the DataFlow fails a second time, a notification will be sent as usual.\u00a0\u00a0\nFor more information, see\u00a0Running a DataFlow.\nFor more information, see\u00a0Setting Up Publication Groups.\n\"Started By\" column\nIn the Details view for a DataFlow you can now see whether a given run was executed manually or as a result of input DataSets\u00a0being updated.\u00a0\n\u00a0\nEnd of Support for Workbench 2\u00a0\u00a0\nBeginning in September,\u00a0Domo will be ending support for Workbench 2.\u00a0\nGetting Help\nYou can view the latest release notes information in the Help Center, which you can access from Domo by clicking\u00a0\u00a0> Help Center.\nIf you have questions about Domo,\nsearch for a topic in the Help Centertrain in Domo University at http://www.domo.com/universityget answers in the Domo Community at dojo.domo.comcontact Technical Support by using @DomoSupport in Buzz, by email at support@domo.com\u00a0or by phone at 801-805-9505 (Monday through Friday, 7 am to 6 pm MST)reach out to your Domo Customer Success Manager or Technical Consultant\nIf you have feedback, please send it from within Domo (\u00a0> Feedback). Or send an email to product.feedback@domo.com.\nFor more information about getting help, see Getting Help.", "source": "../../raw_kb/article/august_2016_release_notes/index.html", "title": "August 2016 Release Notes"}, {"objectID": "13287733a938-0", "text": "Title\n\nAugust 2017 Release Notes\n\nArticle Body\n\nNew features and enhancements\nFeatures and enhancements in this release include the following:\nAnalyzer (formerly Card Builder)\nIntroducing the new Analyzer\u2014data exploration, powerful analytics, insights, and a self-guided experience all in one place. As data continues to flow into businesses like never before and the breadth of different data sources increases, the new Analyzer is an analytic heaven for analysts to explore, discover, slice\u00a0and\u00a0dice, and empower others to stop making decisions with their gut and start making decisions with data.\nAll of the following new and updated features are now available:\nUser Interface. We\u2019ve added a beautiful new design with additional flexibility so you can be the driver of your own workspace, using the analytic tools you want when you need them.\n\nFor more information about the Analyzer user interface, see\u00a0Analyzer Layout.Period-over-Period.\u00a0With the powerful and flexible Period-over-Period experience in Analyzer, you\u2019ll be able to compare numerous time periods to see trends and comparisons. No other BI tool has an easier Period-over-Period experience. All you need is a date range, and your options are endless: Year over Year, Quarter over Quarter, Past 30 days, and many other options. Want to see a growth trend in your sales performance for the past few years, and add a variance percentage line along with that? Done and done.\u00a0Measure the past 30 days vs any previous days? Compare monthly sales to the same day 4, 8, 12, even 52 weeks ago? We've\u00a0got you covered. Analyzer makes the visualization of trends even easier.", "source": "../../raw_kb/article/august_2017_release_notes/index.html", "title": "August 2017 Release Notes"}, {"objectID": "13287733a938-1", "text": "For more information about implementing Period-over-Period chart, including use cases, see\u00a0Period-over-Period Charts.Quick Filters. Quick Filters are pre-defined filters that live on a card to help guide and focus management to see results in specific areas. As analysts explore their data and find important insights, they want to share these answers with management. With Quick Filters, you now can easily create a self-guided experience for others to explore data on a card and answer questions for themselves. Quick Filters you set will appear on cards in both the desktop and mobile versions of Domo.\n\n\u00a0\n\n\n\u00a0\nFor more information about Quick Filters, see\u00a0Adding Filters to Your Chart - Applying Quick Filters.Data Lineage.\u00a0Trusting your data is critical. As you find answers and explore your data, you need to be confident all the data is up-to-date and running smoothly. Since Domo can effectively bring all your data into one place as no other tool can, you may need to see all your data is combining correctly for true peace of mind. With the new Data Lineage in Analyzer, you can see the data is flowing correctly and on time without having to check the Data Center.\n\nFor more information, see\u00a0Viewing the Lineage of a DataSet in Analyzer.\u00a0Conditional Colors.\u00a0As analysts explore their data, it\u2019s often important to have consistent colors in order to highlight exceptions. If, for instance, you want a specific company to be blue or red on a DataSet to more easily identify those trends spanning many charts, you would need the Conditional Color in Analyzer. This new feature enables you to see exceptions and trends with a consistent color\u2014making analysis even easier.", "source": "../../raw_kb/article/august_2017_release_notes/index.html", "title": "August 2017 Release Notes"}, {"objectID": "13287733a938-2", "text": "For more information, see\u00a0Setting Color Rules for a Chart.\nData Table.\u00a0The new Data Table in Analyzer allows you to see and manipulate the data in all columns you have applied to your chart, along with any other unused columns you want to show. Changes you make to the data in the Data Table are reflected in your chart preview. Likewise, when you make changes to the chart using other tools such as filters and sorting, the data in the table is updated.\n\nFor more information, see\u00a0Understanding the Data Table in Analyzer.60+ New Chart Types and Improved Table Card. The new Analyzer includes over 60 new chart types. We\u2019ve also enhanced\u00a0table cards\u00a0with colors, % to total, and sub totals. All of these inclusions enables more analysis and less paralysis.\nNew chart types include the following:\nVertical and horizontal Marimekko\u00a0graphsVertical and horizontal Overlay graphsVertical Line graphsVertical and horizontal Lollipop graphs19 new Area graphsData Science graphs (Predictive Modeling, Forecasting, Outliers, and XY Line)Nautilus & Nautilus Donut graphsNightingale Rose graphStream & Stream Funnel graphs4 new maps\u2014Italy, South Korea, Switzerland, and Indonesia\nHere are screenshots of just a few of the new chart types available in Analyzer:\nVertical Line Graph\n\n\u00a0\nVertical Grouped Lollipop Graph\n\nPredictive Modeling Graph\n\n\u00a0\nNightingale Rose Graph\n\n\u00a0\nImproved Table (with subtotal rows)", "source": "../../raw_kb/article/august_2017_release_notes/index.html", "title": "August 2017 Release Notes"}, {"objectID": "13287733a938-3", "text": "Predictive Modeling Graph\n\n\u00a0\nNightingale Rose Graph\n\n\u00a0\nImproved Table (with subtotal rows)\n\nTo watch a series of videos about the new chart types and properties included in this release, visit\u00a0August 2017 Analyzer Release -- New Chart Types and Properties Videos.\nSnapshot Annotation\nLet\u2019s face it, data is messy.\u00a0It's easy to feel overwhelmed and lost when trying to decipher data visualizations. Now you can clarify the chaos with the new Snapshot Annotation feature. Pictures are worth a thousand words, and probably more when working with data. Snapshot Annotations makes analytic life easier for everyone.With the Snapshot Annotation feature, you don\u2019t need to use thousands of words to call out trends and insights. When you see a spike or trends in your data, you can now annotate on any card. Specifically, you can now draw free-form, place shapes, write text, and highlight, on any card\u2014 both on the web and on your mobile device. Then, once you\u2019ve made things more clear, sharing annotated cards is seamless in Buzz. Everyone now can spend less typing and more time making business decisions.Analysts, Managers, Senior Execs, and all data consumers need tools to help craft their data story or to visually call attention to important trends and differences on their charts. Having annotation tools makes analytic life easier for everyone because it's more efficient to communicate with visual tools to help educate others.", "source": "../../raw_kb/article/august_2017_release_notes/index.html", "title": "August 2017 Release Notes"}, {"objectID": "13287733a938-4", "text": "For more information about this feature, see\u00a0Taking Snapshots of Cards.\nNew Self-Service Connectors\u00a0\nThis month we have polished and released another 18 connectors that are now available to all customers in their Domo account. These connectors are self-service, so you no longer need to contact support to get help powering them up. Here are the new connectors:\nConnectorLink to Knowledge Base articleBase CRMBase CRM ConnectorDataSet via Email (described in more detail below)DataSet via Email ConnectorDoceboDocebo ConnectorDotmailerDotmailer ConnectorGradLeadersGradLeaders ConnectorIndeedIndeed ConnectorInfusionsoftInfusionSoft ConnectorKafkaKafka ConnectorKlaviyoKlaviyo ConnectorMavenlinkMavenlink ConnectorPaydirtPaydirt ConnectorPostgreSQL SSHPostgreSQL SSH ConnectorSilverpopSilverpop ConnectorSmartRecruitersSmartRecruiters ConnectorSportradarSportradar ConnectorTeradata via ODBCConnecting to ODBC Data in Workbench 5The Trade DeskThe Trade Desk ConnectorTreasure DataTreasure Data Connector\nDataSet via Email Connector\nThe Dataset via Email connector enables you to get your data into Domo quickly and easily by capturing and processing .xls, .xlsx, and .csv files that have been included as attachments to an email. When you configure this connector, it generates a unique email address that corresponds to your Dataset via Email DataSet in Domo. Once the email address is created, you can send the spreadsheet as an attachment to that email address. The connector will then process the email and update your data accordingly.\nFor more information, see\u00a0DataSet via Email Connector.\nWorkbench UI Improvements\nList View\nWorkbench now includes\u00a0a list view that allows\u00a0you to\u00a0search\u00a0and sort\u00a0DataSet jobs, see upcoming schedules, and organize jobs by status,\u00a0giving better visibility into jobs to facilitate job management\u00a0in an instance.", "source": "../../raw_kb/article/august_2017_release_notes/index.html", "title": "August 2017 Release Notes"}, {"objectID": "13287733a938-5", "text": "Status Panel\nThe new Status Panel in Workbench provides you with\u00a0better insights around when jobs are being run and allows\u00a0you\u00a0to make changes, pause, cancel and run jobs. The Status Panel will be visible from every job details page to give real-time information about job status.\n\nRank &\u00a0Window Functions\u00a0in Magic ETL\nDomo's Magic ETL\u00a0tool now includes\u00a0Rank &\u00a0Window\u00a0functions. These new functions enable you to rank and compare data without having to be familiar with complex SQL functions and syntax.\u00a0These functions are available in Magic ETL in a new action tile called Rank & Window, which\u00a0can be found in the Edit Columns section.\nThe new Rank &\u00a0Windows functions include all of the following:\nRankDense RankRow NumberAvgCountSumLagLead\n\nFor detailed information about each of these functions and step-by-step instructions for using them, see ETL Actions: Rank and Window.\nSelf-Service Payments\nBeginning August 10th, existing Domo customers will be able to add and pay for licenses right from within the Domo product. With this change, please be aware of the following:\nThis will be rolled out over time, so some customers may not see this change right away.The price you see is based on your current contract.Tax is applied to the purchase price in the product.\u00a0\nMobile Page Filters\nWe have taken the powerful and complex page filters tool and released it to the mobile platform in an accessible and intuitive way. Any business user can immediately gain value. Whether you're an executive walking into one of your retail stores or a manufacturing manager looking at a specific product line, you can quickly filter your full page to find the story.\nTry it out for yourself. Navigate to any of your pages in Domo on mobile, tap into the menu, and filter away.", "source": "../../raw_kb/article/august_2017_release_notes/index.html", "title": "August 2017 Release Notes"}, {"objectID": "13287733a938-6", "text": "Mobile Card/Page Sharing\nDomo has introduced a simple way to collaborate even more easily while on your phone. Have an important card or page you need to share with an individual or your team? Do it now from your phone. As of today you can tap into any card or page and easily share it with others.\nPage API\nWith Domo\u2019s Page public API, users can now automate the creation or duplication of pages and seamlessly update the page\u2019s content. The ability to manage pages through the API gives you the ability to create systems and processes to easily share existing information and content inside a Domo page that can be tailored to any situation that arises.\nTo begin developing with Domo\u2019s Pages API requires little effort. Once you create authentication tokens in the developer portal\u2019s client section, you can then explore the Page API functionality by following the steps within the Page API Quickstart\u00a0guide or by downloading pre-built Page API examples from any of Domo\u2019s official SDK libraries.\nDeveloper Sandbox\nBy obtaining a sandbox, developers can now\u00a0quickly explore, iterate, and accelerate their development cycles on Domo\u2019s APIs. A developer sandbox also empowers developers with a self-service tool to simulate scenarios with APIs to better understand and handle certain error conditions. This ultimately enables the creation of more resilient applications that can be confidently introduced into production environments.\nThe option to obtain a developer sandbox is now available\u00a0in Domo\u2019s Developer portal, https://developer.domo.com/dev-sandbox-request.\u00a0 After you submit a request for your sandbox, a new Domo instance is created automatically, and an invitation is sent\u00a0to your email to activate your login.\u00a0\nOnce you activate your account, you will then be able to explore specific developer links in your Sandbox as well as leverage pre-built sample data, cards, and content to begin developing solutions.\nGetting Help", "source": "../../raw_kb/article/august_2017_release_notes/index.html", "title": "August 2017 Release Notes"}, {"objectID": "13287733a938-7", "text": "Getting Help\nYou can view the latest release notes information in the Help Center, which you can access from Domo by clicking\u00a0\u00a0> Help Center.\nIf you have questions about Domo,\nsearch for a topic in the Help Centertrain in Domo University at http://www.domo.com/universityget answers in the Domo Community at https://dojo.domo.comcontact Technical Support by entering a help ticket in the Domo Support Portal, by sending a Buzz message to \\support, or by emailing support@domo.com.reach out to your Domo Customer Success Manager or Technical Consultant\nIf you have feedback, please send it from within Domo (\u00a0> Feedback). Or send an email to product.feedback@domo.com.\nFor more information about getting help, see Getting Help.", "source": "../../raw_kb/article/august_2017_release_notes/index.html", "title": "August 2017 Release Notes"}, {"objectID": "3dfe0c105af6-0", "text": "Title\n\nAugust 2018 Release Notes\n\nArticle Body", "source": "../../raw_kb/article/august_2018_release_notes/index.html", "title": "August 2018 Release Notes"}, {"objectID": "3dfe0c105af6-1", "text": "New Features and Enhancements\nFeatures and enhancements in this release include the following:\nUpdated Buzz\nTurn conversations into action with the updated Buzz.\u00a0Experience a simpler and cleaner way to communicate about data with a clear focus on moving conversations into action. Collaborate on data with every necessary person, no matter his or her device or location. Turn conversations and data into tasks and track them to completion. Flag messages for follow-up when you\u2019re ready. And thanks to focused and clear notifications, never miss out on a conversation you care about or a decision you need to make.\u00a0\nThe latest release of Buzz includes all of the following:\nA simplified navigation menu for quickly locating recent conversations as well as those you have added as Favorites.\n\u00a0\nThe ability to flag important messages for later,\u00a0then open them in a special pane so you can\u00a0take action on them. \t\u00a0A new\u00a0Discover\u00a0pane that shows you trending topics, online users, messages that have had the most reactions, and recent conversations.\u00a0 \t\u00a0A greatly improved keyword search that identifies results based on conversation title, member name, or message content.\u00a0 \t\u00a0\nFor more information about the layout of the new Buzz, see\u00a0Chatting in Buzz.\nData Tab in Data Center\nA new\u00a0Data\u00a0tab is available when you open the details page for a DataSet in the Data Center. This tab empowers you to view and manipulate\u00a0your DataSet data in ways you never could before, including reordering columns, viewing Domo-generated\u00a0metrics about your DataSet usage, searching and filtering columns, and more.\u00a0\nThe\u00a0Data\u00a0tab consists of three subtabs, as follows:", "source": "../../raw_kb/article/august_2018_release_notes/index.html", "title": "August 2018 Release Notes"}, {"objectID": "3dfe0c105af6-2", "text": "The\u00a0Data\u00a0tab consists of three subtabs, as follows:\nThe\u00a0Table\u00a0subtab lets you\u00a0see all of the columns in your DataSet. Options are available for searching, sorting, and filtering so you can find the data you need. \t\u00a0The\u00a0Schema\u00a0subtab lets you\u00a0reorder the columns in your DataSet, add descriptions and tags, etc. \t\u00a0The\u00a0Stats\u00a0subtab lets you view\u00a0Domo-generated statistics for the columns in your DataSet (i.e. DataSet Metrics). For numeric and date/date-time columns, you can correlate the columns against other numeric columns in the DataSet, and for text columns, you can see the highest aggregated values for the sum or\u00a0count of any selected column.\u00a0\t\u00a0\n\u00a0\nFor more information about the\u00a0Data\u00a0tab, see\u00a0Data Center Layout.\nAbility to Add an Existing Card to a Page\nYou can now add any existing card to a page from that page, without leaving the page. With the new Add Card menu option, you can save time building your page and prevent multiple versions of the same card. Under the Add Card menu option, you can select Add Card > Existing to search for a Card from anywhere in Domo and immediately add it to your page.\u00a0\n\u00a0\nFor more information, see\u00a0Adding an Existing Card to a Page.\nMobile Table Cell Wrapping\nDomo's Mobile team has added support for text wrapping in tables while on the go from your iOS device.\nConnector API Change Announcements\nOther items our users should know about are as follows.\nGoogle AdWords and DoubleClick\u00a0Campaign Manager API Update", "source": "../../raw_kb/article/august_2018_release_notes/index.html", "title": "August 2018 Release Notes"}, {"objectID": "3dfe0c105af6-3", "text": "Google AdWords and DoubleClick\u00a0Campaign Manager API Update\nTo our valued customers,\u00a0What is happening?\u00a0At Domo, we take pride in providing the industry leading, best time to value solution with an eco-system of over 500 connectors. We regularly review our connector offerings and make enhancements to provide a more robust experience for our customers. Today we are pleased to announce upcoming API updates to the Google AdWords and DoubleClick Campaign Manager connectors.\u00a0\u00a0When will it happen?\u00a0August 20th.\u00a0\u00a0How does this impact me?\u00a0You will be able to enjoy a better experience with these connectors.\u00a0What do I have to do?\u00a0There should be nothing you would need to do to take advantage of these enhancements. Nevertheless, please verify all your DataSets\u00a0are working properly and reach out to us if you have any questions.\nSalesforce API Change\nDue to Salesforce's changing API, on August 1st, Domo will be releasing new Salesforce and Salesforce Advanced connectors. These\u00a0connectors will work in conjunction with a Salesforce managed package installed via the Salesforce App Store. The new version will enable Salesforce administrators to control who can use the Domo Salesforce connectors inside Salesforce, providing ease of administration and enhanced security.\nWill my current DataSets continue to work?\nCurrent Salesforce and Salesforce Advanced connector DataSets do not work in conjunction with the Salesforce managed package, and no dependencies need to be installed for the connector to function. Existing DataSets will continue to run until September 1st.\nI want to create a new Salesforce DataSet. What should I do?\nBeginning August 1st, any new DataSets created will require you to download the Salesforce managed package via the Salesforce App Store to properly function.\nWhat will happen if I don't install the Salesforce managed package before September 1st?\nAny Salesforce instance bringing data into Domo via the Salesforce or Salesforce Advanced connector will stop running until the Salesforce managed package is installed.", "source": "../../raw_kb/article/august_2018_release_notes/index.html", "title": "August 2018 Release Notes"}, {"objectID": "3dfe0c105af6-4", "text": "For more information on how to download and install the Salesforce managed package, see Installing Salesforce's Domo Connector Package.\nFacebook API Change\nFacebook recently made a number of major changes to its API due to security issues. Our development team is doing all it can to address issues caused by these changes. If you run into any problems with your Facebook DataSets, please open a case with Domo\u00a0Support.\u00a0\nGetting Help\u00a0\nYou can view the latest release notes information in the Help Center, which you can access from Domo by clicking\u00a0\u00a0> Help Center.\nIf you have questions about Domo,\nsearch for a topic in the Help Centertrain in Domo University at\u00a0http://university.domo.comsearch for training apps in the Appstoreget answers in the Domo Community at\u00a0https://dojo.domo.comcontact Technical Support by entering a help ticket in the Domo Support Portal or\u00a0by sending a Buzz message to\u00a0/support.reach out to your Domo Customer Success Manager or Technical Consultant\nIf you have feedback, please send it from within Domo (\u00a0> Feedback). Or send an email to\u00a0product.feedback@domo.com.\nFor more information about getting help, see\u00a0Getting Help.", "source": "../../raw_kb/article/august_2018_release_notes/index.html", "title": "August 2018 Release Notes"}, {"objectID": "0ddc1613e5fc-0", "text": "TitleAurora ConnectorArticle BodyIntro\nAurora is a MySQL and PostgreSQL compatible relational database built for the cloud, that combines the performance and availability of high-end commercial databases with the simplicity and cost-effectiveness of open source databases. Use the Aurora MySQL connector to efficiently and securely bring in data from your database. Once your data is in Domo, you can give managers and stakeholders transparency with real-time, interactive dashboards. Set up alerts to be notified of changes in your key data points. And you can access all your data in real-time, from any device.\nDomo's Aurora\u00a0Connector is an open-source database. To learn more about the Amazon Aura connector \u00a0API, visit their page (\u00a0https://aws.amazon.com/rds/aurora/).\nThe Aurora\u00a0Connector is a \"Cloud App\" Connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the Connector page for this and other Cloud App Connectors by clicking\u00a0Cloud App\u00a0in the toolbar at the top of the window.\nYou connect to your Amazon Aura account in the Data Center. This topic discusses the fields and menus that are specific to the Amazon Aura\u00a0Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Amazon Aurora account and create a DataSet, you must have the following:\nThe username and password\u00a0you use to log into your databaseThe hostname or IP address of your database serverThe port number for\u00a0your databaseThe database name or schema name\nYou can also include the URL where your SSL CA Certificate is located, though this is optional.\nConnecting to Your Aura Account", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "0ddc1613e5fc-1", "text": "Connecting to Your Aura Account\nThis section enumerates the options in the Credentials and Details panes in the Aura Connector page. The components of the other panes on this page,\u00a0Scheduling,\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThe Domo Aurora\u00a0Connector uses OAuth to connect, so there is no need to enter credentials within Domo. Click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if you have existing Aurora database accounts in Domo) to open the Aurora OAuth screen where you can enter your Aurora database\u00a0username and password. Once you have entered valid Aurora database\u00a0credentials, you can use the same account any time you go to create a new Aurora connector\u00a0DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\nThe following Credential table describes what is needed for each field:\u00a0\nFieldDescriptionUsernameEnter your database username.PasswordEnter your database password.HostEnter the hostname or IP address of your database server. Example:\u00a0db.company.comPortEnter the port number for your database server.Database NameEnter your database name.CA Certificate\u00a0(optional)Enter the URL where the SSL CA Certificate is located.\nOnce you have entered valid credentials, you can use the same account any time you go to create a new\u00a0Amazon Aurora\u00a0Partition\u00a0DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "0ddc1613e5fc-2", "text": "Note:\u00a0If you are already logged into Aurora connector when you connect in Domo, you are authenticated automatically when you click\u00a0Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of the Aurora connector.", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "0ddc1613e5fc-3", "text": "Details Pane\nThis page contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionQueryEnter the Structured Query Language (SQL) query to use in selecting the data you want. For example:\nselect * from Employee\nYou can use the\u00a0Query Helper\u00a0parameter to help you write a usable SQL query. To use the\u00a0Query Helper, do the following:Select your database table\u00a0and table columns in the appropriate menus.Copy the SQL statement that appears in the\u00a0Query Helper\u00a0field.Paste the copied SQL statement into the\u00a0Query\u00a0field.Query TypeSelect the desired MySQL query type\u2014either with or without a parameter.Query ParameterEnter the query parameter value. This is the initial value for the query parameter. You can use this option to retrieve new data since the last run.\u00a0\nFor example, if you entered the following query in the\u00a0Query\u00a0field...\nselect * from test.lastValue where id <= !{lastvalue:id}! order by id desc\n...and then entered the following for the\u00a0Query Parameter...\n!{lastvalue:id}!=3\n...then the first run would return 3 rows, and all subsequent runs would return 1 row, and the results would be ordered from largest to smallest.\u00a0\nSimilarly, if you entered the following in the\u00a0Query\u00a0field...\nselect * from test.lastValue where time > !{lastrundate:time}!\n...and then entered the following for the\u00a0Query Parameter...\n!{lastrundate:time}!=01/01/1990", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "0ddc1613e5fc-4", "text": "!{lastrundate:time}!=01/01/1990\n...then the first run would return 5 rows and all subsequent runs would return 0 rows.Partition Column NameSelect partition column name.Past DaysEnter the number of past days you want to get data for. Value can be a positive integer. For example 30.Date FormatSelect the required date format. By default,\u00a0yyyy-MM-dd\u00a0will be used.TinyInt Values Treated as Bit (Boolean) ValuesSelect Yes if you want the tiny integer\u00a0values to be treated as boolean\u00a0values, else select No.Cast Boolean Values To...Select whether the String or Integer boolean values will be cast within your dataset.\u00a0String: False/TrueInteger: 0/1Database Table (Optional)Select the database table you want to import into Domo.\u00a0Table Columns (Optional)Select the table columns you want to import into Domo.Query Helper (Optional)\u00a0Copy and paste the SQL statement in this field into the\u00a0Query\u00a0field. For more information, see\u00a0Query, above.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\u00a0\nWhat kind of credentials do I need to power up this connector?\nYou need the username, password, hostname or IP address, port number, and database name associated with your database. You can also include the URL where your SSL CA certificate is located, though it is optional.\nHow to get a Certificate Authority (CA) certificate?\nTo get a certificate bundle that contains both the intermediate and root certificates for all AWS Regions, download from\u00a0https://truststore.pki.rds.amazonaws.com/global/global-bundle.pem.", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "0ddc1613e5fc-5", "text": "If your application is on Microsoft Windows and requires a PKCS7 file, you can download the PKCS7 certificate bundle. This bundle contains both the intermediate and root certificates at\u00a0https://truststore.pki.rds.amazonaws.com/global/global-bundle.p7b.", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "0ddc1613e5fc-6", "text": "Note:\u00a0Amazon RDS Proxy\u00a0and Aurora Serverless use\u00a0certificates from the AWS Certificate Manager (ACM). If you are using RDS Proxy, you don't need to download Amazon RDS certificates or update applications that use RDS Proxy connections. For more information about using TLS/SSL with RDS Proxy, see\u00a0Using TLS/SSL with RDS Proxy.\nIf you are Aurora Serverless, downloading Amazon RDS certificates isn't required. For more information about using TLS/SSL with Aurora Serverless, see\u00a0Using TLS/SSL with Aurora Serverless v1.", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "0ddc1613e5fc-7", "text": "Are there any API limits that I need to be aware of?\nLimits depend on your server configuration.\nHow frequently will my data update?\nDatasets can run as often as every 15 minutes. However, depending on the runtime of the query, datasets may need to run less frequently.\nWhat do I need to be aware of while writing a query?\nMake sure that all the words, table names, and field names are correctly spelled.\nWhy can't I connect to my database? Do I need to whitelist any IP addresses?\nBefore you can connect to your database, you must also whitelist a number of IP addresses on your database server on the port you want to connect to. For the full list of IP addresses, see Whitelisting IP Addresses.\nWhat's the difference between the Amazon Aurora and Amazon Aurora Partition connectors?\nIn the Amazon Aurora connector, you need to select a JDBC driver to connect to your database. The Amazon Aurora connector gives you a choice to write an SQL query with or without the query parameter. It allows you to select the database table and table columns. It also provides a Query Helper to guide you in writing an SQL query.\nThe Amazon Aurora Partition connector supports partitioning, thus optimizing your database performance. You need to select the table name and partition column name. You also need to specify the number of past days you want to get data for. It gives you a choice if you want your TinyInt values to be treated as String or Integer, though it's optional. You can select the Date format for your data to be retrieved.\nTroubleshooting", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "0ddc1613e5fc-8", "text": "Troubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/aurora_connector/index.html", "title": "Aurora Connector"}, {"objectID": "454f8badde8c-0", "text": "TitleAustralian Bureau of Statistics ConnectorArticle BodyIntro\nUse the Australian Bureau of Statistics connector to pull public Australian population\u00a0data for a variety of categories, such as quarterly population estimates, aboriginal population projections, and more.\u00a0\u00a0\nYou connect to your Australian Bureau of Statistics account in the Data Center. This topic discusses the fields and menus that are specific to the Australian Bureau of Statistics connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nNone.\u00a0This connector pulls in public data, so there is no need to enter connection information.\nConnecting to Your Australian Bureau of Statistics Account\nThis section enumerates the options in the Detail\u00a0panes in the Australian Bureau of Statistics Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nDetails Pane\nThis pane contains a primary\u00a0Dataset\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/australian_bureau_of_statistics_connector/index.html", "title": "Australian Bureau of Statistics Connector"}, {"objectID": "454f8badde8c-1", "text": "MenuDescriptionDatasetSelect the Australian Bureau of Statistics report you want to run.\u00a0The following reports are available:Aboriginal Population Projections IndigenousReturns aboriginal and Torres Strait Islander population projections by indigenous regionAboriginal Population Projections RemoteReturns aboriginal and Torres Strait Islander population projections by Remoteness Area.Aboriginal Population ProjectionsReturns aboriginal and Torres Strait Islander population projections by state/territory.Estimated Resident Population, Country of BirthReturns estimated resident population (including country of birth, age and sex) from 30 June 1992 onwardsEstimated Resident\u00a0Population, Country of Birth, Median Age and Sex RatioReturns estimated resident population\u00a0(including country of birth, median age, and sex ratio)\u00a0 from 30 June 1992 onwardsEstimated Resident Population, Country of Birth, StateReturns census-estimated resident population (including country of birth, state/territory, age, and sex)\u00a0 from 30 June 1996 onwardsPopulation ClockReturns population clock component dataPopulation Projections by RegionReturns population projections by region from 2012 to 2061Population ProjectionsPopulation projections for 2012 to 2101Quarterly Population EstimatesReturns quarterly population estimates", "source": "../../raw_kb/article/australian_bureau_of_statistics_connector/index.html", "title": "Australian Bureau of Statistics Connector"}, {"objectID": "454f8badde8c-2", "text": "to 2101Quarterly Population EstimatesReturns quarterly population estimates (ERP) by state/serritory, sex and ageSexSelect the gender(s) you want to return information for.AgeSelect the age range(s) you want to return information for.Country of BirthSelect the country or countries of birth you want to return information for.State/TerritorySelect the Australian states and/or territories you want to return information for.RegionSelect the Australian region(s) you want to return information for.Indigenous RegionSelect the Australian indigenous region(s) you want to return information for.Projected SeriesSelect the projected series you want to return information for.FrequencySelect the desired date granularity for your report. For example, if you select\u00a0Annual, data will be returned for individual years.Remoteness AreaSelect the Remoteness Area(s) you want to return information for.Fertility AssumptionSelect whether to pull data for high, low, and/or medium fertility levels.Mortality AssumptionSelect whether to pull data for high and/or medium life expectancy.Net Overseas MigrationSelect whether to pull data for high, medium, low, and/or zero net overseas migration.Net Interstate MigrationSelect whether to", "source": "../../raw_kb/article/australian_bureau_of_statistics_connector/index.html", "title": "Australian Bureau of Statistics Connector"}, {"objectID": "454f8badde8c-3", "text": "zero net overseas migration.Net Interstate MigrationSelect whether to pull data for large, medium, or small interstate flows.StartSelect the first year you want to be included in your report data.EndSelect the last year you want to be included in your report data.DetailSelect the desired level of detail for your report.Dimension at Observation LevelSelect how many dimensions you want to appear in your report.", "source": "../../raw_kb/article/australian_bureau_of_statistics_connector/index.html", "title": "Australian Bureau of Statistics Connector"}, {"objectID": "454f8badde8c-4", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits I should be aware of?\nNo.", "source": "../../raw_kb/article/australian_bureau_of_statistics_connector/index.html", "title": "Australian Bureau of Statistics Connector"}, {"objectID": "c0d1e9f59da8-0", "text": "TitleAutomatic ConnectorArticle BodyIntro\nThe Automatic\u00a0app and plug-in car adapter turns just about any car into a connected car. To learn more about the Automatic API, visit their page (https://developer.automatic.com/api-reference/#introduction).  \nYou connect to your Automatic account in the Data Center. This topic discusses the fields and menus that are specific to the Automatic connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0 a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Automatic account and create a DataSet, you must have Automatic account credentials.\nConnecting to Your Automatic Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the Automatic Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0 a DataSet Using a Data Connector.\nCredentials Pane\nThe Domo Automatic connector uses OAuth to connect, so there is no need to enter credentials within Domo. Click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if\u00a0you have\u00a0existing Automatic accounts in Domo) to open the Automatic OAuth screen where you can enter your Automatic credentials. Once you have entered valid Automatic credentials, you can use the same account any time you go to create a new Automatic DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the\u00a0Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.\n\n\n\u00a0\n\nNote:\u00a0If you are already logged into Automatic when you connect in Domo, you are authenticated automatically when you click Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of Automatic.", "source": "../../raw_kb/article/automatic_connector/index.html", "title": "Automatic Connector"}, {"objectID": "c0d1e9f59da8-1", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Automatic report you want to run.\u00a0The following reports are available:DeviceReturns information about the authenticated user's Automatic devices.TripReturns information about trips taken in vehicles linked to the authenticated user's Automatic account.\u00a0VehicleReturns information about vehicles linked\u00a0to the authenticated user's Automatic account.\u00a0Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.", "source": "../../raw_kb/article/automatic_connector/index.html", "title": "Automatic Connector"}, {"objectID": "c0d1e9f59da8-2", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/automatic_connector/index.html", "title": "Automatic Connector"}, {"objectID": "50080b4cd720-0", "text": "Title\n\nAutoML (Machine Learning)\n\nArticle Body\n\nIntro\nDomo leads the way in making machine learning accessible to everyone.\n\nYou depend on Domo to access your data from a wide variety of sources and make it usable in record time. Machine learning (ML) is the next step to provide insight to complex business problems, improve decision-making, and automate business processes. However, many organizations don\u2019t have the data science expertise to create a machine learning model for their data. Additionally, organizations who have data science teams are often drowning in opportunities and spend months exploring different business problems without ever getting their machine learning solutions into production. AutoML provides a solution in these situations.\nDomo and AWS: A partnership for success\n\nWe\u2019re thrilled to announce that machine learning is now within everyone\u2019s grasp with our new automated machine learning (AutoML). In partnership with Amazon SageMaker Autopilot, we\u2019ve created AutoML capabilities that allow you to augment your analytics with machine learning, whether you\u2019re a data scientist or a data science novice, then share those insights with anyone. Amazon SageMaker Autopilot is an Amazon Web Services (AWS) solution that automatically trains and tunes ML models based on data provided by a customer. Companies can now use their data in Domo as input into Amazon SageMaker Autopilot, automatically create the highest performing model and deploy a prediction pipeline that adapts to new, incoming data. The combination of Domo and Amazon SageMaker Autopilot helps make ML accessible to more employees and propels ML-driven insights for business.", "source": "../../raw_kb/article/automl_machine_learning/index.html", "title": "AutoML (Machine Learning)"}, {"objectID": "50080b4cd720-1", "text": "Note:\u00a0 This feature is available\u00a0on demand and paid.\n\u00a0\nTo request this feature be enabled,\nReach out to your Domo Customer Success Manager, Technical Consultant, or Account Executive.If you do not have contact information for your CSM, TC, or AE, contact Technical Support. For information on how to contact Support, please see: Getting Help\nDepending on the feature, you may be required to complete training before you can use the feature.\n\n\n\n\nHow AutoML works", "source": "../../raw_kb/article/automl_machine_learning/index.html", "title": "AutoML (Machine Learning)"}, {"objectID": "50080b4cd720-2", "text": "Domo leverages Amazon SageMaker Autopilot to make it easy to automatically train and tune ML models needed to predict outcomes. With just a few clicks, AutoML will transform your data to be ready for machine learning and launch hundreds of training jobs on any DataSet in Domo to find the model that achieves the best performance for your task. You can then easily deploy the model on your Domo DataSets with the new AutoML Interface tile in Magic ETL. All that is required to get success in machine learning with AutoML is a well-defined problem and a clean DataSet\u2014Domo will take care of the rest.\nCheck out this video to see it in action.\nSteps to use AutoML\nAlign your data toward your business objective and prepare it for machine learning.Launch an AutoML training job.Evaluate model performance on the Model Leaderboard Page.If the model performs adequately for your business objective, deploy the AutoML Inference\u00a0tile in Magic ETL.Set up model monitoring Dashboards.Create an app or Story on predictions for stakeholders.\nPrepare data for AutoML\nTake some time to align with your business stakeholders so you have a thorough understanding of your business problems, which tasks would be worthwhile to automate, what model performance would be acceptable for your business problem, and what data is available. Spend some time gathering your data together into one DataSet that has one column with\u00a0the output variable (the variable you want to predict that AutoML will train on) and many columns that enumerate the inputs that you expect to have an effect/influence on your output column. Be sure to only include one output column for your DataSet. Additionally, each row should encapsulate one record in your business problem. For example, if you would like to predict which sales opportunity will close, each row should enumerate one distinct sales opportunity, from start to finish, with the output column listing the result of the sales opportunity (ie won or lost).", "source": "../../raw_kb/article/automl_machine_learning/index.html", "title": "AutoML (Machine Learning)"}, {"objectID": "50080b4cd720-3", "text": "There are many wonderful blog posts, courses, books and videos on data cleaning for machine learning that you can find online or Domo\u2019s professional services team are always a resource at your disposal to enhance your data cleaning skillset.\nHow to Launch an AutoML Training Job\nBelow are instructions on how you can use AutoML along with a sample DataSet for you to try. This is a DataSet outlining customer churn at a phone company, originally obtained from the UCI machine learning repository.\nClick on this link to download the test DataSet and then upload it to your instance\u00a0as a new DataSet.\nOnce your DataSet is uploaded to Domo, you should be automatically directed to the DataSet Details page for your new Churn DataSet. Click\u00a0on the AutoML\u00a0tab or select the Train New Algorithm\u00a0option and then click Get started.AutoML will ask you which column you want to automate. You want to predict customer churn, which is based on the \u201cclass\u201d column in the DataSet. So, under the Numbers\u00a0section, find and\u00a0select class\u00a0(you might have to click on the ellipsis button to find it).Once your Target Column (\u201cclass\u201d) has been selected, determine what training task you would like AutoML to run. For our scenario, you can choose Automatic, but you\u2019re welcome to specify your task explicitly if you know which you would like run.Click Start Training\u00a0and then sit back and watch as Mr Roboto does all the work for you!\nHow to evaluate model performance", "source": "../../raw_kb/article/automl_machine_learning/index.html", "title": "AutoML (Machine Learning)"}, {"objectID": "50080b4cd720-4", "text": "How to evaluate model performance\nOn the Model Overview Page, the top performing model will automatically be highlighted for you. On this Page you can compare the many different trials that AutoML ran on your behalf. You can view model performance against the training and validation sets and look at what hyperparameters produced the best performing model. If performance is acceptable to your business problem you can deploy the model in production via a Magic ETL DataFlow. This will allow you to make predictions against your data every time the data updates.\nHow to make predictions on new data via the AutoML\u00a0Interface\u00a0tile in MagicETL\nConnect a DataSet with a matching schema to your training DataSet and the AutoML Inference tile will use that machine learning model trained by AutoML to make predictions on all the rows of your DataSet.", "source": "../../raw_kb/article/automl_machine_learning/index.html", "title": "AutoML (Machine Learning)"}, {"objectID": "cc679c80bb59-0", "text": "Title\n\nAvailable Area Charts\n\nArticle Body", "source": "../../raw_kb/article/available_area_charts/index.html", "title": "Available Area Charts"}, {"objectID": "cc679c80bb59-1", "text": "An Area graph is a Multi-Line graph in which the area between the topmost line and the category axis is colored in. There are three primary types of Area graphs in Domo:\nStandard Area graphs (referred to simply as \"Area\" graphs in Domo), in which all series begin from the same axis and are overlaid on top of each other. You can distinguish an individual series by mousing over that series in the legend or in the graph itself. This type of Area graph is useful for quickly discerning the value for individual series, as each series begins from the same baseline. You can learn how to build these graphs in Area Graph.Stacked Area graphs, in which series are stacked on top of each other. Because all series do not share the same baseline, this type of Area graph is better for comparing trends than for discerning individual amounts. You can learn how to build these graphs in\u00a0Stacked Area Graph.The Stream graph, in which series are centered, similar in appearance to a Stream Funnel graph but horizontal instead of vertical. Series in a Stream graph do not share the same baseline and do not overlap. Thus they are better than standard Area and Stacked Area graphs at showing changes between stages. \u00a0\nStandard and Stacked Area graphs can be broken down further into various subtypes, including vertical and horizontal, straight lines versus curved and step, etc.\nThe following tables list the types of Area graphs available in Domo. You can click a thumbnail image to see a larger image.\nStandard (Overlay) Area Graphs\nIn a standard Area graph, all series begin from the same axis and are overlaid on top of each other. In their default state these graphs are typically not very useful because data for some series is usually hidden. However, when you mouse over a series in the graph or legend, you can see just the data for that series.\u00a0\nFor information about building standard Area graphs, see Area Overlay Chart.", "source": "../../raw_kb/article/available_area_charts/index.html", "title": "Available Area Charts"}, {"objectID": "cc679c80bb59-2", "text": "For information about building standard Area graphs, see Area Overlay Chart.\nChart TypeDescriptionExampleVertical Area graph\nIn a vertical Area graph, categories are shown on the x-axis and values on the y-axis, and lines are straight instead of curved. As with other standard Area graphs, all series begin from the baseline, so overlapping of series occurs. However, you can see individual series by mousing over series items in the graph or legend.\nThe first example at right shows a vertical Area graph in its default state. Seven series are present, but only three are visible in the graph because of overlapping. In the second example, the user is mousing over the LinkedIn series, so only that series appears in the graph.\nHorizontal Area graph\nA horizontal Area graph is the same as a vertical Area graph except that the categories are shown on the y-axis and the values on the x-axis.\nThe first example at right shows a horizontal Area graph in its default state. Seven series are present, but only three are visible in the graph because of overlapping. In the second example, the user is mousing over the LinkedIn series, so only that series appears in the graph.\nVertical Curved Area graph\nIn a vertical Curved Area graph, categories are shown on the x-axis and values on the y-axis, and lines curved. As with other standard Area graphs, all series begin from the baseline, so overlapping of series occurs. However, you can see individual series by mousing over series items in the graph or legend.\nThe first example at right shows a vertical Curved Area graph in its default state. Seven series are present, but only three are visible in the graph because of overlapping. In the second example, the user is mousing over the LinkedIn series, so only that series appears in the graph.\nHorizontal Curved Area graph", "source": "../../raw_kb/article/available_area_charts/index.html", "title": "Available Area Charts"}, {"objectID": "cc679c80bb59-3", "text": "Horizontal Curved Area graph\nA horizontal Curved Area graph is the same as a vertical Curved Area graph except that the categories are shown on the y-axis and the values on the x-axis.\nThe first example at right shows a horizontal Curved Area graph in its default state. Seven series are present, but only three are visible in the graph because of overlapping. In the second example, the user is mousing over the LinkedIn series, so only that series appears in the graph.\nVertical Step Area graph\nIn a vertical Step Area graph, categories are shown on the x-axis and values on the y-axis, and lines are \"stepped\" throughout the graph. As with other standard Area graphs, all series begin from the baseline, so overlapping of series occurs. However, you can see individual series by mousing over series items in the graph or legend.\nThe first example at right shows a vertical Step Area graph in its default state. Seven series are present, but only three are visible in the graph because of overlapping. In the second example, the user is mousing over the LinkedIn series, so only that series appears in the graph.\nHorizontal Step Area graph\nA horizontal Step Area graph is the same as a vertical Step Area graph except that the categories are shown on the y-axis and the values on the x-axis.\nThe first example at right shows a horizontal Step Area graph in its default state. Seven series are present, but only three are visible in the graph because of overlapping. In the second example, the user is mousing over the LinkedIn series, so only that series appears in the graph.", "source": "../../raw_kb/article/available_area_charts/index.html", "title": "Available Area Charts"}, {"objectID": "cc679c80bb59-4", "text": "Stacked Area Graphs\nA Stacked Area graph is a multi-line Line graph in which the space under each line is filled in with a different color, creating colored \"segments\" that are stacked on top of each other, as in a vertical Stacked Bar graph. The total size of the area for each segment reflects the cumulative value of all data items in that category. These graphs are useful for showing group trends and comparing trends between data series.\nFor information about building Stacked Area graphs, see Stacked Area Graph.\nChart TypeDescriptionExampleVertical Stacked Area graph\nIn a vertical Stacked Area graph, categories are shown on the x-axis and values on the y-axis, and lines are straight instead of curved or stepped.Horizontal Stacked Area graph\nA horizontal Stacked Area graph is the same as a vertical Stacked Area graph except that the categories are shown on the y-axis and the values on the x-axis.Vertical Curved Stacked Area graph\nA vertical Curved Stacked Area graph is the same as a regular Vertical Stacked Area graph except that the lines are curved instead of straight.Horizontal Curved Stacked Area graph\nA horizontal Curved Stacked Area graph is the same as a regular horizontal Stacked Area graph except that the lines are curved instead of straight.Vertical Step Stacked Area graph\nA vertical Step Stacked Area graph is the same as a regular Vertical Stacked Area graph except that the lines are \"stepped.\"Horizontal Step Stacked Area graph\nA horizontal Step Stacked Area graph is the same as a regular horizontal Stacked Area graph except that the lines are \"stepped.\"Vertical 100% Stacked Area graph\nIn a 100% Stacked Area graph, segments are shown as a percentage of 100, similar to a Pie graph. In the standard vertical version, categories are shown on the x-axis and values on the y-axis, and lines are straight instead of curved or stepped.Horizontal 100% Stacked Area graph", "source": "../../raw_kb/article/available_area_charts/index.html", "title": "Available Area Charts"}, {"objectID": "cc679c80bb59-5", "text": "This graph type is the same as a vertical 100% Stacked Area graph except that categories are shown on the y-axis and values on the x-axis.Vertical Curved 100% Stacked Area graph\nThis graph type is the same as a standard vertical 100% Stacked Area graph except that the lines are curved instead of straight.Horizontal Curved 100% Stacked Area graph\nThis graph type is the same as a standard horizontal 100% Stacked Area graph except that the lines are curved instead of straight.Vertical Step 100% Stacked Area graph\nThis graph type is the same as a standard vertical 100% Stacked Area graph except that the lines are \"stepped\" instead of straight.Horizontal Step 100% Stacked Area graph\nThis graph type is the same as a standard horizontal 100% Stacked Area graph except that the lines are \"stepped\" instead of straight.\nStream Graph\nUnlike the other types of Area graphs in Domo, there is only one type of Stream graph. This type of graph is most similar to the\u00a0Stream Funnel graph (classified in Domo\u00a0with the Pie-type graphs) but is horizontal instead of vertical.\nChart TypeDescriptionExampleStream graph\nA Stream graph is a cross between a horizontal Area graph and a Funnel graph.\u00a0Series in a Stream graph are centered. They do not share the same baseline and do not overlap. Thus they are better than standard Area and Stacked Area graphs at showing changes between stages.", "source": "../../raw_kb/article/available_area_charts/index.html", "title": "Available Area Charts"}, {"objectID": "87617c36d46c-0", "text": "Title\n\nAvailable Bar Charts\n\nArticle Body", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-1", "text": "Bar graphs are used to measure quantities or amounts. There are usually two axes, one vertical and one horizontal. One of these axes generally displays the items being measured, and the other displays a numerical scale for measuring the items. The lengths of the bars represent the quantities for the items.\nThe following tables list the types of Bar graphs available in Domo. You can click a thumbnail image to see a larger image.\nVertical Bar Graphs\nChart TypeDescriptionExampleVertical Bar graph\nIn a vertical Bar graph, the most common type of bar graph, the categories are represented along the x-axis, or horizontal axis, and the values are represented along the y-axis, or vertical axis.\nFor more information, see Bar Chart.Vertical Grouped Bar\u00a0graph\nA vertical Grouped Bar graph is a type of bar graph in which multiple sets of data items are compared, with a single color used to denote a specific series across all the sets. In this way you can compare more information in a single graph.\nFor more information, see Grouped Bar Chart.Vertical Grouped Bar with Line graph\nA Grouped Bar with Line graph is a combination of a Line graph and a Grouped Bar graph. It includes two y-axes, one on each side of the graph. One of these is used to measure the values along the line, and the other is used to measure the values of the bars. By default, the first data item appears as a line and all the others appear as bars; however, you can change this in Chart Properties. This type of graph is useful in situations in which you need to show a trend along with the specific\u00a0quantities or amounts associated with that trend.\nFor more information, see Grouped Bar with Line Chart.Vertical Line with Grouped Bar graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-2", "text": "For more information, see Grouped Bar with Line Chart.Vertical Line with Grouped Bar graph\nThis graph type is exactly the same as a Grouped Bar with Line\u00a0graph except that by default, the first data item appears as a bar and all of the others appear as lines. As with a Grouped Bar with Line graph, you can change this default behavior in Chart Properties.\nFor more information, see Grouped Bar with Line Chart.Vertical Grouped Bar with Curved Line graph\nThis graph type is exactly the same as a Grouped Bar with Line graph except that the line segments are curved instead of straight.\u00a0\nFor more information, see Grouped Bar with Line Chart.Vertical Grouped Bar with Symbols graph\nA Grouped Bar with Symbols graph is the same as a Grouped Bar with Line graph, except that it includes symbols instead of lines.\nFor more information, see Grouped Bar with Line Chart.Vertical Stacked Bar graph\nIn a vertical Stacked Bar graph, categories are represented as bars, as in a basic Bar graph, but the bars are composed of series that are \"stacked\" on top of each other, with each series representing its value.\nFor more information, see Stacked Bar Graph.Vertical Stacked Bar with Line graph\nA Stacked Bar with Line graph combines the features of a Line graph and a vertical Stacked Bar graph. There are two y-axes, one on the left and the other on the right. The y-axis on the left measures the values along the line, and the one on the right measures the values of the bars.\nFor more information, see Stacked Bar with Line Graph.Vertical Stacked Bar with Curved Line graph\nThis graph type is exactly the same as a Stacked Bar with Line graph except that the line segments are curved instead of straight.\u00a0\nFor more information, see Stacked Bar with Line Graph.Vertical Stacked Bar with Symbols graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-3", "text": "For more information, see Stacked Bar with Line Graph.Vertical Stacked Bar with Symbols graph\nA Stacked Bar with Symbols graph is the same as a Stacked Bar with Line graph, except that it includes symbols instead of lines.\nFor more information, see Stacked Bar with Line Graph.Vertical Nested Bar graph\nIn a vertical Nested Bar graph, the total values for all categories are represented as vertical gray bars, and the values for all series are represented as smaller, colored bars and are shown side-by-side in the bars for their respective categories.\nFor more information, see Nested Bar Chart.Vertical Nested Bar with Line graph\nThis graph is a combination of a Grouped Bar with Line graph and Vertical Nested Bar graph. Total values for all categories are represented as vertical gray bars. Of the series, the first series in the legend is represented as a line and all of the other series are represented as smaller, colored bars within their respective category bars. The line and gray total bars are measured on the left vertical axis, and the colored series bars are measured on the right axis.\u00a0\nFor more information, see Nested Bar with Line\u00a0Chart. \u00a0Vertical 100% Stacked Bar graph\nA vertical 100% Stacked Bar graph is a vertical Stacked Bar graph in which each series in a category represents a percentage of that category, similar to a Pie-type graph.\nFor more information, see 100% Stacked Bar Chart.Vertical 100% Stacked Bar with Line graph\nA 100% Stacked Bar with Line graph is the same as a 100% Stacked Bar graph in that each series in a percentage stack represents a percentage of that category. It also includes one or more trendlines, similar to our other bar-line combination graphs.\nFor more information, see\u00a0100% Stacked Bar with Line Chart.Vertical Percent of Total graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-4", "text": "For more information, see\u00a0100% Stacked Bar with Line Chart.Vertical Percent of Total graph\nA vertical Percent of Total graph is a vertical Bar graph with a percent scale instead of a value scale\u2014each category bar represents the relative size of its value as a percentage rather than the actual value. Percent of Total graphs are essentially Pie graphs with bars; however, because no legend is required to match names with categories, these graphs are actually easier to read and interpret than Pie-type graphs.\nFor more information, see Percent of Total Graph.Vertical Marimekko graph\nA Marimekko graph is somewhat similar to a 100% Stacked Bar graph in that items within categories represent a percentage of a category, as in a Pie-type graph. However, it is different in that it represents values through column width as well as height, making it easier to see differences in the values of different items.\nFor more information, see Marimekko Chart.Pareto graph\nA Pareto graph is a bar graph with categories ordered according to the size of their data item (largest to smallest). A line above the bars shows the cumulative value of all of the data items to that point. A Pareto graph has two scales\u2014one on the left that measures the actual data values, and one on the right that measures the percentage of the total value of the data series. These graphs are useful for discerning the impact of certain categories of data on a data series, and they allow you to see those categories that most (or least) contributed to the entire series.\nFor more information, see Pareto Graph.Vertical Grouped and Stacked Bar graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-5", "text": "For more information, see Pareto Graph.Vertical Grouped and Stacked Bar graph\nGrouped and Stacked Bar graphs combine the features of Grouped Bar and Stacked Bar graphs. In a Grouped and Stacked Bar graph, the first two series of a category are shown side by side, and all additional series are shown as segments in one or both bars. This kind of chart is useful in situations when you want to compare a primary series (such as a series representing sales data for your company headquarters) against several secondary series (such as those series representing sales data for branch offices). In vertical Grouped and Stacked Bar graphs, categories appear on the horizontal axis (or x-axis) and values are measured along the vertical axis (or y-axis).\nFor more information, see Grouped and Stacked Bar Chart.Vertical Grouped and Stacked Bar with Line graph\nA Grouped and Stacked Bar with Line graph is a combination of a vertical Grouped and Stacked Bar graph and a Stacked Bar with Line graph. As in a Stacked Bar with Line graph, two value scales are present and series can appear as either lines or bars; this chart type also gives you the ability to divide series segments between bars, as in a Grouped and Stacked Bar graph. As with standard Grouped and Stacked Bar graphs, this chart type is useful when you want to compare a primary series against several secondary series; however, because it includes a trendline, it is more compatible with time-series data than a regular Grouped and Stacked Bar graph.\nFor more information, see Grouped and Stacked Bar with Line Chart.Vertical Bullet graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-6", "text": "For more information, see Grouped and Stacked Bar with Line Chart.Vertical Bullet graph\nBullet graphs are used to depict progress towards a goal, and contain two or more ranges (for example, \"poor,\" \"average,\" and \"excellent\"). Unlike other kinds of gauges, which represent a single data value, Bullet graphs can measure multiple data values in a smaller space, providing a richer and more meaningful viewing experience. In Vertical Bullet graphs, categories appear on the horizontal axis (or x-axis) and the category values are measured along the vertical axis (or y-axis).\nFor more information, see Bullet Chart.Vertical Waterfall graph\nWaterfall graphs are essentially Bar graphs that show a series of profits and losses for a specified time period, together with a summary bar showing the amount remaining at the end of the time period. Profits appear as green bars, losses appear as red bars, and start and summary bars appear as blue bars. By default, Waterfall graphs display only value and category data, but you can show summary bars for individual series by checking a box in Chart Properties. In vertical Waterfall graphs, categories appear on the horizontal axis (or x-axis) and category values are measured along the vertical axis (or y-axis).\nFor more information, see Waterfall Graph.Vertical Running Total Bar graph\nIn a Running Total Bar graph, each successive category bar shows the cumulative value of all categories up to that point in the succession. In a Running Total Bar graph, each category contains only one series, so only one bar displays for each data point.\nFor more information, see Running Total Bar Graph.Vertical Running Total Grouped Bar graph\nIn a Running Total Grouped Bar graph, multiple series are present for two or more categories, and these series are shown side by side, as in a vertical Grouped Bar Graph. Each series bar shows the cumulative value for all series bars in that category up to that point.", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-7", "text": "For more information, see Running Total Grouped Bar Graph.Vertical Running Total Stacked Bar graph\nIn a Running Total Stacked Bar graph, multiple series are present for two or more categories, and these series are shown stacked on top of each other, as in a vertical Stacked Bar Graph. Each bar in a stack shows the cumulative value for all series bars in that category up to that point.\nFor more information, see Running Total Stacked Bar Graph.Vertical Overlay Bar graph\nA Vertical Overlay Bar is similar to a standard Stacked Bar graph except that instead of being stacked, all series items begin from the baseline, so overlapping of series items occurs in a category when multiple series are present. In its initial non-rollover state, an Overlay graph is rather meaningless, as shown in the first example at right. However, by mousing over a series item in the legend, you can see just that series in the chart, as shown in the second example at right. In many applications this may be more useful than a Stacked Bar because you can more quickly compare series items in different categories. (In a Stacked Bar this is difficult because series in different categories do not usually begin from the same baseline.)\nFor more information, see Overlay Bar Graph.\u00a0\n\u00a0\nVertical Histogram graph\nA Histogram is a chart type, usually used for statistical analysis, that represents the frequency at which certain ranges of values appear in a DataSet. For example, you could create a histogram showing the frequency at which specific height ranges appear in a group. In a vertical Histogram, values are represented along the x-axis as ranges, and categories are counted and represented numerically on the y-axis.\nFor more information, see Histogram.\nHorizontal Bar Graphs\nChart TypeDescriptionExampleHorizontal Bar graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-8", "text": "For more information, see Histogram.\nHorizontal Bar Graphs\nChart TypeDescriptionExampleHorizontal Bar graph\nA horizontal Bar graph has the same basic concept as a vertical Bar graph, but the x- and y-axes are reversed; that is, the categories are represented along the y-axis, or vertical axis, and the values are represented along the x-axis, or horizontal axis.\nFor more information, see Bar Chart.Horizontal Grouped Bar graph\nA horizontal Grouped Bar graph has the same basic concept as a vertical Grouped Bar graph, but the x- and y-axes are reversed, so that the categories are represented along the y-axis, or vertical axis, and the values are represented along the x-axis, or horizontal axis.\nFor more information, see Grouped Bar Chart.Horizontal Grouped Bar with Line graph\nA horizontal Grouped Bar with Line graph has the same basic concept as a vertical Grouped Bar with Line graph, but the x- and y-axes are reversed, so that the categories are represented along the y-axis, or vertical axis, and the values are represented along the x-axes, or horizontal axes. Bars are measured on the bottom x-axis and lines are measured on the top x-axis.\u00a0\nFor more information, see Grouped Bar with Line Chart.Horizontal Grouped Bar with Symbols graph\nA horizontal Grouped Bar with Symbols graph is the same as a horizontal Grouped Bar with Line graph but uses symbols instead of lines.\nFor more information, see Grouped Bar with Line Chart.Horizontal Stacked Bar graph\nA horizontal Stacked bar graph has the same basic concept as a vertical Stacked Bar graph, but the x- and y-axes are reversed, so the series appear side-by-side in the chart rather than stacked on top of each other.\nFor more information, see Stacked Bar Chart.Horizontal Stacked Bar with Line graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-9", "text": "For more information, see Stacked Bar Chart.Horizontal Stacked Bar with Line graph\nA horizontal Stacked Bar with Line graph has the same basic concept as a vertical Stacked Bar with Line graph, but the x- and y-axes are reversed, so that the categories are represented along the y-axis, or vertical axis, and the values are represented along the x-axis, or horizontal axis. Bars are measured on the bottom x-axis and lines are measured on the top x-axis.\u00a0\nFor more information, see Stacked Bar with Line Chart.Horizontal Stacked Bar with Symbols graph\nA horizontal Stacked Bar with Symbols graph is the same as a horizontal Stacked Bar with Line graph but uses symbols instead of lines.\nFor more information, see Stacked Bar with Line Chart.Horizontal Nested Bar graph\nA horizontal Nested Bar graph has the same basic concept as a vertical Nested Bar graph, but the x- and y- axes are reversed, so the bars extend to the right instead of vertically.\nFor more information, see Nested Bar Chart.Horizontal Nested Bar with Line graph\nA horizontal Nested Bar with Line graph has the same basic concept as a vertical Nested Bar with Line graph, but the x- and y- axes are reversed, so the bars extend to the right instead of vertically, and the line goes from top to bottom.\nFor more information, see Nested Bar with Line Chart.Horizontal Marimekko graph\nA horizontal Marimekko graph has the same basic concept as a vertical Marimekko graph but the categories are on the x-axis and the percent scale is on the y-axis, so the rectangles extend left to right instead of top to bottom.\nFor more information, see Marimekko Chart.Horizontal 100% Stacked Bar graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-10", "text": "For more information, see Marimekko Chart.Horizontal 100% Stacked Bar graph\nA horizontal 100% Stacked Bar graph has the same basic concept as a vertical 100% Stacked Bar graph, but the x- and y-axes are reversed, so the series appear side-by-side in the chart rather than stacked on top of each other.\nFor more information, see 100% Stacked Bar Chart.Horizontal 100% Stacked Bar with Line graph\nA 100% Stacked Bar with Line graph is the same as a 100% Stacked Bar graph in that each series in a percentage stack represents a percentage of that category. It also includes one or more trendlines, similar to our other bar-line combination graphs.\nFor more information, see\u00a0100% Stacked Bar with Line Chart.Horizontal Percent of Total graph\nA horizontal Percent of Total graph is the same as a vertical Percent of Total graph, but the x and y axes are reversed, so that the percent scale appears on the bottom and the categories on the left.\nFor more information, see Percent of Total Graph.Horizontal Grouped and Stacked Bar graph\nHorizontal Grouped and Stacked Bar graphs have the same basic concept as vertical Grouped and Stacked Bar graphs, but the x- and y-axes are switched, so that the categories appear on the vertical axis and the values are measured along the horizontal axis.\nFor more information, see Grouped and Stacked Bar Chart.Horizontal Grouped and Stacked with Line graph\nThis chart type is the same as a vertical Grouped and Stacked with Line graph, , but the x- and y-axes are switched, so that the categories appear on the vertical axis and the values are measured along the horizontal axes.\nFor more information, see Grouped and Stacked Bar with Line Chart.Horizontal Bullet graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-11", "text": "For more information, see Grouped and Stacked Bar with Line Chart.Horizontal Bullet graph\nHorizontal Bullet graphs have the same basic concept as vertical Bullet graphs, but the x- and y-axes are switched, so that the categories appear on the vertical axis and the values are measured along the horizontal axis.\nFor more information, see Bullet Chart.Horizontal Waterfall graph\nHorizontal Waterfall graphs have the same basic concept as vertical Waterfall graphs, but the x- and y-axes are switched, so that the categories appear on the vertical axis and the values are measured along the horizontal axis.\nFor more information, see Waterfall Graph.Horizontal Running Total Bar graph\nHorizontal Running Total Bar graphs are the same as vertical Running Total Bar graphs, but the x- and y-axes are switched, so that the categories appear on the vertical axis and the values are measured along the horizontal axis.\nFor more information, see Running Total Bar Graph.Horizontal Running Total Grouped Bar graph\nHorizontal Running Total Grouped Bar graphs are the same as vertical Running Total Grouped Bar graphs, but the x- and y-axes are switched, so that the categories appear on the vertical axis and the values are measured along the horizontal axis.\nFor more information, see Running Total Grouped Bar Graph.Horizontal Running Total Stacked Bar graph\nHorizontal Running Total Stacked Bar graphs are the same as vertical Running Total Stacked Bar graphs, but the x- and y-axes are switched, so that the categories appear on the vertical axis and the values are measured along the horizontal axis.\nFor more information, see Running Total Stacked Bar Graph.Funnel Bars graph", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "87617c36d46c-12", "text": "For more information, see Running Total Stacked Bar Graph.Funnel Bars graph\nA Funnel Bars graph is a combination of a Funnel graph and a horizontal Bar graph. This graph type shows differences between one stage in a process (such as a sales cycle) and the next. For each stage in the process, the data for the previous round drops off, so that only the relationships between the current round and all subsequent rounds is shown. In essence, this chart type shows \"what's left\" after each stage in a process. The example at right shows a Funnel Bars graph for a typical sales cycle. Each subsequent stage depicts the number of potential customers who have progressed to that stage.\nFor more information, see Funnel Chart.Horizontal Overlay Bar graph\nA horizontal Overlay Bar graph is the same as a vertical Overlay Bar graph, x- and y-axes are switched, so that the categories appear on the vertical axis and the values are measured along the horizontal axis.\nFor more information, see Overlay Bar Graph.Horizontal Histogram graph\nA horizontal Histogram is the same as a vertical Histogram, but the axes are reversed, so that category counts appear on the horizontal x-axis and value ranges appear on the vertical y-axis.\nFor more information, see Histogram.\nMiscellaneous\u00a0Bar Graphs\nThough technically a vertical Bar\u00a0graph, the Spark Bar graph is included here because it is vastly different in many respects from standard Bar graphs. This graph type is found in the\u00a0Other Charts\u00a0category in the Chart Picker.\u00a0\u00a0\u00a0\nChart TypeDescriptionExampleSpark Bar graph\nA Spark Bar graph is essentially a Bar\u00a0graph with no axes (providing a cleaner, uncluttered experience) and a built-in gauge\u00a0that by default shows the degree of change between the first and last data points in the graph. Spark Bar graphs are excellent for quickly showing relative rises and falls in data over time.\u00a0\nFor more information, see Spark Bar Graph.", "source": "../../raw_kb/article/available_bar_charts/index.html", "title": "Available Bar Charts"}, {"objectID": "cf252ec2225d-0", "text": "TitleAvailable Custom ConnectorsArticle BodyIntro\nAt Domo, our engineering team has developed hundreds of connectors but we understand the difficulties of developing for a new API platform. That's why we've created the Connector IDE, an intuitive environment for managing and creating connections between Domo and the web's most powerful APIs. If there isn't a connector built for an API you use in the AppStore, you now have the ability to build it.\u00a0\nFor documentation about building custom connectors in Domo, see\u00a0https://developer.domo.com/connect-to-domo.\u00a0\n\n\n\u00a0\n\nImportant: Due to these connectors being built by a third party, we do not support or document these connectors.\n\n\n\n\n\n\u00a0\n\nNote:\u00a0If you do not see a connector below in your instance and would like to add it, please contact your Customer Success Manager.", "source": "../../raw_kb/article/available_custom_connectors/index.html", "title": "Available Custom Connectors"}, {"objectID": "cf252ec2225d-1", "text": "Available third-party custom connectors\nList of all available third-party custom connectors. Links to their documentation are provided when available.", "source": "../../raw_kb/article/available_custom_connectors/index.html", "title": "Available Custom Connectors"}, {"objectID": "cf252ec2225d-2", "text": "ThumbnailNameDocumentationDialpad\u00a0https://www.dialpad.com/developers/docsAddigyhttps://addigy.freshdesk.com/support/solutions/articles/8000054029-api-documentationBroncos Newshttps://newsapi.org/s/nfl-news-apiChristopherson Business Travel\u00a0CIRCL CVE Searchhttps://cve.circl.lu/api/Connatix\u00a0Reportshttps://support.connatix.com/hc/en-us/categories/115000391345-APIs-DocumentationDelaconhttp://www.delacon.com.au/integrations/delacon-xml-api/Dovetalee-Stathttps://www.e-stat.go.jp/api/KnowBe4https://developer.knowbe4.com/Mododata\u00a0Oracle Sales Cloudhttps://docs.oracle.com/en/cloud/saas/sales/r13-update17d/faaps/toc.htmRaw Data Extractor\u00a0Restaurant 365https://help.restaurant365.net/support/solutions/articles/12000040999-r365-api-connectorStella Connecthttps://apidocs.stellaconnect.net/Talkwalkerhttps://www.talkwalker.com/user-manual/apiTrackMaven\u00a0USGS Well Datahttps://www.usgs.gov/products/data-and-tools/apisLynda.com\u00a0Critical Mentionhttp://www.criticalmention.com/api/Cryptocurrency\u00a0USGS Water Level Datahttps://www.usgs.gov/products/data-and-tools/apisMoodWire\u00a0Weather Sourcehttps://developer.weathersource.com/documentation/CAO Revenue\u00a0Jomablue Event Intelligence\u00a0Yexthttps://developer.yext.com/docs/Searchmetricshttps://www.searchmetrics.com/api/RXA Voice of CustomerNAAlpha Vantage Stock FX and Crypto", "source": "../../raw_kb/article/available_custom_connectors/index.html", "title": "Available Custom Connectors"}, {"objectID": "cf252ec2225d-3", "text": "Voice of CustomerNAAlpha Vantage Stock FX and Crypto Statshttps://www.alphavantage.co/documentation/Rhumbixhttps://platform.rhumbix.com/public_api/v2/docs/MarqiiNADigimind SocialNAFranConnecthttps://docs.franconnect.net/ClipcentricNANeo4jhttps://neo4j.com/docs/rest-docs/current/Pepperjamhttps://support.pepperjam.com/s/publisher-api-documentationPiwik PROhttps://developers.piwik.pro/en/latest/custom_reports/http_api/http_api.htmlCosmos DBNAQuintlyhttps://api.quintly.com/SentinelOne Agent Detailshttps://www.novell.com/developer/plugin-sdk/ref/restapi/7.0/Ready SignalNAOmny Studiohttps://api.omnystudio.com/api-docs/indexFulcrumhttps://learn.fulcrumapp.com/dev/rest/introFulcrum Queryhttps://learn.fulcrumapp.com/dev/query/introFederal Reserve Economic Datahttps://fred.stlouisfed.org/docs/api/fred/overview.htmlZinier Generic QueryZinierCode Climatehttps://codeclimate.com/docs/apiPeakonhttps://help.peakon.com/en/Tubular Reportshttps://tubularlabs.comDash Hudson\u00a0NAAvidRatingsNAGSCSearchAnalyticshttps://developers.google.com/webmaster-toolsFaciliohttps://facilio.com/Docs/index.htmlKeyedInhttps://www.keyedin.com/ChatFunnelsNAYogiNAScraping HubNACatchpointNAFuel", "source": "../../raw_kb/article/available_custom_connectors/index.html", "title": "Available Custom Connectors"}, {"objectID": "cf252ec2225d-4", "text": "HubNACatchpointNAFuel APIhttps://api.nsw.gov.au/AgentboxNAGuideCXhttps://api.guidecx.com/api/v1/docspersoniohttps://developer.personio.de/reference#introductionUberallhttps://uberall.com/en/developers/apiDocumentation\u00a0Onboard.iohttps://docs.onboard.io/StackAdaptNASimpleNexusNATalent.com Campaign ReportNALogicGate Risk Cloudhttps://help.logicgate.com/hc/en-us/articles/4402674121492UKGREST API v2 \u2014 UKG HR Service Delivery 2.0.0 documentation (people-doc.com)Testimonial TreeAPI and Automation   - Testimonial Tree Help (zendesk.com)GainsightAPI and Developer Docs - Gainsight Inc.Twilio Writeback[API Reference - In-Depth Reference for all Twilio APIs and SDKs | Twilio|https://www.twilio.com/docs/api]Pleohttps://developers.pleo.io/reference/introductionBrexhttps://www.brex.com/product/api/Ambition Dataset Pusherhttps://help.ambition.com/hc/en-us/sections/360000677111-APIgBizINFO   https://info.gbiz.go.jp/api/index.html", "source": "../../raw_kb/article/available_custom_connectors/index.html", "title": "Available Custom Connectors"}, {"objectID": "cf252ec2225d-5", "text": "Service Desk Plushttps://www.manageengine.com/products/service-desk/sdpod-v3-api/SDPOD-V3-API.htmlSansanhttps://docs.ap.sansan.com/en/api/openapi/index.htmlSprinklrhttps://developer.sprinklr.com/freeeKB Articles:\u00a0\nJP: freee third party connector \u2013 Domo\nEN: freee Third Party Connector \u2013 Domo\n\u00a0Lojistic\u00a0Introduction \u2013 Lojistic APILINE MUSIC AnalyticsKB Article:\nEN: LINE MUSIC Analytics Connector \u2013 Domo", "source": "../../raw_kb/article/available_custom_connectors/index.html", "title": "Available Custom Connectors"}, {"objectID": "df55fad3285b-0", "text": "Title\n\nAvailable Data Science Charts\n\nArticle Body", "source": "../../raw_kb/article/available_data_science_charts/index.html", "title": "Available Data Science Charts"}, {"objectID": "df55fad3285b-1", "text": "Data science graphs enable you to perform in-depth statistical analyses on your Domo data.\nThe following table lists the types of Data Science graphs available in Domo. You can click a thumbnail image to see a larger image.\nChart TypeDescriptionExampleScatter Plot graph\nA standard Scatter Plot graph has two value scales, one on the vertical axis (or y-axis) and one on the horizontal axis (or x-axis). These values are treated as coordinates; for every x and y value pair, a single point is plotted on the graph. Points can be assigned into specific groups by including series data in the graph.\nYou can also create a Scatter Plot Time graph, in which you include timeline data instead of value data on the x-axis. In this case, all points are plotted on their appropriate date/time coordinates.\u00a0The position of a\u00a0point\u00a0for a given date/time\u00a0coordinate is still determined by its y coordinate value, however.\nThe first example shows a standard Scatter Plot graph. The second example shows a Scatter Plot Time graph.\nAs of our July 2017 release, the Scatter Plot graph has been updated and no longer aggregates data (which had the effect of lumping all data in a series into a single dot). If you want to use the old version of the graph, you can do so by selecting the Scatter Plot Legacy chart type.\nFor more information, see Scatter Plot Chart.Bubble graph (deprecated)", "source": "../../raw_kb/article/available_data_science_charts/index.html", "title": "Available Data Science Charts"}, {"objectID": "df55fad3285b-2", "text": "For more information, see Scatter Plot Chart.Bubble graph (deprecated)\nDeprecated Bubble graphs are similar to Scatter Plot graphs in that they have two value scales, and x and y coordinate pairs are plotted on the graph. Bubble graphs are more complex than Scatter Plot graphs in that they include an additional dimension\u2014bubble size. Thus they require an additional DataSet column containing size values for each bubble. In the example at right, the y-axis measures employee salaries and the x-axis measures the average number of employees receiving those salaries. In addition, the size of each bubble represents the total percentage of the payroll for each department.\nYou can also create a\u00a0Bubble Time graph, in which you include timeline data instead of value data on the x-axis. In this case, all\u00a0bubbles are plotted on their appropriate date/time coordinates.\u00a0The position of a\u00a0point\u00a0for a given date/time\u00a0coordinate is still determined by its y coordinate value, however.\nThe first example shows a standard Bubble graph. The second example shows a Bubble Time chart.\nAs of our July 2017 release, you are no longer required to include series data in a Bubble graph, though this option is still available. If you want to use the old version of the graph, you can do so by selecting the Bubble Legacy chart type.\nFor more information, see Bubble Chart.XY Line\nAn XY Line graph uses x and y coordinate pairs instead of date/time data to plot trendlines. Because of this mathematical plotting, there are not evenly spaced intervals between data points as there are in graphs that use date/time data; therefore, these graphs are useful for portraying trendlines with greater accuracy.", "source": "../../raw_kb/article/available_data_science_charts/index.html", "title": "Available Data Science Charts"}, {"objectID": "df55fad3285b-3", "text": "A basic XY line graph requires only two columns of data to draw the line, but you can also include a series column to show multiple lines. You can also add a line for a median value and specify lower and upper range values. Be aware that this chart type does not currently include algorithms to identify your median line and upper and lower ranges. You must identify these elements yourself in the DataSet you use to power the graph. However, you can use tools such as R and Python to help you identify these elements.\nThe screenshot shows an XY graph with a single trendline, median line, and upper and lower bounds set.\nFor more information, see XY Line Chart.Predictive Modeling\nA Predictive Modeling graph is essentially a Scatter Plot graph that includes a model fit line. If you want, you can also specify the upper and lower bounds of the model fit line.\nBe aware that this chart type does not currently include algorithms to identify your model fit line and upper and lower bounds. You must identify these elements yourself in the DataSet you use to power the graph. However, you can use tools such as R and Python to help you identify these elements.\nFor more information, see Predictive Modeling Chart.Forecasting\nA Forecasting graph consists of a basic trendline for all data up until the current date/time, along with a forecast line showing predicted changes beyond the current date/time. You can optionally indicate the lower and upper bounds of the forecast line. If you want, you can even include a second forecast line with its own upper and lower bounds.\u00a0\nBe aware that this chart type does not currently include algorithms to create forecast line(s) and upper and lower bounds. This data must already be present in the DataSet you are using to power the graph. However, you can use tools such as R and Python to create these columns in your data source before bringing them into Domo.\nFor more information, see Forecasting Chart.Outliers", "source": "../../raw_kb/article/available_data_science_charts/index.html", "title": "Available Data Science Charts"}, {"objectID": "df55fad3285b-4", "text": "For more information, see Forecasting Chart.Outliers\nAn Outliers graph is nearly identical to a basic Line graph but allows you to call out any points you deem to be outliers.\nBe aware that this chart type does not currently include algorithms to identify outlier points. You must identify these points yourself in the DataSet you use to power the graph. However, you can use tools such as R and Python to help you identify these outliers.\nFor more information, see Outliers Chart.Vertical Box Plot graph\nBox Plot graphs are commonly used to represent statistics and quality measurements. For any given category, at least five values are required in the DataSet. From these values, Domo derives a high value, a Q3 (Quartile 3) value, a median value, a Q2 (Quartile 2) value, and a low value. These values are plotted on the graph as a box and whisker plot, with the Q1, median, and Q3 values forming the box and the high and low values forming the \"whiskers.\"\nIn a vertical Box Plot graph, values are represented on the vertical axis (or y-axis) and categories are represented on the horizontal axis (or x-axis) so the boxes stretch from top to bottom.\nFor more information, see Box Plot Chart.Horizontal Box Plot graph\nHorizontal Box Plot graphs are the same as vertical Box Plot graphs, except that categories are represented on the vertical axis (or y-axis) and values are represented on the horizontal axis (or x-axis), so the boxes stretch from left to right rather than top to bottom.\nFor more information, see Box Plot Chart.SPC\u00a0(statistical process control) chart", "source": "../../raw_kb/article/available_data_science_charts/index.html", "title": "Available Data Science Charts"}, {"objectID": "df55fad3285b-5", "text": "For more information, see Box Plot Chart.SPC\u00a0(statistical process control) chart\nSPC\u00a0(statistical\u00a0process control) charts, also known as control charts,\u00a0Shewhart charts,\u00a0or\u00a0process-behavior charts, are line charts\u00a0used to determine if a\u00a0manufacturing\u00a0or\u00a0business process\u00a0is in a state of\u00a0control. Domo's\u00a0SPC charts lets you set up rules from SPC\u00a0standards by configuring\u00a0them in the Chart Properties. When values outside the specified rules\u00a0are encountered, these are flagged in the chart as outliers.\n8 different rules are available. You can implement the rules singly or in combination.\u00a0\nFor more information, see\u00a0Building SPC Charts.", "source": "../../raw_kb/article/available_data_science_charts/index.html", "title": "Available Data Science Charts"}, {"objectID": "4c28836d51ea-0", "text": "Title\n\nAvailable Filter Charts\n\nArticle Body\n\nVideo - Filter Chart Types", "source": "../../raw_kb/article/available_filter_charts/index.html", "title": "Available Filter Charts"}, {"objectID": "4c28836d51ea-1", "text": "The following table lists the types of Filter charts available in Domo.\nChart TypeDescriptionExampleSlicer Card\nSlicer Cards display\u00a0values from a selected DataSet column. When you add them\u00a0to a Card Page in Domo, users visiting that Page can click on one or more values to filter the other Cards in the Page. For example, you could build a Slicer Card with names of U.S. states and add it to a Card Page. If you then clicked \"Texas\" and \"Oklahoma,\" all of the Cards in the Page would be filtered to show only data for those states.\nFor more information about creating Slicer Cards, see Applying Page-Level Filters.Checkbox Card\nCheckbox Cards are almost exactly the same as Slicer Cards (see above) but use checkboxes instead of buttons for filtering. As with Slicer Cards, you can filter on multiple values when you use Checkbox Cards.\u00a0\nFor more information about creating Slicer Cards, see Applying Page-Level Filters.\u00a0 \u00a0Radio Button Card\nRadio Button Cards are similar to Slicer and Checkbox Cards (see above). The key difference is that while Slicer and Checkbox Cards allow you to set multiple filters at once, with Radio Button Cards only one filter may be set at a time.\nFor more information about creating Slicer Cards, see Applying Page-Level Filters.\u00a0 \u00a0Range Card\nRange Cards display a slider with a minimum and maximum value. When you add a range Card to a Card Page in Domo, users visiting that Page can use the slider buttons to narrow the range between the minimum and maximum values.\u00a0All other Cards in the Page then filter to show the data for the newly selected range.\u00a0\nFor more information about creating Range Cards, see Applying Page-Level Filters.\u00a0 \u00a0Date Selector Card", "source": "../../raw_kb/article/available_filter_charts/index.html", "title": "Available Filter Charts"}, {"objectID": "4c28836d51ea-2", "text": "For more information about creating Range Cards, see Applying Page-Level Filters.\u00a0 \u00a0Date Selector Card\nDate Selector\u00a0Cards display a calendar-style range of dates. When you add them\u00a0to a Card Page in Domo, users visiting that Page can select a range of dates from the calendar as a filter. Alternatively, they can choose from a number of preset date filters such as \"Last year,\" \"Last 2 months,\" etc. The date filter you choose is applied to all other Cards on the Page. For example, if you selected all dates from April 1st to July 31st on the calendar, all other Cards in the Page would be filtered to show just the data for these dates.\u00a0\nFor more information about creating Date Selector Cards, see Applying Page-Level Filters.\u00a0 \u00a0\u00a0Dropdown Selector CardDropdown Selector Cards display values from a selected DataSet column. When you add them to a Dashboard in Domo, users visiting that Dashboard can click on values to filter the other Cards on the Dashboard. You can set up the Dropdown Selector Card to filter on a single value or multiple values at once.\nFor more information about creating Dropdown Selector Cards, see Applying Page-Level Filters.", "source": "../../raw_kb/article/available_filter_charts/index.html", "title": "Available Filter Charts"}, {"objectID": "4fc2a40407a2-0", "text": "Title\n\nAvailable Gauges\n\nArticle Body\n\nGauges classify data values as belonging to a predetermined range. An example would be a bulb that turns green or red, depending on the air quality.\nThe following table lists the types of gauges available in Domo:\n\u00a0\nGauge TypeDescriptionExampleSingle Value\u00a0gauge\nA Single Value gauge displays the first value in your DataSet as a numeral. No other graphics or images are used. This chart type is useful when you want to see a single value quickly without having to interpret bars, lines, etc.\nFor more information, see Single Value Gauge.\n\u00a0Filled gauge\nFilled gauges are circular gauges that \"fill\" to indicate the current value of the gauge, like a thermometer.\nFor more information, see Filled Gauge.Progress Bar gauge", "source": "../../raw_kb/article/available_gauges/index.html", "title": "Available Gauges"}, {"objectID": "4fc2a40407a2-1", "text": "A Progress Bar gauge is similar to a Filled gauge but is bar-shaped rather than circular and shows its value as a percentage.\nFor more information, see Progress Bar Gauge.Radial gauge\nRadial gauges use an indicator that pivots around the gauge scale to indicate the current data value associated with the gauge. Common Radial gauges include dials and speedometers.\nFor more information, see Radial Gauge.Face gauge\nIn a Face gauge, any of three different versions of a face (smiling, happy, or neither) is shown, depending on where the current value falls. This can be used to instantly classify a value as poor, fair, or good.\nFor more information, see Face Gauge.Comparative gauge\nA Comparative gauge is a simple gauge consisting of a single value that shows the degree of change between a previous value and a current value. The degree of change may be shown as a percentage or as the actual difference between values. Alternatively, you may choose to show only the current value.\nFor more information, see\u00a0Comparative Gauge.Comparative Fill gauge (Basic)\nA basic Comparative Fill Gauge is similar to a Filled Gauge but includes a\u00a0parameter for a previous value.\u00a0The gauge\u00a0fills in only between this previous value and the current value so viewers can see the degree of change since the gauge was last updated. In addition,\u00a0the percentage and direction of change\u00a0appears in the center of the chart. This gauge type also includes a parameter for a target value, but this is optional.\nThis\u00a0gauge type is the same as an advanced Comparative Fill Gauge but is simpler, requiring only four columns (with an optional fifth), whereas the\u00a0advanced\u00a0version requires eight.\nFor more information about\u00a0Comparative Fill Gauges, see\u00a0Comparative Fill Gauge.Comparative Fill gauge (Advanced)", "source": "../../raw_kb/article/available_gauges/index.html", "title": "Available Gauges"}, {"objectID": "4fc2a40407a2-2", "text": "An advanced Comparative Fill Gauge has the same features as\u00a0the basic version but also allows you to\u00a0create a lower and upper range and\u00a0specify a median value.\u00a0These elements are displayed\u00a0on a second circle in the gauge for a cleaner look.\u00a0\nFor more information about Comparative Fill Gauges, see\u00a0Comparative Fill Gauge.\u00a0Radial Progress gauge\nFilled gauges are circular gauges that \"fill\" to indicate the current value of the gauge, like a thermometer. For more information, see Radial Progress Gauge.Shape gauge\nA shape gauge uses any of 45 different images to indicate a current value. By using different images to indicate different ranges, you can convey to users whether the current value is positive, neutral, or negative. For more information, see Shape Gauge.Multi Radial Progress gauge\nFilled gauges are circular gauges that \"fill\" to indicate the current value of the gauge, like a thermometer. The Multi Radial Gauge allows for displaying for more than one metric. For more information, see Multi Radial Progress Gauge.Multi-Value gauge\n\u00a0A multi-value gauge displays the current value of your as DataSet\u00a0as a numeral, along with the\u00a0degree of change for the gauge data, whether positive or negative.\u00a0This chart type is useful when you want to see a change in value quickly, without having to interpret bars, lines, etc. For more information, see Multi-Value Gauge.Waffle gauge\nWaffle charts provide a new, simple and intuitive way to visualize data as it includes many customizable settings to tell a more informative story. For more information, see Waffle Gauge.", "source": "../../raw_kb/article/available_gauges/index.html", "title": "Available Gauges"}, {"objectID": "8ef4a6781c40-0", "text": "Title\n\nAvailable Line Charts\n\nArticle Body", "source": "../../raw_kb/article/available_line_charts/index.html", "title": "Available Line Charts"}, {"objectID": "8ef4a6781c40-1", "text": "Line charts are most often used to represent trends over time. Data items in a line chart are represented by points, and these are connected by a line. Some types of line charts, such as grouped bar with line charts, combine lines with elements found in other types of charts.\nThe following tables list the types of line charts available in Domo. You can click a thumbnail image to see a larger image.\nHorizontal line charts\nLine charts are usually displayed with the line running left to right, with the value scale on the vertical axis and the category scale on the horizontal axis. There are 12 such chart types in Domo (counting 5 line-bar combination-type charts, which in the Chart Picker in Domo are found under the Bar charts section).\nChart TypeDescriptionExampleLine chart\nBasic line charts use a single trendline to display data.\nFor more information, see Line chart.Symbol line chart\nA symbol line chart is a sub-type of the basic line chart in which symbols are included at each data point.\nFor more information, see Line chart.Curved line chart\nA curved line chart is a sub-type of the basic line chart in which points are connected by curved lines instead of straight lines.\nFor more information, see Line chart.Curved symbol line chart\nA curved symbol line chart is the same as a curved line chart except that it includes symbols at each data point.\nFor more information, see Line chart.Step line\nA step line is a sub-type of the basic line chart in which step lines are used instead of straight lines.\nFor more information, see Line chart.Multi-line chart\nA multi-line chart is a basic line chart with one or more additional lines that represent comparison trends.\nFor more information, see Multi-line chart.Grouped bar with line chart", "source": "../../raw_kb/article/available_line_charts/index.html", "title": "Available Line Charts"}, {"objectID": "8ef4a6781c40-2", "text": "For more information, see Multi-line chart.Grouped bar with line chart\nA grouped bar with line chart is a combination of a line chart and a grouped bar chart. It includes two y-axes, one on each side of the chart. One of these is used to measure the values along the line, and the other is used to measure the values of the bars. This type of chart is useful in situations in which you need to show a trend along with the specific\u00a0quantities or amounts associated with that trend.\nFor more information, see Grouped Bar with Line chart.Grouped bar with symbols chart", "source": "../../raw_kb/article/available_line_charts/index.html", "title": "Available Line Charts"}, {"objectID": "8ef4a6781c40-3", "text": "A grouped\u00a0bar with symbols chart is the same as a grouped bar with line chart, except that it includes symbols instead of lines.\nFor more information, see Grouped Bar with Line chart.Stacked bar with line chart\nA stacked bar with line chart combines the features of a line chart and a vertical stacked bar chart. There are two y-axes, one on the left and the other on the right. The y-axis on the left measures the values along the line, and the one on the right measures the values of the bars.\nFor more information, see Stacked Bar with Line chart.Stacked bar with symbols chart\nA stacked bar with symbols chart is the same as a stacked bar with line chart, except that it includes symbols instead of lines.\nFor more information, see Stacked Bar with Line chart.Grouped and stacked bar with line chart\nA grouped and stacked bar with line chart is a combination of a vertical grouped and stacked bar chart and a stacked bar with line chart. As in a stacked bar with line chart, two value scales are present and series can appear as either lines or bars; this chart type also gives you the ability to divide series segments between bars, as in a grouped and stacked bar chart. As with standard grouped and stacked bar charts, this chart type is useful when you want to compare a primary series against several secondary series; however, because it includes a trendline, it is more compatible with time-series data than a regular grouped and stacked bar chart.\nFor more information, see\u00a0Grouped and Stacked Bar with Line chart.Running total line chart\nIn a running total line chart, each data point shows the cumulative value of all data points up to that point. If more than one series is present, each series displays as its own individual line.\nFor more information, see Running Total Line chart.\nVertical line charts", "source": "../../raw_kb/article/available_line_charts/index.html", "title": "Available Line Charts"}, {"objectID": "8ef4a6781c40-4", "text": "For more information, see Running Total Line chart.\nVertical line charts\nLess commonly, line charts can be displayed with values on the horizontal axis and categories on the vertical axis. If the chart exceeds the vertical confines of the display area, scroll bars appear so users can scroll to see more of the chart. There are 5 types of vertical line charts in Domo.\nChart TypeDescriptionExampleVertical line chart\nA vertical line chart includes one or more lines that represent comparison trends. The chart in the first example includes only one trendline so the chart appears solid blue. The chart in the second example includes multiple lines.\nFor more information, see Line chart.\nVertical curved line chart\nA vertical curved line chart is a sub-type of the basic vertical line chart in which points are connected by curved lines instead of straight lines.\nFor more information, see Line chart.Vertical step line chart\nA vertical step line chart is a sub-type of the basic line chart in which step lines are used instead of straight lines.\nFor more information, see Line chart.Vertical symbol line chart\nA vertical symbol line chart is a sub-type of the basic vertical line chart in which symbols are included at each data point.\nFor more information, see Line chart.Vertical curved symbol line chart\nA vertical curved symbol line chart is the same as a vertical curved line chart except that it includes symbols at each data point.\nFor more information, see Line chart.\nMiscellaneous\u00a0line charts\nThough technically a horizontal line chart, the spark line chart is included here because it is vastly different in many respects from standard line charts. This chart type is found in the\u00a0Other Charts\u00a0category in the Chart Picker.\u00a0\u00a0\u00a0\nChart TypeDescriptionExampleSpark line chart", "source": "../../raw_kb/article/available_line_charts/index.html", "title": "Available Line Charts"}, {"objectID": "8ef4a6781c40-5", "text": "Chart TypeDescriptionExampleSpark line chart\nA spark line chart is essentially a line chart with no axes (providing a cleaner, uncluttered experience) and a built-in gauge\u00a0that by default shows the degree of change between the first and last data points in the chart. Spark line charts are excellent for quickly showing relative rises and falls in data over time.\u00a0\nFor more information, see Spark line chart.", "source": "../../raw_kb/article/available_line_charts/index.html", "title": "Available Line Charts"}, {"objectID": "b0971f47d109-0", "text": "Title\n\nAvailable Lollipop Charts\n\nArticle Body\n\nLollipop graphs are essentially modified Bar graphs in which the bars are represented as thin lines so you can fit more in a small area, with small circles to represent mouse-over points. There are eight types of Lollipop graphs in Domo, all of them modified versions of familiar Bar and Line Bar graphs. For more information about these graphs, see Lollipop Chart.\nChart TypeEquivalent Bar GraphExampleGrouped Lollipop\nGrouped BarStacked Lollipop\nStacked BarGrouped Lollipop with Line\nGrouped Bar with LineStacked Lollipop with Line\nStacked Bar with LineHorizontal Grouped Lollipop\nHorizontal Grouped BarHorizontal Stacked Lollipop\nHorizontal Stacked BarHorizontal Grouped Lollipop with Line\nHorizontal Grouped Bar with LineHorizontal Stacked Lollipop with Line\nHorizontal Stacked Bar with Line", "source": "../../raw_kb/article/available_lollipop_charts/index.html", "title": "Available Lollipop Charts"}, {"objectID": "cac7e78ea303-0", "text": "Title\n\nAvailable Maps\n\nArticle Body\n\nIn Domo, maps are used to represent numerical data across two or more regions. Maps are accompanied by legends so you can determine the range for each state or country. Various color themes are available for maps; you can change the existing color theme. For more information, see\u00a0Chart Properties.\nVideo - Map Charts", "source": "../../raw_kb/article/available_maps/index.html", "title": "Available Maps"}, {"objectID": "cac7e78ea303-1", "text": "The following table lists the types of Maps available in Domo. You can click a thumbnail image to see a larger image.\nMap TypeDescriptionExampleCountry map\n\u00a0Represents data for states, provinces, subdivisions, and prefectures in any of a number of different countries. Regions in a country may be represented using two-letter abbreviations, full names, or both, depending on the country.\nFor more information, see Country Map.Continental or regional mapRepresents data for countries within a specific continent or region. Countries in a continent or region may be represented using two-letter ISO 3166-1-alpha-2 codes or full names.\nFor more information, see Continental/Regional Map.World map\nRepresents data for world nations. Nations in your DataSet are represented using ISO 3166-1-alpha-2 codes. For a current list of countries and their corresponding codes, see country codes.\nFor more information, see World Map.Latitude-longitude map\nDisplays data for any region in the world using latitude and longitude coordinates. The region that displays for a DataSet is \"zoomed\" depending on the given coordinates. For example, if all of the cities in your DataSet were in Germany, the resulting map would show Germany; likewise, if some cities in the DataSet were in Germany and others were in France, the map would be \"zoomed out\" to show western Europe. Filters you apply to a latitude/longitude map also work this way. For example, if you were in the Details view for a map of the United States and you used Analyzer to show only cities in Washington and California, your map would be updated to show the westmost region of the United States. For more information about applying filters, see\u00a0Adding Filters to Your Chart.\nFor more information about latitude/longitude maps, see\u00a0Latitude-Longitude Map.Latitude-longitude route map", "source": "../../raw_kb/article/available_maps/index.html", "title": "Available Maps"}, {"objectID": "cac7e78ea303-2", "text": "A latitude-longitude map in which coordinate points are connected using lines. Two value columns are required; one of these is applied to the start point of a given route line and the other is applied to the end point. In the map, differences in the values of start and end points are represented by the thickness of the lines. In the example at right, all of the start points are set to 0 and the end points are set to the value of the product being shipped. In this way the viewer can see where their most valuable customers are located by looking at the thickness of the lines.In other respects, a latitude-longitude route map is the same as a latitude-longitude map. The provided coordinates and applied filters determine the \"zoom\" level of the map. You can also have multiple distinct routes by uploading an optional series column. For example, in the screenshot there are three distinct routes, one colored orange, one colored green, and one colored blue.\nFor more information about latitude-longitude route maps, see Latitude-Longitude Route Map.U.S. county map\nA U.S. county map shows data for U.S. counties at the country level. This type of map requires only two data columns\u2014one with values and the other with county FIPS codes.\nFor more information about U.S. county maps, see U.S. County Map.", "source": "../../raw_kb/article/available_maps/index.html", "title": "Available Maps"}, {"objectID": "193f1f82ef1b-0", "text": "Title\n\nAvailable Miscellaneous Charts\n\nArticle Body\n\nThe following table lists the types of miscellaneous charts available in Domo. You can click a thumbnail image to see a larger image. \u00a0\nChart TypeDescriptionExampleRadar chart", "source": "../../raw_kb/article/available_miscellaneous_charts/index.html", "title": "Available Miscellaneous Charts"}, {"objectID": "193f1f82ef1b-1", "text": "A radar chart is essentially a collection of line charts that encircle a centralized point (representing zero) on a polygonal grid. Each vertex of the grid pertains to a different category, and series are represented as individual colored polygons within the chart, with values plotted at the vertices of each polygon.\nRadar\u00a0charts are used when you want to compare aggregate values for two or more data series. In the example at right, the ACT scores for three job candidates are compared using a radar chart. You can see from the size and shape of each polygon that the candidates named \"Feldman\" and \"Murtz\" were strong in two categories and weak in the others, while the candidate named \"Hubbbard\" was strong in all four categories.\nFor more information, see Radar chart.\n\u00a0Word cloud\nA word cloud is a visual representation of text data, in which the importance or frequency of individual words is represented using font size and color. This format allows users to spot the most important or frequently used words in the DataSet. The example shows a word cloud showing top-searched terms for an electronics company. The size and color of each word represents the number of times it is used in\u00a0searches\u2014the more frequently a word was searched for, the larger and more brightly colored it is.\nFor more information, see Word Cloud Chart.Heat map\nA heat map is a special type of chart in which both the X and Y axes contain category data. Rectangle colors vary depending on how a particular data point fits into the specified data ranges. No series data is represented. Heat maps allow you to visualize relationships between data categories and draw attention to \"hot spots\" of activity and trends. In the example at right, the intensity of the blue color indicates the number of venomous snakes that were encountered in four Florida cities during a certain time period. The darkest blue areas appear for Miami, showing that it was a particular hotspot for snake activity.", "source": "../../raw_kb/article/available_miscellaneous_charts/index.html", "title": "Available Miscellaneous Charts"}, {"objectID": "193f1f82ef1b-2", "text": "For more information, see Heat Map.Calendar\nA calendar is used to show events associated with a series of dates. You can choose whether your calendar shows data for an entire year, for a month, or for a single day.\nFor more information, see Calendar.Vertical box plot chart\nBox plot charts are commonly used to represent statistics and quality measurements. For any given category, at least five values are required in the DataSet. From these values, Domo derives a high value, a Q3 (Quartile 3) value, a median value, a Q2 (Quartile 2) value, and a low value. These values are plotted on the chart as a box and whisker plot, with the Q1, median, and Q3 values forming the box and the high and low values forming the \"whiskers.\"\nIn a vertical box plot chart, values are represented on the vertical axis (or y-axis) and categories are represented on the horizontal axis (or x-axis) so the boxes stretch from top to bottom.\nFor more information, see Box Plot chart.Horizontal box plot chart\nHorizontal box plot charts are the same as vertical box plot charts, except that categories are represented on the vertical axis (or y-axis) and values are represented on the horizontal axis (or x-axis), so the boxes stretch from left to right rather than top to bottom.\nFor more information, see Box Plot chart.Gantt chart\nGantt charts are used to illustrate project schedules. Each task in your DataSet is represented as a horizontal bar with a start date and end date. If you want, you can break tasks down by project, with each project category shown in a separate color; however, this is optional.\nFor more information, see Gantt Chart.Gantt percent complete chart", "source": "../../raw_kb/article/available_miscellaneous_charts/index.html", "title": "Available Miscellaneous Charts"}, {"objectID": "193f1f82ef1b-3", "text": "For more information, see Gantt Chart.Gantt percent complete chart\nA Gantt percent complete chart is a sub-type of the standard Gantt chart. The only difference in a Gantt percent complete chart is that it requires an additional column containing the percent complete values for each task. In the chart, these values are represented by the amount of colored fill in each task bar. The amount of fill compared to the total length of the task bar gives the viewer an idea of the percentage complete for that task. For example, in the screenshot at right, the percent complete for \"Google+ profile\" is about 33%.\nFor more information, see Gantt Chart.High low chart\nA high low chart is used to represent stock data. For any given category, a vertical line is shown. The topmost point of the line represents the highest price for a stock and the bottommost point represents the lowest price for that stock.\nFor more information, see High Low chart.Candlestick chart\nA candlestick chart is used to represent stock data. This chart type is the same as a High Low chart but requires two additional dimensions. For any given category, a graphic is shown consisting of a box with lines protruding from the top and bottom. As in a high low chart, the ends of the lines represent the high and low prices for the stock respectively. The top and bottom boundaries of the box represent the opening and closing prices of the stock. If the opening price of the stock is lower than the closing price (that is, the stock gained value), the graphic appears green. If the opening price is higher than the closing price (that is, the stock decreased in value), the graphic appears red.\nFor more information, see Candlestick chart.Category scatter chart", "source": "../../raw_kb/article/available_miscellaneous_charts/index.html", "title": "Available Miscellaneous Charts"}, {"objectID": "193f1f82ef1b-4", "text": "For more information, see Candlestick chart.Category scatter chart\nA category scatter chart is similar to a scatter plot chart in that items are plotted using X-Y coordinates. However, whereas in a scatter plot chart two value scales are present, in a category scatter chart categories are listed down the y-axis, as in a horizontal bar chart, and values are measured on the x-axis. Series data is optional for a category scatter chart; however, no legend is provided\u2014you must mouse over an item to see its series. For more information, see Category Scatter chart.Vertical symbol chart\nSymbol charts are similar to grouped bar charts in that series items are represented side by side within their respective categories. However, instead of bars, symbol charts use symbols to indicate series values. This may help viewers to more easily identify highs and lows within their data. In a vertical symbol chart, categories are represented along the horizontal axis (or x-axis) and values are measured on the vertical axis (or y-axis).\nFor more information, see Symbol chart.Horizontal symbol chart", "source": "../../raw_kb/article/available_miscellaneous_charts/index.html", "title": "Available Miscellaneous Charts"}, {"objectID": "193f1f82ef1b-5", "text": "A horizontal symbol chart is the same as a vertical symbol chart except that the axes are switched; categories are measured on the vertical axis (or y-axis) and values are measured on the horizontal axis (or x-axis).\nFor more information, see Symbol chart.Vertical symbol overlay\nSymbol overlay charts are similar to standard symbol charts. However, where in a symbol chart symbols in categories are shown side by side (as in a grouped bar chart), in a symbol overlay chart symbols in categories are overlaid on top of one another (like the individual bars in a category in a stacked bar chart). As with a symbol chart, viewers can easily identify the highs and lows in each category. In a vertical symbol overlay chart, categories are represented along the horizontal axis (or x-axis) and values are measured on the vertical axis (or y-axis).\nFor more information, see Symbol Overlay chart.Horizontal symbol overlay\nA horizontal symbol overlay chart is the same as a vertical symbol overlay chart except that the axes are switched; categories are measured on the vertical axis (or y-axis) and values are measured on the horizontal axis (or x-axis).\nFor more information, see Symbol Overlay chart.Spark line chart\nA Spark line chart is essentially a line chart with no axes (providing a cleaner, uncluttered experience) and a built-in gauge\u00a0that by default shows the degree of change between the first and last data points in the chart. Spark line charts are excellent for quickly showing relative rises and falls in data over time.\u00a0\nFor more information, see Spark Line chart.Spark bar chart\nA Spark bar chart is essentially a bar\u00a0chart with no axes (providing a cleaner, uncluttered experience) and a built-in gauge\u00a0that by default shows the degree of change between the first and last data points in the chart. Spark bar charts are excellent for quickly showing relative rises and falls in data over time.", "source": "../../raw_kb/article/available_miscellaneous_charts/index.html", "title": "Available Miscellaneous Charts"}, {"objectID": "193f1f82ef1b-6", "text": "For more information, see Spark Bar chart.Sankey chart\nSankey\u00a0charts are a type of flow diagram in which the thickness of the connecting lines between elements is proportional to the flow rate. They are usually used in scientific applications, especially physics, in which they often show energy inputs, useful output, and wasted output. However, they are often used in business as well, especially in showing cost breakdowns, inventory flows, web traffic, and more.\u00a0\u00a0\nIn a Sankey chart, items are connected to other items using colored lines or arrows. Each pairing has a value. The thickness of any connecting line is determined by the value of the pairing in comparison to other values found in the dataset. For example, if point A was connected to point B and had a value of 10,000, and this value was among the greater values in the dataset, the line would appear thick. On the other hand, if the value of 10,000 was among the smaller values in the dataset, it would appear thin.\nSankey charts may simply connect one set of input items to one set of output items or may have intermediary items acting as both input and output items, depending on how your data is structured.\nFor more information, see\u00a0Sankey Chart.", "source": "../../raw_kb/article/available_miscellaneous_charts/index.html", "title": "Available Miscellaneous Charts"}, {"objectID": "037db5845da1-0", "text": "Title\n\nAvailable Period-over-Period Charts\n\nArticle Body", "source": "../../raw_kb/article/available_periodoverperiod_charts/index.html", "title": "Available Period-over-Period Charts"}, {"objectID": "037db5845da1-1", "text": "There are currently seven period-over-period chart types in Domo, all of which have a corresponding standard chart type. All period-over-period charts include only two fields, one for an x-axis column (which will always include time period data) and one for a y-axis column (which will always include value data). For information about applying columns to a chart, see\u00a0Applying DataSet Columns to Your Chart.\nThe following table lists and describes all of the available period-over-period charts. You can click a thumbnail image to see a full-sized version of that image. For information about building all of the charts in this table, see Period-over-Period Charts.", "source": "../../raw_kb/article/available_periodoverperiod_charts/index.html", "title": "Available Period-over-Period Charts"}, {"objectID": "037db5845da1-2", "text": "Period-over-Period Chart TypeDescriptionScreenshotBar LinePeriod being compared is shown as bars; all other periods are shown as lines.\u00a0Variance Bar LinePeriods are shown as bars and variance values are shown as lines.Line BarPeriod being compared is shown as a line; all other periods are shown as bars.Variance Line BarPeriods are shown as lines and variance values are shown as bars.TrendlineAll periods are shown as lines.Variance TrendlineAll periods and variance values are shown as lines.Grouped BarAll periods are shown as bars.Running Total LineAll periods are split into\u00a0running total lines.\u00a0Multi-Value GaugeShows the value for the selected date range, with the percent change from the selected comparison timeframe (e.g. the quarter-to-date value with the percent change from one quarter ago for comparison). For more information about these gauges, see\u00a0Multi-Value Gauge - BETA.Shape GaugeShows a different shape depending on your chart properties and the percent of change from the selected comparison timeframe compared to the selected date range. For more information about these gauges, see\u00a0Shape Gauge - BETA.Flex TableAll periods are shown as individual rows within the table, with their own spark line or spark bar chart, percent of change, etc. For more information about these tables, see\u00a0Flex Table.Filled GaugeGauge shows the value for the selected date range, while labels indicate this value along with the value for the selected comparison timeframe. For more information about these gauges, see\u00a0Filled Gauge.Progress BarGauge shows the percent change for the selected comparison timeframe. For more information about these gauges, see\u00a0Progress Bar Gauge.", "source": "../../raw_kb/article/available_periodoverperiod_charts/index.html", "title": "Available Period-over-Period Charts"}, {"objectID": "2af5930ae2ee-0", "text": "Title\n\nAvailable Pie-Type Charts\n\nArticle Body", "source": "../../raw_kb/article/available_pietype_charts/index.html", "title": "Available Pie-Type Charts"}, {"objectID": "2af5930ae2ee-1", "text": "A pie-type graph is a graph divided into sections that\u00a0usually add up to 100%. Each section represents the relative size of its corresponding value. Pie-type graphs are usually accompanied by legends so you can tell what each section represents. In Domo, legends are added automatically.\nWhen you are powering up a pie-type graph, the values in your values column do not need to be percentages, because Domo sums the values and converts each into the correct percentage of the total.\nThe following table lists the Pie-type graphs\u00a0available in Domo. You can click a thumbnail image to see a larger image.\nChart TypeDescriptionExamplePie graph\nThe most common of the pie-type graphs, a basic Pie graph is a circle divided into cone-shaped \"wedges,\" each of which represents a percentage of the overall graph.\nFor more information, see Pie Graph.Donut graph\nDonut graphs are almost identical to Pie graphs; the only difference is that they contain a hole in the middle, like a doughnut, so the sections are arcs of a circle rather than wedges.\nFor more information, see Donut Chart.Nautilus graph\nA Nautilus graph is the same as a standard Pie graph, except that the \"wedges\" are arranged in a logarithmic spiral and become smaller closer to the center of the spiral. This layout makes it easier to distinguish small differences between slices than it is with a regular Pie graph.\nFor more information, see Pie Graph.Nautilus Donut graph\nA Nautilus Donut graph is the same as a standard Donut graph, except that the \"wedges\" are arranged in a logarithmic spiral and become smaller closer to the center of the spiral.\nFor more information, see Pie Graph.Nightingale Rose graph", "source": "../../raw_kb/article/available_pietype_charts/index.html", "title": "Available Pie-Type Charts"}, {"objectID": "2af5930ae2ee-2", "text": "For more information, see Pie Graph.Nightingale Rose graph\nAs with other Pie-type graphs, Nightingale Rose graphs are divided into sections, each of which represents a percentage of the overall graph. These graphs are unique in that they are based on a polar coordinate system, i.e. each wedge originates from a common point in the center of the graph. This layout makes it easy to see smaller contributors to the overall pie.\nFor more information, see Pie Graph.Funnel graph\nFunnel graphs are similar to basic Pie graphs, except that they use a funnel shape divided into horizontal sections.\u00a0\nBy default,\u00a0in the legend for a Funnel graph, the percentage for each successive layer is derived from the value of the previous layer. This functionality is appropriate for\u00a0conversion charts, in which you want to show the degree of change\u00a0from one process to the next. If you want, you can turn off this default functionality. In this case,\u00a0percentages are derived from the chart total, as in a standard Pie chart.\u00a0The screenshot shows a\u00a0typical conversion Funnel graph.\nFor more information, see Funnel Chart.Stream Funnel graph\nA Stream Funnel graph is cross between a Stacked Area graph. Unlike other pie-type graphs in Domo, Stream Funnel graphs allow you to include series data (though this is not required). Legends are not shown in a Stream Funnel graph unless series data is included. Stream Funnel graphs also do not show value data except upon mouse-over. Therefore they are best used for showing relative differences between stages rather than representing exact amounts.\u00a0\nFor more information, see\u00a0Funnel Chart.Folded Funnel graph\nA Folded Funnel graph is a Funnel graph in which the sections have been \"folded.\" This type of Funnel graph shows the relative sizes of sections more accurately than a regular Funnel graph.\nFor more information, see Funnel Chart.Funnel Bars graph", "source": "../../raw_kb/article/available_pietype_charts/index.html", "title": "Available Pie-Type Charts"}, {"objectID": "2af5930ae2ee-3", "text": "For more information, see Funnel Chart.Funnel Bars graph\nA Funnel Bars graph is a combination of a Funnel graph and a horizontal Bar graph. This graph type shows differences between one stage in a process (such as a sales cycle) and the next. For each stage in the process, the data for the previous round drops off, so that only the relationships between the current round and all subsequent rounds is shown. In essence, this chart type shows \"what's left\" after each stage in a process. The example at right shows a Funnel Bars graph for a typical sales cycle. Each subsequent stage depicts the number of potential customers who have progressed to that stage.\nFor more information, see Funnel Chart.Tree Map\u00a0\nA Tree Map displays the names of categories within rectangles of differing sizes. The sizes of the rectangles reflect the relative values of each category.\nFor more information, see Tree Map.", "source": "../../raw_kb/article/available_pietype_charts/index.html", "title": "Available Pie-Type Charts"}, {"objectID": "d87e800b5258-0", "text": "Title\n\nAvailable Table-Type Charts\n\nArticle Body\n\nThe following table lists the types of tables available in Domo. You can click a thumbnail image to see a larger image.\nTable TypeDescriptionExampleHTML table", "source": "../../raw_kb/article/available_tabletype_charts/index.html", "title": "Available Table-Type Charts"}, {"objectID": "d87e800b5258-1", "text": "In a table, items in each column are grouped with the other items in the same row.\nIn the sample image, employees (in the \"Employee\" column) are grouped with their email addresses and telephone extensions.\nFor more information, see Table.Heat Map Table\nA heat map table is the same as a regular table except that values in value columns are highlighted in different shades depending on where they fall in the given range. For example, if the values in a column ranged from 10 to 1000, the values around \"10\" would appear light-colored, the values around \"1000\" would appear dark-colored, and all other values would fall somewhere in between.\nFor more information, see Table.Textbox\nA textbox is used to represent a piece of text. The text can be as long as you want as long as it fits in a single cell in a DataSet, and can contain any character. You can format text to be centered, left-aligned, or right-aligned, and can set the space of the margins. You can also specify text to override the text from your DataSet, as well as reference a value column instead using a macro.\nFor more information, see Textbox.Flex Table", "source": "../../raw_kb/article/available_tabletype_charts/index.html", "title": "Available Table-Type Charts"}, {"objectID": "d87e800b5258-2", "text": "For more information, see Textbox.Flex Table\nA flex table displays different rows for each series of a graph, with all series depicted as individual\u00a0spark bar\u00a0graphs. For each individual series, information about that series is provided in the other columns in the table. By default, this information includes the name of the series, an indicator showing whether the percent change for the series is positive or negative, and the degree of change. However, in the Chart Properties for the table, you can change columns to reflect any of a variety of other aspects of the series, such as the minimum or maximum value in the chart, the second-to-last value, a change value (as opposed to a percentage), etc. You can also add additional columns to the table (up to 10) with this information.\nFor more information, see\u00a0Flex Table.Pivot Table\nPivot tables provide a more powerful alternative to\u00a0HTML tables\u00a0in Domo.\u00a0They allow you to quickly and easily summarize large quantities of data from a DataSet as well as explore data by different dimensions and measures.\u00a0\nWith pivot tables, you can display data using both rows and columns. Each cell in the table\u00a0contains data for the intersecting column and row. You can even have multiple columns and/or rows. Each subsequent column or row you add appears as a new grouping within the previously added grouping.\nFor more information, see\u00a0Pivot Table.Mega Table\nMega tables have all of the same functionality as HTML tables and heat map tables; in addition, they can have up to 25,000 rows (HTML tables are limited to 10,000 cells), have expand/collapse and column sorting functionality, and can include functional URLs and images.\u00a0\nFor more information, see\u00a0Table.", "source": "../../raw_kb/article/available_tabletype_charts/index.html", "title": "Available Table-Type Charts"}, {"objectID": "e771b80f6f8b-0", "text": "TitleAventri ConnectorArticle BodyIntro\nAventri is a cloud-based event management software company. To learn more about the Aventri API, visit their page (https://developer.aventri.com/).\nYou connect to your Aventri account in the Data Center. This topic discusses the fields and menus that are specific to the Aventri connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Aventri account and create a DataSet, you must have the following:\nAn Aventri API key (for information about obtaining an API key, visit\u00a0https://developer.aventri.com/)Your Aventri account ID\nConnecting to your Aventri account\nThis section enumerates the options in the Credentials and Details panes in the Aventri connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials pane\nThis pane contains fields for entering credentials to connect to your Aventri account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAPI KeyEnter your Aventri API key. For information about obtaining an API key, see\u00a0https://developer.aventri.com/Aventri\u00a0Account IDEnter your Aventri account ID.Server RegionSelect the server region for your Aventri account.\nOnce you have entered valid Aventri credentials, you can use the same account any time you go to create a new Aventri DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails pane", "source": "../../raw_kb/article/aventri_connector/index.html", "title": "Aventri Connector"}, {"objectID": "e771b80f6f8b-1", "text": "Details pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Aventri report you want to run.\u00a0The following reports are available:AttendeeReturns detailed information for attendees at a given event.CategoriesReturns the list of available categories for a given event.Line ItemsReturns a list of line items for a given event.ReportsReturns detailed information for a given report and event.SessionReturns detailed information for sessions at a given event.Event IDSelect the event you want to retrieve information for.Report IDSelect the report you want to retrieve information for.\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nFAQs\nWhat version of the Aventri API does this connector use?\nThis connector uses version 2 of the Aventri API (https://api-{region}.eventscloud.com/api/v2/ereg/.)\nWhat endpoint does each report call in this connector?\nReport NameEndpoint URLAttendee/getAttendee.jsonCategories/listCategories.jsonLine Items/listLineItems.jsonReports/listReports.jsonSession/getSession.json\nCan I use the same account to create multiple DataSets?\nYes.\nHow often can the data be updated?\nAs often as needed.\nAre there any API limits I should be aware of?\nNo.", "source": "../../raw_kb/article/aventri_connector/index.html", "title": "Aventri Connector"}, {"objectID": "067fd68592b2-0", "text": "Title\n\nAvidTrak Connector\n\nArticle Body", "source": "../../raw_kb/article/avidtrak_connector/index.html", "title": "AvidTrak Connector"}, {"objectID": "067fd68592b2-1", "text": "Intro\nAvidTrak\u00a0is multi-featured software for call tracking, recording, and analytics for advertising campaigns. To learn more about the AvidTrak API, visit their page (http://avidtrak.com/Call_Tracking_API.php).  \nYou connect to your AvidTrak account in the Data Center. This topic discusses the fields and menus that are specific to the AvidTrak connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your AvidTrak account and create a DataSet, you must have the following:\nAn AvidTrak pin code (provided by AvidTrak when you sign up)The base URL of your AvidTrak instance\u00a0\nConnecting to Your AvidTrak Account\nThis section enumerates the options in the Credentials and Details panes in the AvidTrak Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AvidTrak account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionPin CodeEnter your AvidTrak pin code.Base URLEnter the URL of your AvidTrak instance.\nOnce you have entered valid AvidTrak credentials, you can use the same account any time you go to create a new AvidTrak DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with options for selecting the date or date range for the report.", "source": "../../raw_kb/article/avidtrak_connector/index.html", "title": "AvidTrak Connector"}, {"objectID": "067fd68592b2-2", "text": "MenuDescriptionReportSelect the AvidTrak report you want to run.\u00a0The following reports are available:Geographical DataReturns geographical data for calls for a selected date or date range.Offline Portal CallsReturns data for offline portal calls for a selected date or date range.Duration\u00a0Select whether you want to pull data for a specific date or a date range. If you select\u00a0Default, yesterday's data is pulled.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Date OffsetEnter the number of past days that should appear in the report.\u00a0\u00a0Date FromSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0Date ToSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Date FromSelect\u00a0the first date in your date range.\u00a0Select Specific Date ToSelect the second date in your date range.\u00a0Date From OffsetEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Date To OffsetEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.", "source": "../../raw_kb/article/avidtrak_connector/index.html", "title": "AvidTrak Connector"}, {"objectID": "067fd68592b2-3", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/avidtrak_connector/index.html", "title": "AvidTrak Connector"}, {"objectID": "db62f00588e0-0", "text": "TitleAwardco ConnectorArticle BodyIntro\nAwardco is a cloud-based employee recognition solution designed to boost employee engagement and simplify the reward process with incentivized programs. With Awardco, managers can recognize and reward the deserving employee within a matter of seconds, making the recognition process more empowering, relaxing, and flavorful from service awards to sales incentives. Use Domo's Awardco connector to retrieve data about recognitions, redemptions, transactions, and users in your organization. To learn more about the Awardco API, visit their page (https://api.awardco.com/).\nYou connect to your Awardco account in the Data Center. This topic discusses the fields and menus that are specific to the Awardco connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Awardco account and create a DataSet, you must have your Awardco API key.\nObtaining the API key:\nLog in to your Award Force account.Go to Settings > General > Integration.Note your Account ID\u00a0(It will be required for validating your access tokens).Next to the API key\u00a0field, click Generate key.You will see the API key field populated with your new key. Copy this key and paste it in the Connector's Credentials section (API Key). You can generate a new key at any time.\u00a0Once a new key is generated, the old key will stop working.\n\n\n \n\n\nNote:\u00a0In order to generate the API key, you must have the \"Awards manager\"\u00a0account privileges.", "source": "../../raw_kb/article/awardco_connector/index.html", "title": "Awardco Connector"}, {"objectID": "db62f00588e0-1", "text": "Connecting to Your Awardco Account\nThis section enumerates the options in the Credentials and Details panes in the Awardco Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Awardco account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAPI KeyEnter your Awardco API key. See \"Prerequisites\" for more details.\nOnce you have entered valid Awardco credentials, you can use the same account any time you go to create a new Awardco DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/awardco_connector/index.html", "title": "Awardco Connector"}, {"objectID": "db62f00588e0-2", "text": "MenuDescriptionReportSelect the Awardco report you want to run.\u00a0The following reports are available:Recognition DetailReturns a detail level report for recognition within the selected time frame.Recognition SummaryReturns a summary level report for recognition within the selected time frame.Redemption DetailReturns a detail level report for redemption within the selected time frame.Redemption FifoReturns the information for redemption fifo report within the selected time frameRedemption SummaryReturns a summary level report for redemption within the selected time frame.TransactionsReturns all transactions within the selected time frame.UsersReturns a list of all users.Date SelectionSelect the date format for your data.Single DateSelect whether the report data is for a specific date or for a relative number of days back from today.Specific DateSelect the specific date using the date selector.Relative DateEnter the number of days back that you would like to get data for in the\u00a0Days Back\u00a0field. Specify either today or 0, yesterday or 1, or today-7 or 7 to get data for 7 days into the past.Date RangeSelect the specific or relative date range.Start Date - SpecificSelect\u00a0the first date in your date range using the date selector.End Date - SpecificSelect the last date in your date range\u00a0using the date selector.Start Date - RelativeEnter the number of days back that you would like to get data from (start day). Combine with\u00a0End Date\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Start Date\u00a0and\u00a05\u00a0for\u00a0End Date, the report would contain data for\u00a010 days ago up until\u00a05 days ago.End Date - RelativeEnter the number of days back that you would like to get data to (end day). Combine with\u00a0Start Date\u00a0to create a range of represented days.", "source": "../../raw_kb/article/awardco_connector/index.html", "title": "Awardco Connector"}, {"objectID": "db62f00588e0-3", "text": "For example, if you entered\u00a010\u00a0for\u00a0Start Date\u00a0and\u00a05\u00a0for\u00a0End Date, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Time PeriodChoose the time period that you would like to receive data for.Starting Day of the WeekSelect the day you would like your week to start with.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/awardco_connector/index.html", "title": "Awardco Connector"}, {"objectID": "1858fc9640c8-0", "text": "TitleAWS Cost and Usage DashboardArticle BodyIntro\nMillions of businesses worldwide rely on AWS to power their cloud computing platforms. It's a reliable and efficient way to harness the power of cloud computing and scale your company's technological capabilities, but it's critical. You have a way to monitor usage so you don't run into surprises and can easily budget for your access to the cloud.The AWS Cost and Usage connector provides visibility into your organization's AWS usage, letting you optimize your AWS spend. Get all the critical metrics you need in one glance, drilling down to get more detailed information when needed.\nThe AWS Cost & Usage Report contains the most comprehensive set of AWS cost and usage data available, including additional metadata about AWS services, pricing, and reservations (Example: Amazon EC2 Reserved Instances (RIs)). With this connector you can pull your most updated AWS Cost and Usage report data. The first run of the connector pulls data from the current year. Subsequent runs pull historical data. During subsequent runs, the connector will also look at the modified dates on the files and re-pull any modified data. For more information about the AWS API, visit their website (https://docs.aws.amazon.com/AmazonS3/latest/dev/auth-request-sig-v2.html).\nYou connect to your AWS Cost and Usage account in the Data Center. This topic discusses the fields and menus that are specific to the AWS Cost and Usage connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo configure this connector, you must have the following:\nYour Amazon S3 Bucket name where your CUR report is locatedThe report name\u00a0used when you set up the CUR report in AWS billingYour AWS Access Key and Secret KeyAWS region where your S3 bucket is located\nCreating the Cost and Usage report in AWS:", "source": "../../raw_kb/article/aws_cost_and_usage_dashboard/index.html", "title": "AWS Cost and Usage Dashboard"}, {"objectID": "1858fc9640c8-1", "text": "Creating the Cost and Usage report in AWS:\nVisit\u00a0https://console.aws.amazon.com/billing/home#/reportsClick the\u00a0Create Report\u00a0button.Enter the report name, select the required options, and click the Next\u00a0button.", "source": "../../raw_kb/article/aws_cost_and_usage_dashboard/index.html", "title": "AWS Cost and Usage Dashboard"}, {"objectID": "1858fc9640c8-2", "text": "4. Now, set the delivery options for your report.\u00a0\n5. In order to receive AWS Cost & Usage Reports, you must have an Amazon S3 bucket created and configured with the appropriate access permissions. You can add an existing bucket or create a new one.\n\n\n\n \n\n\nImportant: Ensure to note\u00a0the Report path prefix and the Report name while creating your cost and usage report. You will need these to enter in\u00a0the connector UI credentials section.\u00a0\n\n\n\n6. Click the\u00a0Next button, review your report configuration, and create the report.\nConnecting to Your AWS Cost and Usage Account\nThis section enumerates the options in the Credentials and Details panes in the AWS Cost and Usage Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AWS Cost and Usage account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionBucketEnter your Amazon S3 Bucket name where your CUR report is located.RegionSelect the Amazon S3 Region.Report Path PrefixEnter the path where your CUR report is located.Report NamePut in the same value you used when you set up the CUR report in AWS billing.Authentication TypeSelect method of authentication.Key-AccessEnter your AWS access key.Key-SecretEnter your AWS secret key.\nOnce you have entered valid AWS Cost and Usage credentials, you can run the connector to pull your cost and usage report data.\u00a0\n\n\n \n\nNote:\u00a0While\u00a0selecting the Authentication Type, ensure that your authentication parameters have Read access to the S3 folder where all the data is located.", "source": "../../raw_kb/article/aws_cost_and_usage_dashboard/index.html", "title": "AWS Cost and Usage Dashboard"}, {"objectID": "1858fc9640c8-3", "text": "Details Pane\nAWS Cost and Usage Running Procedure\nThe first run of the connector pulls the current month of data. Subsequent runs will check if this month is updated and then start traveling back pulling data month by month.\nThe Connector is designed to look at the modified dates on the files and again pull the modified data, if any.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/aws_cost_and_usage_dashboard/index.html", "title": "AWS Cost and Usage Dashboard"}, {"objectID": "caaf8105dbc6-0", "text": "TitleAWS Data Exchange ConnectorArticle BodyIntro\nAWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. You can access datasets from qualified data providers that include category-leading brands such as Reuters, who curate data from over 2.2 million unique news stories per year in multiple languages; Change Healthcare, who process and anonymize more than 14 billion healthcare transactions and $1 trillion in claims annually; and Dun & Bradstreet, who maintain a database of more than 330 million global business records. Use Domo's Data Exchange Connector to import data into Domo from your Amazon S3 bucket. With the powerful datasets from AWS combined with the ability to visualize and create business solutions in Domo, you can create high-value solutions by combining your own data with external datasets. This connector includes only the AWS Data Exchange datasets that you've subscribed to and approved.\nTo learn more about the AWS\u00a0API, visit their page (https://aws.amazon.com/api-gateway/).\nYou connect to your AWS\u00a0account in the Data Center. This topic discusses the fields and menus that are specific to the AWS Data Exchange connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your AWS account and create a DataSet, you must have the following:\nYour AWS access key. You can find this in the Security Credentials section of the AWS Console. Alternatively, if you are using IAM, you can find it under Users.Your AWS secret key, which was provided when you created your access key. You can generate a new secret key in the AWS Console.The name of the AWS S3 Bucket you want to retrieve files from.\nCreating a user with the proper permissions\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.", "source": "../../raw_kb/article/aws_data_exchange_connector/index.html", "title": "AWS Data Exchange Connector"}, {"objectID": "caaf8105dbc6-1", "text": "To configure your user in the IAM Amazon Console,\nAdd a new user, setting options as follows:In the Details pane, check the box for Programmatic Access under Select AWS access type.\u00a0In the Permissions pane, select Attach existing policies directly, then check the box for either AmazonS3FullAccess or AmazonS3ReadOnlyAccess.Customer-managed policies do not work.In the Review pane, click Create User.After you create your user, copy the access and secret keys to use in the Credentials pane in Domo.\nConnecting to your AWS account\nThis section enumerates the options in the\u00a0 Credentials \u00a0and\u00a0 Details \u00a0panes in the AWS\u00a0Data Exchange\u00a0Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AWS account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see Prerequisites, above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see Prerequisites, above.BucketEnter the Amazon S3 Bucket you want to pull files from.\nOnce you have entered valid credentials, you can use the same account any time you go to create a new AWS Data Exchange DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains various menus for locating and configuring the file you want to pull into Domo.", "source": "../../raw_kb/article/aws_data_exchange_connector/index.html", "title": "AWS Data Exchange Connector"}, {"objectID": "caaf8105dbc6-2", "text": "This pane contains various menus for locating and configuring the file you want to pull into Domo.\nMenuDescriptionList DatasetsSelect the dataset you want to pull into Domo.\u00a0List Dataset RevisionsSelect the desired dataset revision.\u00a0List Revision AssetsSelect the desired revision asset.\u00a0KeyEnter the name of the Amazon S3 bucket object.\u00a0What Type of File Do You Want to Import?Select the desired file type, either CSV, TSV, or TXT.\u00a0Select the Delimiting CharacterSelect the delimiter used in the CSV file you want to retrieve. If your delimiter is not listed, select Other.Specify Your DelimiterEnter the character used to delimit your CSV text.\u00a0Quote CharacterSelect a quote character to parse the CSV file you want to retrieve. A double quote (\") is the CSV standard. If your quote character is not listed, select\u00a0Other.\u00a0Custom Quote CharacterEnter the desired CSV quote character.\u00a0Escape CharacterSelect an escape character to parse the CSV file you want to retrieve. A backslash (\\) is the CSV standard.Custom Escape CharacterEnter the desired CSV escape\u00a0character.", "source": "../../raw_kb/article/aws_data_exchange_connector/index.html", "title": "AWS Data Exchange Connector"}, {"objectID": "caaf8105dbc6-3", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.\nTroubleshooting\nEnsure that the file is present in the S3 bucket and that the correct file type is specified in the connector settings.NULL columns must be removed before the connector can successfully retrieve data.If you run into a \"Failed to Import Successfully\" error when trying to import a CSV file, you can often get around this by changing the option in the\u00a0Quote Character\u00a0menu to\u00a0No Quote Character.\nFAQ\nHow frequently will my data update?\nAs often as needed.\nAre there any API limits that I need to be aware of?\nNo.\nWhat kind of credentials do I need to power up this connector?\nYou need your AWS credentials: access key, secret key and the bucket name.\u00a0\nWhere can I find my access key and secret key?\nYou can find the access key in the Security Credentials section of the AWS Console. Alternatively, if you are using IAM, you can find it under Users. Your AWS secret key is provided when you create your access key. You can generate a new secret key in the AWS Console.\nDoes this connector list all of the available AWS Data Exchange datasets from the Amazon Marketplace?\nNo, only the ones to which you are subscribed and approved.", "source": "../../raw_kb/article/aws_data_exchange_connector/index.html", "title": "AWS Data Exchange Connector"}, {"objectID": "4ce45a63a44f-0", "text": "TitleAWS Data Exchange for Burbio ConnectorArticle BodyIntro\nAWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. You can access datasets from qualified data providers that include category-leading brands such as Reuters, who curate data from over 2.2 million unique news stories per year in multiple languages; Change Healthcare, who process and anonymize more than 14 billion healthcare transactions and $1 trillion in claims annually; Dun & Bradstreet, who maintain a database of more than 330 million global business records; and Foursquare, whose location data is derived from 220 million unique consumers and includes more than 60 million global commercial venues. Use Domo's Data Exchange Connector to import data into Domo from your Amazon S3 bucket. With the powerful datasets from AWS combined with the ability to visualize and create business solutions in Domo, you can create high-value solutions by combining your own data with external datasets. This connector lists only the AWS Data Exchange datasets that you've subscribed to and approved.\u00a0To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/...I/Welcome.html).\nYou connect to your AWS Data Exchange\u00a0account in the Data Center. This topic discusses the fields and menus that are specific to the AWS Data Exchange for Burbio connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your AWS Data Exchange account and create a DataSet, you must have the following:\nYour AWS access key. You can find this in the\u00a0Security Credentials\u00a0section of the AWS Console. Alternatively, if you are using IAM, you can find it under\u00a0Users.Your AWS secret key, which is provided to you when you create\u00a0your access key. You can generate a new secret key in the AWS Console.", "source": "../../raw_kb/article/aws_data_exchange_for_burbio_connector/index.html", "title": "AWS Data Exchange for Burbio Connector"}, {"objectID": "4ce45a63a44f-1", "text": "Your AWS Access Key and Secret Key should also have AWS Data Exchange Full access.\nCreating a User with the Proper Permissions\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.\nTo configure your user:\n1.\u00a0In the Identity and Access Management Amazon Console, click\u00a0Users.\n2. Click on the\u00a0Add User\u00a0button.\n3.\u00a0Add a new user, setting options as follows:\na. In the\u00a0Details\u00a0pane, check the box for\u00a0Programmatic Access\u00a0under\u00a0Select AWS access type.\n\u00a0 b.\u00a0In the\u00a0Permissions\u00a0pane, select\u00a0Attach existing policies directly, then check the box for either\u00a0AmazonS3FullAccess\u00a0or\u00a0AmazonS3ReadOnlyAccess.Customer-managed policies\u00a0do not work.\n\u00a0 c. Click\u00a0Next: Tags\u00a0button to add tags. Adding tags is\u00a0optional.\u00a0\u00a0\n\u00a0 d. In the\u00a0Review\u00a0pane, click\u00a0Create User.\n4.\u00a0\u00a0Once your user is created, copy the access and secret keys to use in the\u00a0Credentials\u00a0pane in Domo.\nConnecting to Your\u00a0AWS Data Exchange\u00a0Account\nThis section enumerates the options in the Credentials and Details panes in the AWS Data Exchange for Burbio Connector page. The components of the other panes in this page, Scheduling and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AWS Data Exchange\u00a0account. The following table describes what is needed for each field:\n\u00a0\nFieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see \"Prerequisites\" above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see \"Prerequisites\" above.", "source": "../../raw_kb/article/aws_data_exchange_for_burbio_connector/index.html", "title": "AWS Data Exchange for Burbio Connector"}, {"objectID": "4ce45a63a44f-2", "text": "Once you have entered valid AWS Data Exchange credentials, you can use the same account any time you go to create a new AWS Data Exchange for Burbio DataSet. You can manage Connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane lists the AWS Data Exchange\u00a0datasets along with the different parameters that you can select.\nMenuDescriptionRegionSelect the AWS region where the AWS Data Exchange is available. Visit\u00a0AWS Regional Services List\u00a0to get a detailed and updated list of the regions.ReportSelect whether you want to pull the data for Burbio or Covid-19 report.List Data SetsSelect the datasets. This connector lists only the AWS Data Exchange datasets that you've subscribed to and approved.List DataSet RevisionsSelect the DataSet revision.List Revision AssetsSelect the revision asset.What asset type would you like to import?Select the asset type that you would like to parse and import.Select the delimiting characterSelect the delimiting character used in your file. If your delimiter is not listed select 'Other.'Specify your delimiterEnter the character used to delimit your character separated values (CSV) text.Quote CharacterSelect the desired quote character for parsing CSV files. (Double quote is the default quote character for CSV standard)Custom Quote CharacterEnter the desired CSV Quote character.Escape CharacterSelect the desired escape character for parsing CSV files.Custom Escape CharacterEnter the desired CSV escape character.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/aws_data_exchange_for_burbio_connector/index.html", "title": "AWS Data Exchange for Burbio Connector"}, {"objectID": "f20e22fad461-0", "text": "TitleAWS Data Exchange V2 ConnectorArticle BodyIntro\nAWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. You can access datasets from qualified data providers that include category-leading brands such as Reuters, who curate data from over 2.2 million unique news stories per year in multiple languages; Change Healthcare, who process and anonymize more than 14 billion healthcare transactions and $1 trillion in claims annually; Dun & Bradstreet, who maintain a database of more than 330 million global business records; and Foursquare, whose location data is derived from 220 million unique consumers and includes more than 60 million global commercial venues. Use Domo's Data Exchange Connector to import data into Domo from your Amazon S3 bucket. With the powerful datasets from AWS combined with the ability to visualize and create business solutions in Domo, you can create high-value solutions by combining your own data with external datasets. This connector lists only the AWS Data Exchange datasets that you've subscribed to and approved. To learn more about the Amazon S3 API, visit their page (http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html).\nYou connect to your AWS Data Exchange V2 account in the Data Center. This topic discusses the fields and menus that are specific to the AWS Data Exchange V2 connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding\u00a0a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is an excellent choice for retrieving flat files when APIs are not an option.Primary MetricsN/APrimary Company RolesData specialistsMarketing rolesFinance rolesAnyone who has data stored in S3Average Implementation TimeLess than an hour if you have the correct file types in S3.Ease of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices", "source": "../../raw_kb/article/aws_data_exchange_v2_connector/index.html", "title": "AWS Data Exchange V2 Connector"}, {"objectID": "f20e22fad461-1", "text": "Best Practices\nUnderstanding the data stored in S3 and its relation to other S3 databases will be a huge asset in using this connector.\nPrerequisites\nTo connect to your AWS Data Exchange V2 account and create a DataSet, you must have the following:\nYour AWS access key. You can find this in the\u00a0Security Credentials\u00a0section of the AWS Console. Alternatively, if you are using IAM, you can find it under\u00a0Users.Your AWS secret key, which was provided when you created your access key. You can generate a new secret key in the AWS Console.\nYour AWS Access Key and Secret Key should also have AWS Data Exchange Full access.\nCreating a User with the Proper Permissions\nYou must create a user with the proper permissions in the IAM Amazon Console before you can connect to S3 data in Domo.\nTo configure your user:\n1.\u00a0In the Identity and Access Management Amazon Console, click Users.", "source": "../../raw_kb/article/aws_data_exchange_v2_connector/index.html", "title": "AWS Data Exchange V2 Connector"}, {"objectID": "f20e22fad461-2", "text": "2. Click on the Add User button.\n\n3.\u00a0Add a new user, setting options as follows:\nIn the Details pane, check the box for Programmatic Access under Select AWS access type.In the Permissions pane, select Attach existing policies directly, then check the box for either AmazonS3FullAccess or AmazonS3ReadOnlyAccess.Customer-managed policies do not work.", "source": "../../raw_kb/article/aws_data_exchange_v2_connector/index.html", "title": "AWS Data Exchange V2 Connector"}, {"objectID": "f20e22fad461-3", "text": "Click Next: Tags button to add tags. Adding tags is\u00a0optional.\u00a0\u00a0\u00a0 In the Review pane, click Create User.\n4.\u00a0\u00a0Once your user is created, copy the access and secret keys to use in the Credentials pane in Domo.\nConnecting to Your AWS Data Exchange V2 Account\nThis section enumerates the options in the\u00a0 Credentials \u00a0and\u00a0 Details \u00a0panes in the AWS Data Exchange V2 Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your AWS Data Exchange V2 account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionAccess KeyEnter your AWS access key. For information about finding your access key, see Prerequisites\u00a0above.Secret KeyEnter your AWS secret key. For information about finding your secret key, see Prerequisites\u00a0above.\nOnce you have entered valid AWS Data Exchange V2 credentials, you can use the same account any time you go to create a new AWS Data Exchange V2 DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane lists the AWS Data Exchange datasets along with the different parameters that you can select.", "source": "../../raw_kb/article/aws_data_exchange_v2_connector/index.html", "title": "AWS Data Exchange V2 Connector"}, {"objectID": "f20e22fad461-4", "text": "This pane lists the AWS Data Exchange datasets along with the different parameters that you can select.\nMenuDescriptionList Data SetsSelect the datasets. This connector lists only the AWS Data Exchange datasets that you've subscribed to and approved.RegionSelect the AWS region where the AWS Data Exchange is available. Visit\u00a0AWS Regional Services List\u00a0to get a detailed and updated list of the regions.What asset type would you like to import?Select the asset type that you would like to parse and importSelect the delimiting characterSelect the delimiting character used in your file. If your delimiter is not listed select 'Other.'Specify your delimiterEnter the character used to delimit your character separated values (CSV) text.Quote CharacterSelect the desired quote character for parsing CSV files. (Double quote is the default quote character for CSV standard)Custom Quote CharacterEnter the desired CSV Quote character.Escape CharacterSelect the desired escape character for parsing CSV files.Custom Escape CharacterEnter the desired CSV escape character.Enter the WorkBook Sheet NameEnter the WorkBook Sheet Name.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/aws_data_exchange_v2_connector/index.html", "title": "AWS Data Exchange V2 Connector"}, {"objectID": "7066f8755e4a-0", "text": "TitleAWS Domo Verizon IoT PipelineArticle BodyCreating the IoT Pipeline\nPreparation\nRegister an account with Verizon ThingSpace. You will be provided a username, password, and billing account ID. You will also need a client ID and secret key from Verizon\u2019s developer page.\nReference: ThingSpace Critical Asset Sensor Pilot - Streaming Quick Start Guide - v1.3In addition to the Verizon ThingSpace account and developer credentials, you will need an AWS account with admin permissions to install AWS CloudFormation templates. If you do not have these permissions, you can pass these instructions to your DevOps or IT Administration team to create the AWS CloudFormation templates, or to grant permissions to your AWS user.\nAWS CloudFormation\nDomo has developed an AWS CloudFormation template to make deploying a complex IoT pipeline as simple as possible. This diagram describes the architecture in further detail:", "source": "../../raw_kb/article/aws_domo_verizon_iot_pipeline/index.html", "title": "AWS Domo Verizon IoT Pipeline"}, {"objectID": "7066f8755e4a-1", "text": "On the Edge, Critical Asset Sensors publish data on regular intervals into Verizon ThingSpace, which updates an AWS Thing Device Shadow document. The IoT Device Shadow Document data is selected with an IoT Rule. One rule will select from all device shadows, that have the name format ts_123456789012345, which is ts_ plus the IMEI of the device on the Edge. ThingSpace creates these device shadow documents, so no special naming is required. In addition, the pipeline will automatically scale to select any new devices in your fleet. Once they are added to your Verizon billing account and the devices are charged and turned on, Verizon will create shadows for the new devices and begin pushing data to AWS. The rule was designed to automatically select data from the new devices and ingest their data without any action from the administrator. After the data is selected by the IoT Rule, it is forwarded to an IoT Analytics Channel. This channel collects all the events from the Rule and funnels them into the data pipeline. The IoT Analytics Pipeline selects all the data attributes and normalizes the data for ingestion into the data lake. The data then passes into the IoT Analytics DataStore, where it is stored for DataSet creation. A dataset is created on a regular chron schedule of 15 minutes to ingest the IoT data in the DataStore.\nIn Domo, an AWS IoT Analytics connector is created to ingest the normalized data into Domo. On a regular interval, data will be ingested into Domo. You can use Domo\u2019s powerful ETL tools to join the data with other data sources. You can create cards and visualizations that can give more insight into your IoT data. Using these cards and dashboards, you can then create alerts to get notification when values stray out of acceptable tolerances. Using Domo Buzz, your people can connect with each other and data to have meaningful conversations about the data.\nDeploying the AWS CloudFormation Template", "source": "../../raw_kb/article/aws_domo_verizon_iot_pipeline/index.html", "title": "AWS Domo Verizon IoT Pipeline"}, {"objectID": "7066f8755e4a-2", "text": "Deploying the AWS CloudFormation Template\nSign into the AWS Console. Assume a role if required by your organization\u2019s policies. Currently the pipeline only supports regions that support AWS IoT Analytics:\nUS East (Ohio)US East (N. Virginia)US West (Oregon)Asia Pacific (Tokyo)EU (Frankfurt)EU (Ireland)Click Services, then search for CloudFormation. Click on the auto-complete entry for CloudFormation.Click Create Stack. Select Template is ready. Under Specify Template, select Upload a template file.Click Choose file\u00a0and navigate to the JSON template that was provided in the POC.Follow the steps to deploy the CloudFormation Template.\nDomo Configuration App\nDomo has developed a Domo App to assist in creating the pipeline connections between Verizon and AWS. The Verizon Critical Asset Sensor Configuration Tool\u00a0is deployed to your Domo instance.\u00a0\nTo create a new instance of the App, do the following:", "source": "../../raw_kb/article/aws_domo_verizon_iot_pipeline/index.html", "title": "AWS Domo Verizon IoT Pipeline"}, {"objectID": "7066f8755e4a-3", "text": "To create a new instance of the App, do the following:\nLog in to your Domo instance.Click the More\u00a0icon at the top center of the menu bar.Click Asset Library.  Click on the Verizon IoT Configuration Tool icon.  Click New Card.  With your mouse in the charcoal gray area, scroll to the bottom of the page.Click Account Mappings, then click Verizon.  Click Select Account\u2026\u00a0then Add Account\u2026. If no previous accounts were added, the display may not show Select Account\u2026.Scroll to fit all the fields in your view and enter your credentials, then click Connect. After the account is mapped, scroll back up to the top and click Save and Finish. No Datapoint Mappings\u00a0are required for this app.Saving the app should take you to your instance of the App Card. If it doesn\u2019t, it should appear on your Overview page. This is found in the left navigation pane, which can be pinned and unpinned. If it is unpinned, mouse over to the left side and it will appear. Using the pushpin icon will pin the left navigation menu.Open the app card, by finding it on your Overview page and clicking the Wrench\u00a0icon then clicking Details.The first page of the app looks like this:  Enter your Verizon Billing Account ID and click Next. The App will display a spinner for a moment and then will show some information:", "source": "../../raw_kb/article/aws_domo_verizon_iot_pipeline/index.html", "title": "AWS Domo Verizon IoT Pipeline"}, {"objectID": "7066f8755e4a-4", "text": "Verizon\u2019s AWS account ID This is Verizon\u2019s account ID in AWS. You will need this to enable a service IAM role that Verizon will be able to assume to push IoT data into the pipeline.External ID The service IAM role is secured using this security string, and only allows Verizon to assume the role when this External ID matches the ID in AWS. We\u2019ll enable this permission in AWS now.Sign in to AWS. Browse to IAM (Identity and Access Management). From the IAM Dashboard, click Roles in the left column. Find the IAM Service Role created by CloudFormation. The name of this role will contain \"ThingSpaceIoTServiceRole\" in its name.Click the Trust Relationships tab. You can see the Trusted Entity has been populated with Verizon's AWS Account ID on the right. Under Conditions, a ExternalId field is entered but has no value.Click Edit Trust Relationship. This displays the IAM Policy document. Paste the External ID generated in the app as the value for the field \"sts:ExternalId\". Click Update Trust Policy\u00a0to save the values.Copy the IAM Role ARN.Go back to the Domo App. Click Next. Now we will create an AWS Streaming Target in Verizon ThingSpace. Select the region from the dropdown. Enter the ARN you copied in the previous step. Give the target a name and optional description. Click Next.Now we will create an AWS Streaming Subscription. Enter your email address for error events from ThingSpace and give the subscription a name. Click Finish.The last page of the App displays all the information for the resources that were created. Copy the information and save for your records. All the panels in the app are scrollable, so you can see more information if it does not fit in the view finder.The app is only needed to be used once to spin up the pipeline.", "source": "../../raw_kb/article/aws_domo_verizon_iot_pipeline/index.html", "title": "AWS Domo Verizon IoT Pipeline"}, {"objectID": "7066f8755e4a-5", "text": "Data should begin flowing into AWS IoT Core once you have charged and turned on the devices. To turn on the device for the first time, press and hold the touch sensitive button in the center of the device. The device does not turn off. When on battery power, the device does not flash the LED lights during beaconing to save battery.", "source": "../../raw_kb/article/aws_domo_verizon_iot_pipeline/index.html", "title": "AWS Domo Verizon IoT Pipeline"}, {"objectID": "7066f8755e4a-6", "text": "Note: Any Subsequent users will need to create their own app and follow the same process.", "source": "../../raw_kb/article/aws_domo_verizon_iot_pipeline/index.html", "title": "AWS Domo Verizon IoT Pipeline"}, {"objectID": "f7237983167f-0", "text": "TitleAyuda ConnectorArticle BodyIntro\nAyuda Media Sytems provides sofware to OOH (out-of-home)\u00a0advertising companies. \u00a0To learn more about the Ayuda API, visit their page (http://www.ayudasystems.com/Products/AyudaPI).\nYou connect to your Ayuda account in the Data Center. This topic discusses the fields and menus that are specific to the Ayuda connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Ayuda account and create a DataSet, you must have the following:\nAn Ayuda app keyAn Ayuda app secretThe base URL for your Ayuda instance (e.g. https://domo.ayudapreview.com)\nFor help in obtaining credentials, reach out to your Ayuda\u00a0customer representative.\nConnecting to Your Ayuda Account\nThis section enumerates the options in the Credentials and Details panes in the Ayuda Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Ayuda account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionApp KeyEnter your Ayuda app key.App SecretEnter your Ayuda app secret.Base URLEnter the base URL for your Ayuda instance\u00a0(e.g. https://domo.ayudapreview.com).\u00a0\nOnce you have entered valid Ayuda credentials, you can use the same account any time you go to create a new Ayuda DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.", "source": "../../raw_kb/article/ayuda_connector/index.html", "title": "Ayuda Connector"}, {"objectID": "f7237983167f-1", "text": "Details Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Ayuda report you want to run.\u00a0The following reports are available:ActivitiesReturns information about calls, emails, meetings, and tasks.ContactsReturns information about your contacts, including their names, positions, and contact info.Digital AvailabilityReturns information from Digital Avails.\u00a0OpportunitiesReturns information about opportunities, including dates and expected amounts.Sales AccountsReturns information about agencies and advertisers, with their contacts, opportunities, calls, meetings, and tasks.\u00a0UsersReturns information about users.Spot LengthEnter the length of the spot you want to retrieve information for.Face IDsEnter a comma-separated list of face IDs you want to retrieve data for.Duration\u00a0Select whether you want to pull data for a specific date or a date range.\u00a0Report Date\u00a0Select whether the report data is for a specific date or for a relative number of days back from today.\u00a0Select Specific Date\u00a0Select the date for the report.\u00a0Days BackEnter the number of past days that should appear in the report.\u00a0\u00a0Start DateSpecify whether the\u00a0first date in your date range is a specific or relative date.\u00a0You select the last date in your range in\u00a0End Date.\u00a0End DateSpecify whether the second date in your date range is a specific or relative date. You select the first date in your range in\u00a0Start Date.\u00a0\u00a0Select Specific Start DateSelect\u00a0the first date in your date range.\u00a0Select Specific End DateSelect the second date in your date range.\u00a0Days Back to Start FromEnter the number of the farthest day back that should be represented in the report. Combine with\u00a0Days Back to End At\u00a0to create a range of represented days.", "source": "../../raw_kb/article/ayuda_connector/index.html", "title": "Ayuda Connector"}, {"objectID": "f7237983167f-2", "text": "For example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.Days Back to End AtEnter the number of the most recent day back that should be represented in the report. Combine with\u00a0Days Back to Start From\u00a0to create a range of represented days.\nFor example, if you entered\u00a010\u00a0for\u00a0Days Back to Start From\u00a0and\u00a05\u00a0for\u00a0Days Back to End At, the report would contain data for\u00a010 days ago up until\u00a05 days ago.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/ayuda_connector/index.html", "title": "Ayuda Connector"}, {"objectID": "7ac064005f64-0", "text": "TitleAzureIotHub ConnectorArticle BodyIntro\nAzureIotHub Hub is a managed service, hosted in the cloud, that acts as a central message hub for bi-directional communication between an IoT application and the devices it manages. Azure IoT Hub can be used to build IoT solutions with reliable and secure communications between millions of IoT devices and a cloud-hosted solution backend.\nThe AzureIotHub Connector is a \"Cloud App\" Connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the Connector page for this and other Cloud App Connectors by clicking\u00a0Cloud App\u00a0in the toolbar at the top of the window.\nYou connect to your AzureIotHub account in the Data Center. This topic discusses the fields and menus that are specific to the\u00a0Connector user interface. GenAzureIotHuberal information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your AzureIotHub account and create a DataSet, you must have the following:\nAn active\u00a0Azure account. If you don't have one, you can\u00a0create a free account.A\u00a0Translator\u00a0resource.A\u00a0standard performance\u00a0Azure Blob Storage account. You'll create containers to store and organize your files within your storage account. If you don't know how to create an Azure storage account with a storage container, follow these quickstarts:Create a storage account. When you create your storage account, select\u00a0Standard\u00a0performance in the\u00a0Instance details\u00a0>\u00a0Performance\u00a0field.Create a container. When you create your container, set the\u00a0Public access level\u00a0to\u00a0Container\u00a0(anonymous read access for containers and files) in the\u00a0New Container\u00a0window.\nConnecting to Your AzureIotHub Connector\u00a0Account", "source": "../../raw_kb/article/azureiothub_connector/index.html", "title": "AzureIotHub Connector"}, {"objectID": "7ac064005f64-1", "text": "Connecting to Your AzureIotHub Connector\u00a0Account\nThis section enumerates the options in the Credentials and Details panes in the AzureIotHub Connector page. The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThe Domo AzureIotHub Connector uses OAuth to connect, so there is no need to enter credentials within Domo. Click\u00a0Connect\u00a0(or select\u00a0Add Account\u00a0if you have existing AzureIotHub accounts in Domo) to open the AzureIotHub\u00a0\u00a0OAuth screen where you can enter your AzureIotHub username and password. Once you have entered valid AzureIotHub credentials, you can use the same account any time you go to create a new AzureIotHub\u00a0\u00a0DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see\u00a0Managing User Accounts for Connectors.", "source": "../../raw_kb/article/azureiothub_connector/index.html", "title": "AzureIotHub Connector"}, {"objectID": "7ac064005f64-2", "text": "Note:\u00a0If you are already logged into AzureIotHub when you connect in Domo, you are authenticated automatically when you click\u00a0Add account. If you want to connect to an account that is different from the one you are logged into, you must first log out of AzureIotHub.\u00a0\n\n\n\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the AzureIotHub\u00a0report you want to run. The following reports are available:\u00a0Get Configurations\nGet multiple configurations for devices or modules of an IoT Hub.\n\nGet DevicesGet the identities of multiple devices from the IoT hub identity registry.Device Registry Statistics\nRetrieves statistics about device identities in the IoT hubs identity registry.\n\nGet Job DetailsRetrieves details of a scheduled job from an IoT hub.\u00a0Get Modules on DeviceRetrieve all the module identities on the device.Get Module TwinGets a module twin.Query IoT Hub\nQuery an IoT hub to retrieve information regarding device twins using a SQL-like language.\n\nQuery Jobs\nQuery an IoT hub to retrieve information regarding jobs using the IoT Hub query language.\nService StatisticsRetrieves service statistics for this IoT hubs identity registry.Twin\n\u00a0\n\n\n\n\n\n\nGets a device twin.", "source": "../../raw_kb/article/azureiothub_connector/index.html", "title": "AzureIotHub Connector"}, {"objectID": "7ac064005f64-3", "text": "Other Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding a DataSet Using a Data Connector.\nFAQs\nWhat is the base URL of the Connector?\nThe base URL of the connector is https://<fully-qualified-iothubname>.azure-devices.net/\nWhich endpoint(s) does each report call in this Connector?\nReport NameEndpoint URL(s)Get Configurations/configurations/{id}Get Devices/devices/{deviceId}Device Registry Statistics/statistics/devicesGet Job Details/jobs/v2/{jobId}Get Modules on Device/devices/{deviceId}/modulesGet Module Twin/twins/{deviceId}/modules/{mid}Query IoT Hub/devices/query\u00a0Query Jobs/jobs/v2/queryService Statistics/statistics/serviceTwin\n\u00a0/twins/{deviceId}\nWhat kind of credentials do I need to power up this Connector?\nTo connect to AzureIotHub account you may need the following:\nParameter NameWhere to find itShared Access Key NameIt is generated by Azure & will be used for Web UI loginShared Access KeyIt is generated by Azure & will be used for Web UI loginHost NameIt is generated by Azure & will be used for Web UI login\nHow do I know my AzureIotHub Connector credentials are secure?\nThe login process uses the OAuth process, so your credentials are never seen or stored by Domo. This keeps your login secure. You can revoke Domo's access to your account at any time.\nDo I need a certain kind of account to set up the Connector?\nAre there any API limits I should be aware of?\nResource Manager applies a limit on the number of requests per hour to prevent an application from sending too many requests. If the application exceeds those limits, requests are throttled. The response header includes the number of remaining requests for the scope.\nTroubleshooting", "source": "../../raw_kb/article/azureiothub_connector/index.html", "title": "AzureIotHub Connector"}, {"objectID": "7ac064005f64-4", "text": "Troubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/azureiothub_connector/index.html", "title": "AzureIotHub Connector"}, {"objectID": "1e9e1c446966-0", "text": "TitleAzure Data Lake Storage Gen2 Using AAD ConnectorArticle BodyIntro\nAzure Data Lake Storage Gen2 is the one of the world's most productive Data Lake. It is an enterprise-wide hyper-scale repository for big data analytic workloads.\u00a0It makes Azure Storage the foundation for building enterprise data lakes on Azure. It allows you to easily manage massive amounts of data and helps to speed up your transition from proof of concept to production. Use Domo's\u00a0Microsoft Azure Data Lake Store Gen2 using AAD connector to pull data files from your Data Lake Store into Domo. You can retrieve data files in any of the following file types: CSV, TSV, and TXT.\u00a0For more information about Azure's API, see\u00a0https://docs.microsoft.com/en-us/azu...ta-lake-store/.\u00a0\nYou connect to your Microsoft Azure Data Lake Store Gen2 AAD account in the Data Center. This topic discusses the fields and menus that are specific to the Microsoft Azure Data Lake Store Gen2\u00a0using AAD connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to your Microsoft Azure Data Lake Store Gen2\u00a0AAD account and create a DataSet, you must have the following:\nYour client ID (GUID) and secret key of the client web app obtained from Azure Active Directory configuration\u00a0Your Microsoft\u00a0Azure Data Lake Storage\u00a0account nameYour Tenant ID\nConfiguring Your Azure App\nYou must first create a Microsoft Azure Web App to obtain the necessary client ID and secret.\u00a0\nTo configure an Azure App:\nLogin into\u00a0Azure Portal.\u00a0Select\u00a0Azure Active Directory > App Registrations > New Registration.\n3. Fill out the registration form as follows:", "source": "../../raw_kb/article/azure_data_lake_storage_gen2_using_aad_connector/index.html", "title": "Azure Data Lake Storage Gen2 Using AAD Connector"}, {"objectID": "1e9e1c446966-1", "text": "3. Fill out the registration form as follows:\nName:\u00a0Enter name for your\u00a0 application.Supported account types: Select an option to specify who\u00a0can use this application or access this API.Redirect URI: Enter the redirect URI.\u00a0The authentication response is sent to this URI after successfully authenticating the user. Providing the URI\u00a0at this stage\u00a0is optional and it can be changed later, but a value is required for most of the\u00a0authentication scenarios.\n4.\u00a0Click\u00a0Register. An overview of your registered App will appear.\u00a0\n5. The\u00a0Application ID\u00a0that appears here\u00a0is the\u00a0Client ID\u00a0and the\u00a0he\u00a0Directory ID\u00a0is the\u00a0Tenant ID\u00a0that you will enter in the\u00a0Credentials\u00a0pane in Domo.\n6. Now, click on\u00a0Certificates and secrets.\u00a0Certificates can be used as secrets to prove the application\u2019s identity when requesting a token. Also, can be referred to as public keys.\n7. Click\u00a0+New client secret.\n8.\u00a0Specify the client secret description and the expiry period for your client secret, and click\u00a0Add.\n9. Copy the value and paste it into the\u00a0Client Secret\u00a0field in the\u00a0Connector credentials\u00a0section in Domo.\nConnecting to Your\u00a0Microsoft Azure Data Lake Store Gen2 AAD\u00a0Account\nThis section enumerates the options in the Credentials and Details panes in the Microsoft Azure Data Lake Store Gen2\u00a0using AAD Connector page. The components of the other panes in this page, Scheduling and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Microsoft Azure Data Lake Store Gen2\u00a0AAD account. The following table describes what is needed for each field:", "source": "../../raw_kb/article/azure_data_lake_storage_gen2_using_aad_connector/index.html", "title": "Azure Data Lake Storage Gen2 Using AAD Connector"}, {"objectID": "1e9e1c446966-2", "text": "FieldDescriptionAzure Application Client IDEnter the client ID (GUID) of the client web app obtained from Azure Active Directory configuration. See \"Prerequisites\" for more information.Azure Application Client SecretEnter the secret key of the client web app. See \"Prerequisites\" for more information.Tenant IDEnter your Tenant ID. Go to Portal.azure.com > Azure Active Directory > Properties. The Directory ID is your Tenant ID.Account NameEnter your Microsoft\u00a0Azure Data Lake storage account name.\n\u00a0\nOnce you have entered valid\u00a0Microsoft Azure Data Lake Store Gen2\u00a0AAD\u00a0credentials, you can use the same account any time you go to create a new\u00a0Microsoft Azure Data Lake Store Gen2\u00a0AAD\u00a0DataSet. You can manage Connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary Reports menu, along with various other menus which may or may not appear depending on the report type you select.", "source": "../../raw_kb/article/azure_data_lake_storage_gen2_using_aad_connector/index.html", "title": "Azure Data Lake Storage Gen2 Using AAD Connector"}, {"objectID": "1e9e1c446966-3", "text": "MenuDescriptionWhat file type would you like to import?Select the type of the file that you want to import.Container NameSelect the conatiner.How would you like to choose the directory name?Select how would you like to choose the directory name.Discover Directory NameSelect the directory name.Enter Directory NameEnter the directory name using the following examples.Example for main or root directorytest\u00a0Example for sub directorytest/testExample for sub-sub-directorytest/test/testHow would you like to choose the File name?Select how would you like to choose the file name.Enter File NameEnter the file\u00a0name.Discover File NameSelect the File name.File EncodingSelect the file encoding. By default,\u00a0UTF-8\u00a0is selected.Select the delimiting characterSelect the delimiting character used in your file. If your delimiter is not listed here, select 'Other.'Specify your delimiterEnter the character used to delimit your character separated values (CSV) text.Quote CharacterSelect the desired quote character for parsing CSV files (Double quote is the default quote character for CSV standard.)Custom Quote CharacterEnter the desired CSV Quote character.Escape CharacterSelect the desired escape character for parsing CSV files.Custom Escape CharacterEnter the desired CSV escape character.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see Adding a DataSet Using a Data Connector.\nFAQs\nWhat kind of credentials do I need to power up this Connector?\nYou need the client ID, client secret, and tenant ID of the client web app obtained from Azure Active Directory configuration, and your storage account name.\nWhere can I find my client ID, client secret, and tenant ID?\nTo obtain the Client ID:\nLogin to the Azure Portal (https://protal.azure.com).Navigate to Azure Active Directory >> App registrations.Click on the Web App > Application ID.The application ID is your client ID.\nTo obtain the Secret key:", "source": "../../raw_kb/article/azure_data_lake_storage_gen2_using_aad_connector/index.html", "title": "Azure Data Lake Storage Gen2 Using AAD Connector"}, {"objectID": "1e9e1c446966-4", "text": "To obtain the Secret key:\nLogin to the Azure Portal (https://protal.azure.com).Navigate to Azure Active Directory >> App registrations.Click on the Certificates and secrets > +New client secret.Copy the secret value. Note that the secret is generated only once.\nTo obtain the Tenant ID:\nLogin to the Azure Portal (https://protal.azure.com).Navigate to Azure Active Directory >> Properties.Copy the directory ID. The directory ID is your tenant ID.\nAre there any API limits I should be aware of?\nNo\nHow often can the data be updated?\nAs often as needed.\nCan I use the same account to create multiple datasets?\nYes\nTroubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/azure_data_lake_storage_gen2_using_aad_connector/index.html", "title": "Azure Data Lake Storage Gen2 Using AAD Connector"}, {"objectID": "1fe5c2e0f5cd-0", "text": "TitleAzure DevOps ConnectorArticle BodyIntro\nAzure DevOps Server is a Microsoft product that provides version control, reporting, requirements management, project management, automated builds, testing and release management capabilities. It covers the entire application lifecycle, and enables DevOps capabilities. Use the Domo Azure DevOps connector to retrieve data about your projects, teams and work items.\u00a0Use Domo's Azure DevOps connector to retrieve data about your projects, teams and work items.\u00a0To learn more about the Azure DevOps API, visit their page (https://docs.microsoft.com/en-us/res...evops-rest-6.1).\nYou connect to your Azure DevOps account in the Data Center. This topic discusses the fields and menus that are specific to the Azure DevOps connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\n\u00a0\nTo connect to your Azure DevOps account and create a DataSet, you must have the following:\nThe username you use to sign into your Azure DevOps account. If it is an email address, the @subdomain portion may not be necessary.Your\u00a0Personal Access Token that you can generate by going to\u00a0User Settings > Personal Access Token\u00a0in your\u00a0Azure DevOps\u00a0account.Your Azure DevOps organization. For example, if you logged into your Azure DevOps instance at \"https://dev.azure.com/domo\"\", you would enter \"domo\" here.\nConnecting to Your\u00a0Azure DevOps Account\nThis section enumerates the options in the\u00a0Credentials\u00a0and\u00a0Details\u00a0panes in the\u00a0Azure DevOps\u00a0Connector page.\u00a0The components of the other panes in this page,\u00a0Scheduling\u00a0and\u00a0Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in Adding a DataSet Using a Data Connector.\nCredentials Pane", "source": "../../raw_kb/article/azure_devops_connector/index.html", "title": "Azure DevOps Connector"}, {"objectID": "1fe5c2e0f5cd-1", "text": "Credentials Pane\nThis pane contains fields for entering credentials to connect to your\u00a0Azure DevOps\u00a0account. The following table describes what is needed for each field:\nFieldDescriptionUsernameEnter the username you use to sign into your Azure DevOps account. If this is an email address, the @subdomain portion may not be necessary.Personal Access TokenEnter your\u00a0Personal Access Token that you can generate by going to\u00a0User Settings > Personal Access Token\u00a0in your\u00a0Azure DevOps\u00a0account.OrganizationEnter your Azure DevOps organization. For example, if you logged into your Azure DevOps instance at \"https://dev.azure.com/domo\"\", you would enter \"domo\" here.\nOnce you have entered valid\u00a0Azure DevOps\u00a0credentials, you can use the same account any time you go to create a new\u00a0Azure DevOps\u00a0DataSet. You can manage connector accounts in the\u00a0Accounts\u00a0tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary Reports menu, along with various other menus which may or may not appear depending on the report type you select.\nMenuDescriptionReportSelect the Azure DevOps report you want to run.\u00a0The following reports are available:ProjectsRetrieves details about your projects.TeamsRetrieves details about your teams.Work ItemsRetrieves details about your work items.Backfill DateSelect the date to retrieve Work Items from based on the Work Item's 'ChangedDate' field.Ignore ordering of Work ItemsSelect this checkbox if you want to ignore the ordering of the Ids while retrieving the work items.Enter the value for top query parameterEnter a value to specify the maximum number of work items to be pulled in an API call, else it would use the default value.\nOther Panes\nFor information about the remaining sections of the Connector interface, including how to configure scheduling, retry, and update options, see Adding a DataSet Using a Data Connector.", "source": "../../raw_kb/article/azure_devops_connector/index.html", "title": "Azure DevOps Connector"}, {"objectID": "1fe5c2e0f5cd-2", "text": "Troubleshooting\nMake sure your authentication remains valid.Review the configuration to make sure that all required items have been selected.Review the Connector history for error messages.In rare cases, you may be requesting too much information and reaching API limitations or timeouts. If this is the case, you can review the history of the Connector run to see the error message and duration. If this is the case, you can reduce the number of accounts that are being pulled, choose a smaller number of metrics for the report that you are pulling, or reduce the timeframe that you are trying to pull.", "source": "../../raw_kb/article/azure_devops_connector/index.html", "title": "Azure DevOps Connector"}, {"objectID": "c38d61ba9152-0", "text": "TitleAzure SQL Data Warehouse ConnectorArticle BodyAzure SQL Datawarehouse is now Azure\u00a0Synapse SQL Datawarehouse\nWhat is happening?\nStarting on Monday Nov 16th\u00a02020, the\u00a0Azure SQL Data Warehouse\u00a0Connector will be known as the\u00a0Azure Synapse SQL Connector.\u00a0\nHow does this affect me?\nAll Azure SQL Data Warehouse datasets will continue to run as normal. The functionality of the connector has not been changed.\nCustomers using or planning to use the connector will notice the following:\nThere is a new icon for the connector in the Appstore.\u00a0 Existing datasets will now show up on the Data Center with the new icon.Customers who are trying to search for an Azure Data Warehouse dataset in the Data Center using a \u2018Type\u2019 filter will now need to search for datasets of type \u2018Azure Synapse SQL\u2019 instead of \u2018Azure Data Warehouse\u2019.Customers who want to create a new Azure SQL Data Warehouse will need to search the Appstore for the \u2018Azure Synapse SQL Connector\u2019 instead of the \u2018Azure SQL Data Warehouse Connector.\u2019Connector Accounts for the Azure SQL Data Warehouse Connector will now be called \u2018Azure Synapse SQL\u2019 Accounts.\nDo I need to take any action if I have any Azure Data Warehouse datasets?\nNo action is required on your part.\u00a0Your existing Azure Data Warehouse/Azure Synapse SQL datasets will continue to run without interruption, and you will still be able to create new datasets with existing credentials as needed.\u00a0You do not need to update any of your reports or Accounts.\nPlease visit Azure Synapse SQL Connector documentation for any\u00a0queries related to the connector functionality.", "source": "../../raw_kb/article/azure_sql_data_warehouse_connector/index.html", "title": "Azure SQL Data Warehouse Connector"}, {"objectID": "92a1a994f3a9-0", "text": "TitleAzure Synapse SQL ConnectorArticle Body\n\n \n\n\nImportant: Starting on Monday, Nov 16th 2020, the\u00a0Azure SQL Data Warehouse\u00a0Connector\u00a0will be known as the\u00a0Azure Synapse SQL Connector.\n\nHow does this affect me?\nAll Azure SQL Data Warehouse datasets will continue to run as normal. The functionality of the connector has not been changed.\nCustomers using or planning to use the connector will notice the following:\nThere is a new icon for the connector in the Appstore.\u00a0 Existing datasets will now show up on the Data Center with the new icon.Customers who are trying to search for an Azure Data Warehouse dataset in the Data Center using a \u2018Type\u2019 filter will now need to search for datasets of type \u2018Azure Synapse SQL\u2019 instead of \u2018Azure Data Warehouse\u2019.Customers who want to create a new Azure SQL Data Warehouse will need to search the Appstore for the \u2018Azure Synapse SQL Connector\u2019 instead of the \u2018Azure SQL Data Warehouse Connector.\u2019Connector Accounts for the Azure SQL Data Warehouse Connector will now be called \u2018Azure Synapse SQL\u2019 Accounts.", "source": "../../raw_kb/article/azure_synapse_sql_connector/index.html", "title": "Azure Synapse SQL Connector"}, {"objectID": "92a1a994f3a9-1", "text": "Intro\nAzure Synapse SQL lets you quickly implement a high-performance, globally available, and secure cloud data warehouse. Use Domo's Azure Synapse SQL Connector to bring your Azure data into Domo.\u00a0\nYou connect to your Azure Synapse SQL database in the Data Center. This topic discusses the fields and menus that are specific to the Azure Synapse SQL Connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in Adding a DataSet Using a Data Connector.\nPrerequisites\nTo connect to an Azure Synapse SQL database and create a DataSet, you must have the following:\nThe username and password you use to log into your Azure Synapse database.The server name for the database. You can find this by doing the following:Log into Azure Portal. \u00a0Click on\u00a0Azure Synapse Analytics. Click on the Synapse SQL Pool you want to connect to. Locate the full server name. Example\u00a0yourserver.database.windows.netThe port number for the databaseThe database name\nBefore you can connect to an Azure Synapse SQL\u00a0database, you must also whitelist a number of IP addresses on your database server on the\u00a0port\u00a0you want to connect to. For the full list of IP addresses, see\u00a0Whitelisting IP Addresses for Connectors.\nConnecting to Your Azure SQL\u00a0Database\nThis section enumerates the options in the Credentials and Details panes in the Azure Synapse SQL Connector page. The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your Azure Synapse SQL database. The following table describes what is needed for each field:", "source": "../../raw_kb/article/azure_synapse_sql_connector/index.html", "title": "Azure Synapse SQL Connector"}, {"objectID": "92a1a994f3a9-2", "text": "FieldDescriptionServer NameEnter the name of your server. For information about finding the server name, see \"Prerequisites,\" above.Database NameEnter the name of the Azure Synapse database.PortEnter the port number for the Azure Synapse database.UsernameEnter the username you use to log into your Azure Synapse database.PasswordEnter the password you use to log into your Azure Synapse database.\nOnce you have entered valid credentials, you can use the same account any time you go to create a new Azure Synapse SQL DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nIn this pane you create an SQL query to pull data from your database. The\u00a0Query parameter is required. The other three parameters are here to help you construct this query, if you choose.\nMenuDescriptionQueryEnter the Structured Query Language (SQL) query to use in selecting the data you want. For example:\nselect * from Employee\nIf you want help in constructing your query, copy and paste the automatically-generated query from the\u00a0Query Helper\u00a0field into this field.Database Table (Optional)Select the table containing the data you want to pull into Domo. The selected table will be added to the automatically generated query in the\u00a0Query Helper\u00a0field.\u00a0Table Columns (Optional)Select the table columns with data you want to pull into Domo.\u00a0The selected columns\u00a0will be added to the automatically generated query in the\u00a0Query Helper\u00a0field.\u00a0Query Helper (Optional)Copy and paste this query into the\u00a0Query\u00a0field if you need help building a query. This query is automatically generated when you select a table and columns in the\u00a0Database Table\u00a0and Table Columns\u00a0fields, respectively.\nOther Panes", "source": "../../raw_kb/article/azure_synapse_sql_connector/index.html", "title": "Azure Synapse SQL Connector"}, {"objectID": "92a1a994f3a9-3", "text": "Other Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0Adding\u00a0a DataSet Using a Data Connector.", "source": "../../raw_kb/article/azure_synapse_sql_connector/index.html", "title": "Azure Synapse SQL Connector"}, {"objectID": "c44532e70f10-0", "text": "TitleBacking Up Workbench 4 JobsArticle Body\n\n\n\n\nImportant: Support for Workbench 4 ended on April 15, 2021. Workbench may continue to run on installed machines, but it will no longer receive feature enhancements and security updates. When issues are encountered in Workbench 4, the recommended course of action from the Domo Support team will be to upgrade to the latest version of Workbench 5. To see this article for Workbench 5, click here.\n\n\n\nBacking up DataSet jobs is\u00a0an often overlooked task when using Workbench. You may be wondering why jobs need to be backed up if they\u2019re stored in the Domo cloud. Accidents can happen. Jobs can be accidentally deleted by Workbench users, which will also permanently delete them from the cloud. Further, if the account is accidentally removed from Workbench, all jobs are deleted from the cloud. Exporting your jobs ensures you have a fallback option in the event of a problem.\nYour exported job files should be stored somewhere safe, as they can contain secure information including API keys, queries, database and/or filenames, etc. Treat them the same as you would passwords or financial information.\nTraining Video - Backing Up Jobs\n\nTo back up jobs in Workbench 4,\nOpen Workbench 4.Starting at the bottom and working your way to the top, do the following for each job:Right-click the job\u00a0name and select Export DataSet job.Give the exported job file a name.Choose a location to save it in.\nYou should make a new backup for a job each time a change is made to that job.", "source": "../../raw_kb/article/backing_up_workbench_4_jobs/index.html", "title": "Backing Up Workbench 4 Jobs"}, {"objectID": "62abdeb39b00-0", "text": "Title\n\nBacking Up Workbench 5.1 Jobs\n\nArticle Body\n\nBacking up DataSet jobs is\u00a0an often overlooked task when using Workbench. You may be wondering why jobs need to be backed up if they\u2019re stored in the Domo cloud. Accidents can happen. Jobs can be accidentally deleted by Workbench users, which will also permanently delete them from the cloud. Further, if the account is accidentally removed from Workbench, all jobs are deleted from the cloud. Exporting your jobs ensures you have a fallback option in the event of a problem.\nYour exported job files should be stored somewhere safe, as they can contain secure information including API keys, queries, database and/or filenames, etc. Treat them the same as you would passwords or financial information.\nTo back up jobs in Workbench 5.1,\nOpen Workbench 5.1.Click \u00a0in the icon bar.Click\u00a0Export\u00a0in the left-hand navigation pane. (If this pane does not appear when you click , click \u00a0to open it.)Click\u00a0Save to,\u00a0then choose where you want to save your exported jobs on your machine.\n\n\n\n\n\nNote: It is NOT recommended that you switch jobs between computers. Please set up new jobs on each computer.", "source": "../../raw_kb/article/backing_up_workbench_51_jobs/index.html", "title": "Backing Up Workbench 5.1 Jobs"}, {"objectID": "62abdeb39b00-1", "text": "(Optional) If you would job export files to be categorized by Workbench account, check the box that reads\u00a0Create a new folder for each account.Check the boxes for all jobs you want to export.\u00a0Click\u00a0Export Jobs.\nTo import jobs back into Workbench,\nOpen Workbench 5.1.Click \u00a0in the icon bar.Click\u00a0Import\u00a0in the left-hand navigation pane. (If this pane does not appear when you click , click \u00a0to open it.)Under \"Select backup file,\" in the menu,\u00a0select the Workbench account you want the imported jobs to be associated with.Click\u00a0Browse\u00a0then navigate to the files you want to import.All files you select appear in the pane in the\u00a0Jobs to import\u00a0area at the bottom of the screen.Click\u00a0Import\u00a0to pull the files into Workbench.", "source": "../../raw_kb/article/backing_up_workbench_51_jobs/index.html", "title": "Backing Up Workbench 5.1 Jobs"}, {"objectID": "ce7f4f34f271-0", "text": "TitleBacking Up Workbench 5.1  SettingsArticle BodyIntro\u00a0\nStarting in Workbench 5.1 you can now create a complete backup of your Workbench settings to install on a new computer. This allows you to migrate your settings and accounts exactly as they are from one machine to another. To do this you have to be running Workbench as a local admin on the windows machine.\u00a0\nNote: Additionally, it's still best practice to back up your jobs separately. The process of backing up your Workbench 5.1 settings does not directly back up individual Workbench 5.1 jobs.\u00a0\nTo learn more about backing up your Workbench jobs, refer to the Backing Up Workbench 5.1 jobs article.\nCreating a Workbench 5.1 Back Up\nClick on the ellipses (More) icon.Click Settings.In the Path field, type the file path or click the Browse... button to select a file path.Type the desired name for the .wbf file.Click Save. After you\u2019ve selected where you want to save the backup you can create a passcode used for importing the data into Workbench.\u00a0In the Passcode field, Type the desired passcode for the .wbf file or, click Generate and copy to have Workbench 5.1 automatically create and copy a passcode for you. The passcode must be at least 20 characters in length and contain at least 1 uppercase character(s), 1 lowercase character(s), 1 number(s), and 1 symbol(s).Click Create. If done correctly, you will see a message in green saying success. This creates an encrypted .wbf file, a proprietary format that can be used to quickly restore your workbench settings on a new machine.", "source": "../../raw_kb/article/backing_up_workbench_51_settings/index.html", "title": "Backing Up Workbench 5.1  Settings"}, {"objectID": "ce7f4f34f271-1", "text": "The backup contains credentials and other potentially sensitive details used in your Workbench Jobs, The passcode is used to decrypt that information upon import back in to workbench, so keep it safe.\n\u00a0\nStopping Workbench 5.1\nAfter creating the backup, you will need to turn off the workbench service on the original machine. Running multiple instances of workbench pointed to the same agent in Domo, will result in an error.\n To stop the service:\nClick on the upward facing arrow on the bottom right of your Windows taskbar.\u00a0Right click the Workbench icon.Select Service.then click Stop. \nIf you no longer need Workbench on the existing machine you can uninstall it through the windows control panel.\n\u00a0\nImporting Workbench 5.1 Settings\nIf you lose your Workbench 5.1 settings for any reason, and still have the back up .wbf file you created for your Workbench 5.1 settings, you can import that file using Workbench 5.1 to restore your settings.\u00a0\nClick on the ellipses (More) icon.Click Settings.Click Import Backup.Click the Browse... button to locate the .wbf file with your Workbench 5.1 settings.\u00a0In the Passcode field, paste or type the Passcode associated with your .wbf file.\u00a0Click Import. If successful, there will be a message in green saying success.\u00a0\nYou can learn more about backing up your Workbench 5.1 settings by watching this video.", "source": "../../raw_kb/article/backing_up_workbench_51_settings/index.html", "title": "Backing Up Workbench 5.1  Settings"}, {"objectID": "c9b0d3fcd91c-0", "text": "Title\n\nBacking Up Workbench 5 Jobs\n\nArticle Body\n\nBacking up DataSet jobs is\u00a0an often overlooked task when using Workbench. You may be wondering why jobs need to be backed up if they\u2019re stored in the Domo cloud. Accidents can happen. Jobs can be accidentally deleted by Workbench users, which will also permanently delete them from the cloud. Further, if the account is accidentally removed from Workbench, all jobs are deleted from the cloud. Exporting your jobs ensures you have a fallback option in the event of a problem.\nYour exported job files should be stored somewhere safe, as they can contain secure information including API keys, queries, database and/or filenames, etc. Treat them the same as you would passwords or financial information.\nTo back up jobs in Workbench 5,\nOpen Workbench 5.Click\u00a0\u00a0in the icon bar.Click\u00a0Export\u00a0in the left-hand navigation pane. (If this pane does not appear when you click\u00a0, click\u00a0\u00a0to open it.)Click\u00a0Save to,\u00a0then choose where you want to save your exported jobs on your machine.\n\n\n\n\n\nNote: It is NOT recommended that you switch jobs between computers. Please set up new jobs on each computer.", "source": "../../raw_kb/article/backing_up_workbench_5_jobs/index.html", "title": "Backing Up Workbench 5 Jobs"}, {"objectID": "c9b0d3fcd91c-1", "text": "(Optional) If you would job export files to be categorized by Workbench account, check the box that reads\u00a0Create a new folder for each account.Check the boxes for all jobs you want to export.\u00a0Click\u00a0Export Jobs.\nTo import jobs back into Workbench,\nOpen Workbench 5.Click\u00a0\u00a0in the icon bar.Click\u00a0Import\u00a0in the left-hand navigation pane. (If this pane does not appear when you click\u00a0, click\u00a0\u00a0to open it.)Under \"Select backup file,\" in the menu,\u00a0select the Workbench account you want the imported jobs to be associated with.Click\u00a0Browse\u00a0then navigate to the files you want to import.All files you select appear in the pane in the\u00a0Jobs to import\u00a0area at the bottom of the screen.Click\u00a0Import\u00a0to pull the files into Workbench.", "source": "../../raw_kb/article/backing_up_workbench_5_jobs/index.html", "title": "Backing Up Workbench 5 Jobs"}, {"objectID": "ca7d66af4da5-0", "text": "TitleBambooHR Advanced ConnectorArticle BodyIntro\nBambooHR consolidates all of your employee information from all of your locations into a single view. You can then access, control, sort, analyze, and take action on the data instantly-from your office, the road or home. The connector enables all those reports to be easily imported into Domo. \u00a0For more information about the\u00a0BambooHR API, visit\u00a0their website. (http://www.bamboohr.com/api/documentation/)\nYou connect to your BambooHR account in the Data Center. This topic discusses the fields and menus that are specific to the BambooHR connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0 a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is appropriate for returning head count, payroll, and recurring reporting.Primary MetricsEmployee demographicsSalary informationCustom reportsPrimary Company RolesVPHR directorAverage Implementation Time20 hoursEase of Use (on a 1-to-10 scale with 1 being easiest)4\nBest Practices\nDo not bring in Personally Identifiable   (PII) into Domo. Limit those columns and the number of columns by using the Report Fields selector.\nPrerequisites\nTo connect to your BambooHR account and create a DataSet, you must have the following:", "source": "../../raw_kb/article/bamboohr_advanced_connector/index.html", "title": "BambooHR Advanced Connector"}, {"objectID": "ca7d66af4da5-1", "text": "To connect to your BambooHR account and create a DataSet, you must have the following:\nYour company domain name or the URL of your company BambooHR instance. This is the same\u00a0URL you use to log in to the BambooHR web interface. For example: https://companydomainname.bamboohr.comYour BambooHR\u00a0API key. To generate an API key for a given user, do the following:Log in to BambooHR and click\u00a0your name in the upper right hand corner of any page to access the user context menu.Click API name in the upper right hand corner of any page to access the user context menu.Click API Keys to open a page in which you can generate an API key.        For more information about API keys, see http://www.bamboohr.com/api/documentation/login.php.\nConnecting to Your BambooHR Account\nThis section enumerates the options in the Credentials and Details panes in the BambooHR Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your BambooHR account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionCompany DomainEnter your company domain name or the URL of your company BambooHR instance (such as companydomainname.bamboohr.com).API KeyEnter your BambooHR API key.\nOnce you have entered valid BambooHR credentials, you can use the same account any time you go to create a new BambooHR DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary\u00a0Reports\u00a0menu and two other menus for configuring your report.", "source": "../../raw_kb/article/bamboohr_advanced_connector/index.html", "title": "BambooHR Advanced Connector"}, {"objectID": "ca7d66af4da5-2", "text": "This pane contains a primary\u00a0Reports\u00a0menu and two other menus for configuring your report.\nMenuDescriptionReportSelect the BambooHR report you want to run.\u00a0The following reports are available:Employee DataReturns data about employees, such as name, address, email, birthday, gender, ethnicity, department, supervisor, etc.User DataReturns data about users of this account, including first and last name, email, status, etc.Company ReportReturns a specified company report.Custom FieldsSelect all fields you want to appear in your report.Company Report IDEnter the ID for the company report you want to retrieve. You can find a company report ID by hovering over the report name on\u00a0your BambooHR\u00a0Reports page.\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0\u00a0a DataSet Using a Data Connector.\nTroubleshooting\nVerify the Company Domain Name and API are correct.Validate the results by running the report in BambooHR.If running a custom report, verify the Report ID is correct.", "source": "../../raw_kb/article/bamboohr_advanced_connector/index.html", "title": "BambooHR Advanced Connector"}, {"objectID": "a89b63d1c4bd-0", "text": "TitleBambooHR ConnectorArticle BodyIntro\nBambooHR consolidates all of your employee information from all of your locations into a single view. You can then access, control, sort, analyze, and take action on the data instantly-from your office, the road or home. The connector enables all those reports to be easily imported into Domo. For more information about the\u00a0BambooHR API, visit\u00a0their website. (http://www.bamboohr.com/api/documentation/)\nThe BambooHR connector is a \"Cloud App\" connector, meaning it retrieves data stored in the cloud. In the Data Center, you can access the connector page for this and other Cloud App connectors by clicking Cloud App in the toolbar at the top of the window. \u00a0\u00a0\nYou connect to your\u00a0BambooHR account in the Data Center. This topic discusses the fields and menus that are specific to the\u00a0BambooHR\u00a0connector user interface. General information for adding DataSets, setting update schedules, and editing DataSet information is discussed in\u00a0Adding a DataSet Using a Data Connector.\nPrimary Use CasesThis connector is appropriate for returning head count, payroll, and recurring reporting.Primary MetricsEmployee demographicsSalary informationCustom reportsPrimary Company RolesVPHR directorAverage Implementation Time10 hoursEase of Use (on a 1-to-10 scale with 1 being easiest)3\n\u00a0\nBest Practices\nMake sure to include all applicable custom fields in your report.\nPrerequisites\nTo connect to your\u00a0BambooHR account and create a DataSet, you must have the following:", "source": "../../raw_kb/article/bamboohr_connector/index.html", "title": "BambooHR Connector"}, {"objectID": "a89b63d1c4bd-1", "text": "Your company domain name or the URL of your company BambooHR instance. This is the same\u00a0URL you use to log in to the BambooHR web interface. For example: https://companydomainname.bamboohr.com \u00a0Your BambooHR\u00a0API key. To generate an API key for a given user, log in to BambooHR and click\u00a0your name in the upper right hand corner of any page to access the user context menu. Then click API Keys to open a page in which you can generate an API key. For more information about API keys, see http://www.bamboohr.com/api/documentation/login.php.\nConnecting to Your\u00a0BambooHR Account\nThis section enumerates the options in the\u00a0 Credentials \u00a0and\u00a0 Details \u00a0panes in the\u00a0BambooHR\u00a0Connector page.\u00a0The components of the other panes in this page, Scheduling\u00a0and Name & Describe Your DataSet, are universal across most connector types and are discussed in greater length in\u00a0Adding a DataSet Using a Data Connector.\nCredentials Pane\nThis pane contains fields for entering credentials to connect to your\u00a0BambooHR\u00a0account. The following table describes what is needed for each field: \u00a0\nFieldDescriptionCompany DomainEnter your company domain name or the URL of your company BambooHR instance (such as companydomainname.bamboohr.com).API KeyEnter your BambooHR API key.\nOnce you have entered valid\u00a0BambooHR credentials, you can use the same account any time you go to create a new\u00a0BambooHR DataSet. You can manage connector accounts in the Accounts tab in the Data Center. For more information about this tab, see Managing User Accounts for Connectors.\nDetails Pane\nThis pane contains a primary Report menu in which you select a report type.", "source": "../../raw_kb/article/bamboohr_connector/index.html", "title": "BambooHR Connector"}, {"objectID": "a89b63d1c4bd-2", "text": "Details Pane\nThis pane contains a primary Report menu in which you select a report type.\nMenuDescriptionReportSelect a BambooHR report.\u00a0The following reports are available:Company ReportReturns data of all the reports of the particular company.Complete Employee DataReturns complete data for all the current employees.Employee Bulk DataReturns data of all the employees in a single report.Employee DataReturns data about employees, such as name, address, email, birthday, gender, ethnicity, department, supervisor, etc.\u00a0User DataReturns data about users of this account, including first and last name, email, status, etc.\u00a0Custom Fields\u00a0Select all fields you want to appear in your report.\u00a0\nOther Panes\nFor information about the remaining sections of the connector interface, including how to configure scheduling, retry, and update options, see\u00a0\u00a0a DataSet Using a Data Connector.\nTroubleshooting\nVerify the Company Domain Name and API are correct.Validate the results by running the report in BambooHR.\nFAQ\nWhat version of the BambooHR API does this connector use?\nThis connector uses version 1 of the BambooHR API\n\u00a0https://api.bamboohr.com/api/gateway.php/{subdomain}/v1/).\nWhich endpoint(s) does each report call in this connector?\nReport NameEndpoint URLCompany Report/reports/{reportid}Employee Data/employee/{employeeid}/?fields=(customfields)Employee Bulk Data/reports/customsUser Data/meta/users\nWhat kind of credentials do I need to power up this connector?\nYou need your company domain name and API key.\nWhere can I find my API Key?\nTo generate an API key To generate an API key, log into BambooHR and click your name in the upper right corner of any page to access the user context menu. Then click API Keys to open a page in which you can generate an API key.", "source": "../../raw_kb/article/bamboohr_connector/index.html", "title": "BambooHR Connector"}, {"objectID": "a89b63d1c4bd-3", "text": "Do I need a certain kind of account with the data service to set up the connector?\nAny BambooHR account will work.\nWhat else do I need to do to power up my connector?\nAn individual user must be issued an API key that you then supply to Domo while creating an account. The user must have the necessary permissions set up inside of BambooHR to receive the requested data.\nCan I use the same account multiple times to create datasets?\nYes\nHow do I know if my query is right?\nVerify the returned data within the BambooHR system.\nWhat's the difference between this connector and the BambooHR connector?\nBambooHR Advanced connector supports the company report and company report id as well.", "source": "../../raw_kb/article/bamboohr_connector/index.html", "title": "BambooHR Connector"}]
