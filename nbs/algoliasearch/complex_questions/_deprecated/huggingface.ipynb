{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "#!pip install tiktoken\n",
    "#!pip install langchain\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_CHUNK_SIZE = 4000\n",
    "TEST_ARTICLE_LIMIT = 20 # or NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def open_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise Exception(f\"{file_path} - does not exist\")\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def write_file(file_path, content, method = 'w'):\n",
    "    with open (file_path, method, encoding ='utf-8') as f:\n",
    "        json.dump(content, f, indent = 2)\n",
    "\n",
    "def write_text(file_path, content, method = 'w'):\n",
    "    with open (file_path, method, encoding ='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def crawl_directory(directory_path):\n",
    "\n",
    "    if not os.path.exists(directory_path):\n",
    "        raise Exception(f\"{directory_path} does not exist\")\n",
    "        \n",
    "    file_ls = list()\n",
    "    for root, dirs, files in os.walk(directory_path, topdown=False):\n",
    "        [file_ls.append(os.path.join(root, name)) for name in files]\n",
    "    \n",
    "    return file_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "INLINE_LINK_RE = re.compile(r'\\[([^\\]]+)\\]\\(([^)]+)\\)')\n",
    "FOOTNOTE_LINK_TEXT_RE = re.compile(r'\\[([^\\]]+)\\]\\[(\\d+)\\]')\n",
    "FOOTNOTE_LINK_URL_RE = re.compile(r'\\[(\\d+)\\]:\\s+(\\S+)')\n",
    "\n",
    "def process_article(text):\n",
    "    try:\n",
    "        text = text.split('---', 2)[2]\n",
    "\n",
    "        return re.sub(INLINE_LINK_RE, '', text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# process_article(all_text[2].get('content'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "# recommended for use with text-embedding-ada-002\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=TOKEN_CHUNK_SIZE,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n",
    "\n",
    "# text_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ['OPENAPI_SECRET_KEY']\n",
    "\n",
    "\n",
    "def get_gpt3_embedding(content, engine=\"text-embedding-ada-002\"):\n",
    "    print('accessing chatgpt')\n",
    "    response = openai.Embedding.create(input=[content.replace(\"\\n\", \" \")],\n",
    "                                       model=engine\n",
    "                                       )\n",
    "    vector = response['data'][0]['embedding']\n",
    "    return vector\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all md files in article folder\n",
    "def build_index():\n",
    "    file_ls = crawl_directory('../../../raw_kb/article')\n",
    "    all_text = [{'file_path': file_path,\n",
    "                'content': open_file(file_path)}\n",
    "                for file_path in file_ls if file_path.endswith('index.qmd')]\n",
    "\n",
    "    # subset for testing or set TEST_ARTICLE_LIMIT to NONE\n",
    "    all_text = all_text[:TEST_ARTICLE_LIMIT]\n",
    "\n",
    "    all_text_chunks = [{'file_path': text_obj.get('file_path'),\n",
    "                        'content': chunk,\n",
    "                        'chunk_id': index}\n",
    "                       for text_obj in all_text\n",
    "                       for index, chunk in enumerate(text_splitter.split_text(\n",
    "                           process_article(text_obj.get('content')))\n",
    "                           ) if text_obj.get('content')]\n",
    "\n",
    "    all_text_chunks[0]\n",
    "    result = list()\n",
    "\n",
    "    for chunk in all_text_chunks:\n",
    "        embedding = get_gpt3_embedding(chunk['content'].encode(\n",
    "            encoding='ASCII', errors='ignore').decode())\n",
    "\n",
    "        chunk.update({'vector': embedding})\n",
    "        print(chunk, '\\n\\n\\n')\n",
    "\n",
    "        result.append(chunk)\n",
    "\n",
    "    write_file('index.json', result)\n",
    "\n",
    "# build_index()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Question Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similarity(v1, v2):\n",
    "    \"\"\"returns dot product of two vectors\"\"\"\n",
    "    return np.dot(v1,v2)\n",
    "    \n",
    "def search_index(text, data, count=5):\n",
    "    \"\"\"\n",
    "    calculates the embeddings vector for the `text` question then retrieves the top N matches\n",
    "    will not calculate embedding for the same question twice (retrieves from history)\n",
    "    \"\"\"\n",
    "    ## to do extend this to include other sources\n",
    "\n",
    "    search_scores = 'question_vectors.json'\n",
    "    search_vectors = json.loads(open_file(file_path=search_scores))\n",
    "\n",
    "    match_text = next((search_vector for search_vector in search_vectors\n",
    "                       if search_vector.get('question') == text), None)\n",
    "\n",
    "    vector = None\n",
    "    if match_text:\n",
    "        vector = match_text.get('vector')\n",
    "\n",
    "    else:\n",
    "        vector = get_gpt3_embedding(text)\n",
    "        search_vectors.append({'question': text, 'vector': vector})\n",
    "        write_file(search_scores, content=search_vectors, method='a')\n",
    "\n",
    "    scores = list()\n",
    "\n",
    "    for i in data:\n",
    "        score = similarity(vector, i.get('vector'))\n",
    "        scores.append({**i, 'score': score})\n",
    "    ordered = sorted(scores, key=lambda d: d.get('score'), reverse=True)\n",
    "\n",
    "    return ordered[:count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "\n",
    "def gpt3_completion(prompt,\n",
    "                    engine='text-davinci-003',\n",
    "                    temp=0.6, top_p=1.0, tokens=2000, freq_pen=0.25, pres_pen=0.0,\n",
    "                    stop=['<<END>>']):\n",
    "\n",
    "    max_retry = 1\n",
    "    retry = 0\n",
    "    prompt = prompt.encode(encoding='ASCII', errors='ignore').decode()\n",
    "\n",
    "    while retry <= max_retry:\n",
    "        try:\n",
    "            print('accessing chat gpt')\n",
    "            response = openai.Completion.create(\n",
    "                engine=engine,\n",
    "                prompt=prompt,\n",
    "                temperature=temp,\n",
    "                max_tokens=tokens,\n",
    "                top_p=top_p,\n",
    "                frequency_penalty=freq_pen,\n",
    "                presence_penalty=pres_pen,\n",
    "                stop=stop)\n",
    "\n",
    "            text = response.get('choices')[0].get('text').strip()\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "            filename = f\"{dt.datetime.now().strftime('%Y-%m-%d %H%M')}_gpt3.txt\"\n",
    "\n",
    "            write_text(f\"gpt3_logs/{filename}\",\n",
    "                       content=f\"PROMPT:\\n\\n{prompt} '\\n\\n============\\n\\n RESPONSE:\\n\\n{text}\")\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            retry += 1\n",
    "            if retry >= max_retry:\n",
    "                return f\"GPT3 error {e}\"\n",
    "\n",
    "            print(f\"GPT3 error {e} retrying\")\n",
    "            time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open_file(file_path='index.json')\n",
    "data = json.loads(data)\n",
    "\n",
    "# while True:\n",
    "# query = input(\"Enter your question here: \")\n",
    "query = \"what is the Views Explorer\"\n",
    "\n",
    "matched_articles_chunks = search_index(query, data, count = 5)\n",
    "\n",
    "# generates level2 prompt which sumarizes matched article chunks\n",
    "answers = list()\n",
    "for article_chunk in matched_articles_chunks:\n",
    "    prompt = open_file('prompt_answer.txt').replace('<<PASSAGE>>', answer.get('content')).replace('<<QUERY>>', query)\n",
    "    answer = gpt3_completion(prompt)\n",
    "    print('\\n\\n', answer)\n",
    "    answers.append(answer)\n",
    "\n",
    "all_answers = \"\\n\\n.join(answers)\n",
    "chunks = final_text_splitter.text_split(all_answers)\n",
    "\n",
    "aggregate_answer= list()\n",
    "for chunk in chunks:\n",
    "    prompt = open_file('prompt_summary.txt').replace('<<SUMMARY>>', chunk)\n",
    "    summary =gpt3_completion(prompt)\n",
    "    aggregate_answer.append(summmary)\n",
    "    \n",
    "print('\\n\\n=========\\n\\n', '\\n\\n'.join(final))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
